<!DOCTYPE html>
<!-- START: inst/pkgdown/templates/layout.html --><!-- Generated by pkgdown: do not edit by hand --><html lang="en" data-bs-theme="auto">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<title>Medical Image Processing: All in One View</title>
<meta name="viewport" content="width=device-width, initial-scale=1">
<script src="../assets/themetoggle.js"></script><link rel="stylesheet" type="text/css" href="../assets/styles.css">
<script src="../assets/scripts.js" type="text/javascript"></script><!-- mathjax --><script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      config: ["MMLorHTML.js"],
      jax: ["input/TeX","input/MathML","output/HTML-CSS","output/NativeMML", "output/PreviewHTML"],
      extensions: ["tex2jax.js","mml2jax.js","MathMenu.js","MathZoom.js", "fast-preview.js", "AssistiveMML.js", "a11y/accessibility-menu.js"],
      TeX: {
        extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"]
      },
      tex2jax: {
        inlineMath: [['\\(', '\\)']],
        displayMath: [ ['$$','$$'], ['\\[', '\\]'] ],
        processEscapes: true
      }
    });
    </script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><!-- Responsive Favicon for The Carpentries --><link rel="apple-touch-icon" sizes="180x180" href="../apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
<link rel="manifest" href="../site.webmanifest">
<link rel="mask-icon" href="../safari-pinned-tab.svg" color="#5bbad5">
<meta name="msapplication-TileColor" content="#da532c">
<meta name="theme-color" media="(prefers-color-scheme: light)" content="white">
<meta name="theme-color" media="(prefers-color-scheme: dark)" content="black">
</head>
<body>
    <header id="top" class="navbar navbar-expand-md top-nav incubator"><svg xmlns="http://www.w3.org/2000/svg" class="d-none"><symbol id="check2" viewbox="0 0 16 16"><path d="M13.854 3.646a.5.5 0 0 1 0 .708l-7 7a.5.5 0 0 1-.708 0l-3.5-3.5a.5.5 0 1 1 .708-.708L6.5 10.293l6.646-6.647a.5.5 0 0 1 .708 0z"></path></symbol><symbol id="circle-half" viewbox="0 0 16 16"><path d="M8 15A7 7 0 1 0 8 1v14zm0 1A8 8 0 1 1 8 0a8 8 0 0 1 0 16z"></path></symbol><symbol id="moon-stars-fill" viewbox="0 0 16 16"><path d="M6 .278a.768.768 0 0 1 .08.858 7.208 7.208 0 0 0-.878 3.46c0 4.021 3.278 7.277 7.318 7.277.527 0 1.04-.055 1.533-.16a.787.787 0 0 1 .81.316.733.733 0 0 1-.031.893A8.349 8.349 0 0 1 8.344 16C3.734 16 0 12.286 0 7.71 0 4.266 2.114 1.312 5.124.06A.752.752 0 0 1 6 .278z"></path><path d="M10.794 3.148a.217.217 0 0 1 .412 0l.387 1.162c.173.518.579.924 1.097 1.097l1.162.387a.217.217 0 0 1 0 .412l-1.162.387a1.734 1.734 0 0 0-1.097 1.097l-.387 1.162a.217.217 0 0 1-.412 0l-.387-1.162A1.734 1.734 0 0 0 9.31 6.593l-1.162-.387a.217.217 0 0 1 0-.412l1.162-.387a1.734 1.734 0 0 0 1.097-1.097l.387-1.162zM13.863.099a.145.145 0 0 1 .274 0l.258.774c.115.346.386.617.732.732l.774.258a.145.145 0 0 1 0 .274l-.774.258a1.156 1.156 0 0 0-.732.732l-.258.774a.145.145 0 0 1-.274 0l-.258-.774a1.156 1.156 0 0 0-.732-.732l-.774-.258a.145.145 0 0 1 0-.274l.774-.258c.346-.115.617-.386.732-.732L13.863.1z"></path></symbol><symbol id="sun-fill" viewbox="0 0 16 16"><path d="M8 12a4 4 0 1 0 0-8 4 4 0 0 0 0 8zM8 0a.5.5 0 0 1 .5.5v2a.5.5 0 0 1-1 0v-2A.5.5 0 0 1 8 0zm0 13a.5.5 0 0 1 .5.5v2a.5.5 0 0 1-1 0v-2A.5.5 0 0 1 8 13zm8-5a.5.5 0 0 1-.5.5h-2a.5.5 0 0 1 0-1h2a.5.5 0 0 1 .5.5zM3 8a.5.5 0 0 1-.5.5h-2a.5.5 0 0 1 0-1h2A.5.5 0 0 1 3 8zm10.657-5.657a.5.5 0 0 1 0 .707l-1.414 1.415a.5.5 0 1 1-.707-.708l1.414-1.414a.5.5 0 0 1 .707 0zm-9.193 9.193a.5.5 0 0 1 0 .707L3.05 13.657a.5.5 0 0 1-.707-.707l1.414-1.414a.5.5 0 0 1 .707 0zm9.193 2.121a.5.5 0 0 1-.707 0l-1.414-1.414a.5.5 0 0 1 .707-.707l1.414 1.414a.5.5 0 0 1 0 .707zM4.464 4.465a.5.5 0 0 1-.707 0L2.343 3.05a.5.5 0 1 1 .707-.707l1.414 1.414a.5.5 0 0 1 0 .708z"></path></symbol></svg><a class="visually-hidden-focusable skip-link" href="#main-content">Skip to main content</a>
  <div class="container-fluid top-nav-container">
    <div class="col-md-8">
      <div class="large-logo">
        <img id="incubator-logo" alt="Carpentries Incubator" src="../assets/images/incubator-logo.svg"><span class="badge text-bg-danger">
          <abbr title="This lesson is in the pre-alpha phase, which means that it is in early development, but has not yet been taught.">
            <a href="https://cdh.carpentries.org/the-lesson-life-cycle.html#early-development-pre-alpha-through-alpha" class="external-link alert-link">
              <i aria-hidden="true" class="icon" data-feather="alert-octagon" style="border-radius: 5px"></i>
              Pre-Alpha
            </a>
            <span class="visually-hidden">This lesson is in the pre-alpha phase, which means that it is in early development, but has not yet been taught.</span>
          </abbr>
        </span>

      </div>
    </div>
    <div class="selector-container">
      <div id="theme-selector">
        <li class="nav-item dropdown" id="theme-button-list">
          <button class="btn btn-link nav-link px-0 px-lg-2 dropdown-toggle d-flex align-items-center" id="bd-theme" type="button" aria-expanded="false" data-bs-toggle="dropdown" data-bs-display="static" aria-label="Toggle theme (auto)">
            <svg class="bi my-1 theme-icon-active"><use href="#circle-half"></use></svg><i data-feather="chevron-down"></i>
          </button>
          <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="bd-theme-text">
<li>
              <button type="button" class="btn dropdown-item d-flex align-items-center" data-bs-theme-value="light" aria-pressed="false">
                <svg class="bi me-2 theme-icon"><use href="#sun-fill"></use></svg>
                Light
                <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
            </li>
            <li>
              <button type="button" class="btn dropdown-item d-flex align-items-center" data-bs-theme-value="dark" aria-pressed="false">
                <svg class="bi me-2 theme-icon"><use href="#moon-stars-fill"></use></svg>
                Dark
                <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
            </li>
            <li>
              <button type="button" class="btn dropdown-item d-flex align-items-center active" data-bs-theme-value="auto" aria-pressed="true">
                <svg class="bi me-2 theme-icon"><use href="#circle-half"></use></svg>
                Auto
                <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
            </li>
          </ul>
</li>
      </div>

      <div class="dropdown" id="instructor-dropdown">
        <button class="btn btn-secondary dropdown-toggle bordered-button" type="button" id="dropdownMenu1" data-bs-toggle="dropdown" aria-expanded="false">
          <i aria-hidden="true" class="icon" data-feather="eye"></i> Instructor View <i data-feather="chevron-down"></i>
        </button>
        <ul class="dropdown-menu" aria-labelledby="dropdownMenu1">
<li><button class="dropdown-item" type="button" onclick="window.location.href='../aio.html';">Learner View</button></li>
        </ul>
</div>
    </div>
  </div>
  <hr></header><nav class="navbar navbar-expand-xl bottom-nav incubator" aria-label="Main Navigation"><div class="container-fluid nav-container">
    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle Navigation">
      <span class="navbar-toggler-icon"></span>
      <span class="menu-title">Menu</span>
    </button>
    <div class="nav-logo">
      <img class="small-logo" alt="Carpentries Incubator" src="../assets/images/incubator-logo-sm.svg">
</div>
    <div class="lesson-title-md">
      Medical Image Processing
    </div>
    <div class="search-icon-sm">
      <!-- TODO: do not show until we have search
        <i role="img" aria-label="Search the All In One page" data-feather="search"></i>
      -->
    </div>
    <div class="desktop-nav">
      <ul class="navbar-nav me-auto mb-2 mb-lg-0">
<li class="nav-item">
          <span class="lesson-title">
            Medical Image Processing
          </span>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="../instructor/key-points.html">Key Points</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="../instructor/instructor-notes.html">Instructor Notes</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="../instructor/images.html">Extract All Images</a>
        </li>
        <li class="nav-item dropdown">
          <button class="nav-link dropdown-toggle" id="navbarDropdown" data-bs-toggle="dropdown" aria-expanded="false">
            More <i data-feather="chevron-down"></i>
          </button>
          <ul class="dropdown-menu" aria-labelledby="navbarDropdown">
<hr>
<li><a class="dropdown-item" href="reference.html">Glossary</a></li>
          </ul>
</li>
      </ul>
</div>
    <!--
    <form class="d-flex col-md-2 search-form">
      <fieldset disabled>
      <input class="form-control me-2 searchbox" type="search" placeholder="" aria-label="">
        <button class="btn btn-outline-success tablet-search-button"  type="submit">
          <i class="search-icon" data-feather="search" role="img" aria-label="Search the All In One page"></i>
        </button>
      </fieldset>
    </form>
    -->
    <a id="search-button" class="btn btn-primary" href="../instructor/aio.html" role="button" aria-label="Search the All In One page">Search the All In One page</a>
  </div>
<!--/div.container-fluid -->
</nav><div class="col-md-12 mobile-title">
  Medical Image Processing
</div>

<aside class="col-md-12 lesson-progress"><div style="width: %" class="percentage">
    %
  </div>
  <div class="progress incubator">
    <div class="progress-bar incubator" role="progressbar" style="width: %" aria-valuenow="" aria-label="Lesson Progress" aria-valuemin="0" aria-valuemax="100">
    </div>
  </div>
</aside><div class="container">
      <div class="row">
        <!-- START: inst/pkgdown/templates/navbar.html -->
<div id="sidebar-col" class="col-lg-4">
  <div id="sidebar" class="sidebar">
      <nav aria-labelledby="flush-headingEleven"><button role="button" aria-label="close menu" alt="close menu" aria-expanded="true" aria-controls="sidebar" class="collapse-toggle" data-collapse="Collapse " data-episodes="Episodes ">
          <i class="search-icon" data-feather="x" role="img"></i>
        </button>
        <div class="sidebar-inner">
          <div class="row mobile-row" id="theme-row-mobile">
            <div class="col" id="theme-selector">
              <li class="nav-item dropdown" id="theme-button-list">
                <button class="btn btn-link nav-link px-0 px-lg-2 dropdown-toggle d-flex align-items-center" id="bd-theme" type="button" aria-expanded="false" data-bs-toggle="dropdown" data-bs-display="static" aria-label="Toggle theme (auto)">
                  <svg class="bi my-1 theme-icon-active"><use href="#circle-half"></use></svg><span class="d-lg-none ms-1" id="bd-theme-text">Toggle Theme</span>
                </button>
                <ul class="dropdown-menu dropdown-menu-right" aria-labelledby="bd-theme-text">
<li>
                    <button type="button" class="btn dropdown-item d-flex align-items-center" data-bs-theme-value="light" aria-pressed="false">
                      <svg class="bi me-2 theme-icon"><use href="#sun-fill"></use></svg>
                      Light
                      <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
                  </li>
                  <li>
                    <button type="button" class="btn dropdown-item d-flex align-items-center" data-bs-theme-value="dark" aria-pressed="false">
                      <svg class="bi me-2 theme-icon"><use href="#moon-stars-fill"></use></svg>
                      Dark
                      <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
                  </li>
                  <li>
                    <button type="button" class="btn dropdown-item d-flex align-items-center active" data-bs-theme-value="auto" aria-pressed="true">
                      <svg class="bi me-2 theme-icon"><use href="#circle-half"></use></svg>
                      Auto
                      <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
                  </li>
                </ul>
</li>
            </div>
          </div>
          <div class="row mobile-row">
            <div class="col">
              <div class="sidenav-view-selector">
                <div class="accordion accordion-flush" id="accordionFlush9">
                  <div class="accordion-item">
                    <h2 class="accordion-header" id="flush-headingNine">
                      <button class="accordion-button collapsed" id="instructor" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapseNine" aria-expanded="false" aria-controls="flush-collapseNine">
                        <i id="eye" aria-hidden="true" class="icon" data-feather="eye"></i> Instructor View
                      </button>
                    </h2>
                    <div id="flush-collapseNine" class="accordion-collapse collapse" aria-labelledby="flush-headingNine" data-bs-parent="#accordionFlush2">
                      <div class="accordion-body">
                        <a href="../aio.html">Learner View</a>
                      </div>
                    </div>
                  </div>
<!--/div.accordion-item-->
                </div>
<!--/div.accordion-flush-->
              </div>
<!--div.sidenav-view-selector -->
            </div>
<!--/div.col -->

            <hr>
</div>
<!--/div.mobile-row -->

          <div class="accordion accordion-flush" id="accordionFlush11">
            <div class="accordion-item">

              <button id="chapters" class="accordion-button show" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapseEleven" aria-expanded="false" aria-controls="flush-collapseEleven">
                <h2 class="accordion-header chapters" id="flush-headingEleven">
                  EPISODES
                </h2>
              </button>
              <div id="flush-collapseEleven" class="accordion-collapse show collapse" aria-labelledby="flush-headingEleven" data-bs-parent="#accordionFlush11">

                <div class="accordion-body">
                  <div class="accordion accordion-flush" id="accordionFlush1">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading1">
        <a href="index.html">Summary and Schedule</a>
    </div>
<!--/div.accordion-header-->

  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush2">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading2">
        <a href="introduction.html">1. Course Introduction</a>
    </div>
<!--/div.accordion-header-->

  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush3">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading3">
        <a href="medical_imaging.html">2. Medical Imaging Modalities</a>
    </div>
<!--/div.accordion-header-->

  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush4">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading4">
        <a href="mri.html">3. Working with MRI</a>
    </div>
<!--/div.accordion-header-->

  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush5">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading5">
        <a href="simpleitk.html">4. Registration and Segmentation with SITK</a>
    </div>
<!--/div.accordion-header-->

  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush6">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading6">
        <a href="images_ml.html">5. Preparing Images for Machine Learning</a>
    </div>
<!--/div.accordion-header-->

  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush7">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading7">
        <a href="anonymization.html">6. Anonymizing Medical Images</a>
    </div>
<!--/div.accordion-header-->

  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush8">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading8">
        <a href="generative_ai.html">7. Generative AI in Medical Imaging</a>
    </div>
<!--/div.accordion-header-->

  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

                </div>
              </div>
            </div>

            <hr class="half-width">
<div class="accordion accordion-flush lesson-resources" id="accordionFlush12">
              <div class="accordion-item">
                <h2 class="accordion-header" id="flush-headingTwelve">
                  <button class="accordion-button collapsed" id="lesson-resources" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapseTwelve" aria-expanded="false" aria-controls="flush-collapseTwelve">
                    RESOURCES
                  </button>
                </h2>
                <div id="flush-collapseTwelve" class="accordion-collapse collapse" aria-labelledby="flush-headingTwelve" data-bs-parent="#accordionFlush12">
                  <div class="accordion-body">
                    <ul>
<li>
                        <a href="../instructor/key-points.html">Key Points</a>
                      </li>
                      <li>
                        <a href="../instructor/instructor-notes.html">Instructor Notes</a>
                      </li>
                      <li>
                        <a href="../instructor/images.html">Extract All Images</a>
                      </li>
                      <hr>
<li><a class="dropdown-item" href="reference.html">Glossary</a></li>
                    </ul>
</div>
                </div>
              </div>
            </div>
            <hr class="half-width lesson-resources">
<a href="../instructor/aio.html">See all in one page</a>


            <hr class="d-none d-sm-block d-md-none">
<div class="d-grid gap-1">

            </div>
          </div>
<!-- /div.accordion -->
        </div>
<!-- /div.sidebar-inner -->
      </nav>
</div>
<!-- /div.sidebar -->
  </div>
<!-- /div.sidebar-col -->
<!-- END:   inst/pkgdown/templates/navbar.html-->

        <!-- START: inst/pkgdown/templates/content-extra.html -->
  <div class="col-xl-8 col-lg-12 primary-content">
    <main id="main-content" class="main-content"><div class="container lesson-content">
        
        
<section id="aio-introduction"><p>Content from <a href="introduction.html">Course Introduction</a></p>
<hr>
<p>Last updated on 2024-08-14 |

        <a href="https://github.com/esciencecenter-digital-skills/medical-image-processing/edit/main/episodes/introduction.md" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<p>Estimated time: <i aria-hidden="true" data-feather="clock"></i> 10 minutes</p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul>
<li>How can I use this course to be better at my research?</li>
</ul>
</div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul>
<li>Explain how to get the most from the course</li>
<li>Demonstrate and explain how the course will be laid out</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<p>This is a lesson created in the style of <a href="https://datacarpentry.org/" class="external-link">The Carpentries</a>. It is written
with the assumption that you already possess skills in terms of git,
Python and basic image processing.</p>
<p>The interpretation of medical images for clinical purposes requires
skills that take highly trained professionals such as nuclear medicine
specialists and radiologists many years to master. This course does not
aim to improve such interpretive skills, but rather to enhance the
computational skills needed to answer research questions involving
medical images.</p>
<p>Some examples of the kinds of research questions that can be answered
are:</p>
<ul>
<li><p>Can we predict from brain MRIs when patients will become demented
before they do?</p></li>
<li><p>Can we build machine learning models on ultrasound data which can
aid in the detection of neuromuscular diseases?</p></li>
<li><p>Are there observable anatomical differences in the brains of
autistic people at a population level?</p></li>
<li><p>Can we use existing medical imaging to screen for underdiagnosed
conditions like osteoporosis?</p></li>
</ul>
<p>You are in all likelihood here because you have a research question
which can be answered with the processing and analysis of medical
images. This course is meant to aid you.</p>
<p><em>Note that all figures and data presented are licensed under
open-source terms.</em></p>
<div id="challenge-can-you-do-it" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="challenge-can-you-do-it" class="callout-inner">
<h3 class="callout-title">Challenge: Can You Do It?</h3>
<div class="callout-content">
<p>What is the way to use the challenges and question?</p>
</div>
</div>
</div>
<div id="accordionSolution1" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution1" aria-expanded="false" aria-controls="collapseSolution1">
  <h4 class="accordion-header" id="headingSolution1"> Show me the solution </h4>
</button>
<div id="collapseSolution1" class="accordion-collapse collapse" aria-labelledby="headingSolution1" data-bs-parent="#accordionSolution1">
<div class="accordion-body">
<p>Do not peek, try to solve it yourself. The effort will pay off.</p>
</div>
</div>
</div>
</div></section><section id="aio-medical_imaging"><p>Content from <a href="medical_imaging.html">Medical Imaging Modalities</a></p>
<hr>
<p>Last updated on 2024-10-14 |

        <a href="https://github.com/esciencecenter-digital-skills/medical-image-processing/edit/main/episodes/medical_imaging.md" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<p>Estimated time: <i aria-hidden="true" data-feather="clock"></i> 60 minutes</p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul>
<li>What are the common different types of diagnostic imaging?</li>
<li>What sorts of computational challenges do they present?</li>
</ul>
</div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul>
<li>Explain various common types of medical images</li>
<li>Explain at a high level how images’ metadata is created and
organized</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<section><h2 class="section-heading" id="introduction">Introduction<a class="anchor" aria-label="anchor" href="#introduction"></a>
</h2>
<hr class="half-width">
<p>Medical imaging uses many technologies including X-rays, computed
tomography (CT), magnetic resonance imaging (MRI), ultrasound, positron
emission tomography (PET) and microscopy. Although there are tendencies
to use certain technologies, or modalities to answer certain clinical
questions, many modalities may provide information of interest in terms
of research questions. In order to work with digital images at scale we
need to use information technology. We receive images in certain types
of files, e.g., an x-ray stored at the hospital in DICOM format, but the
image itself is contained in a JPEG inside the DICOM as a 2D-array.
Understanding all the kinds of files we are dealing with and how the
images within them were generated can help us deal with them
computationally.</p>
<p>Conceptually, we can think of medical images as signals. These
signals need various kinds of processing before they are ‘readable’ by
humans or by many of the algorithms we write.</p>
<p>While thinking about how the information from these signals is stored
in different file types may seem less exciting than what the “true
information” or final diagnosis from the image was, it is necessary to
understand this to make the best algorithms possible. For example, a lot
of hospital images are essentially JPEGs. This has implications in terms
of image quality as we manipulate and resize the images.</p>
<p>The details of various forms of imaging will be covered in a lecture
with slides that accompanies this episode. Below are a few summaries
about various ultra-common imaging types. Keep in mind that
manufacturers may have specificities in terms of file types not covered
here, and there are many possibilities in terms of how images could
potentially be stored. Here we will discuss what is common to get in
terms of files given to researchers.</p>
</section><section><h2 class="section-heading" id="x-rays">X-Rays<a class="anchor" aria-label="anchor" href="#x-rays"></a>
</h2>
<hr class="half-width">
<p>Historically, x-rays were the first common form of medical imaging.
The diagram below should help you visualize how they are produced. The
signal from an x-ray generator crosses the subject. Some tissues
attenuate the radiation more than others. The signal is captured by an
x-ray detector (you can think of this metaphorically like photographic
film) on the other side of the subject.</p>
<figure><img src="../fig/x_ray_dia.png" alt="X-ray image creation schematic." class="figure mx-auto d-block"><div class="figcaption">Schematic of x-ray image creation.</div>
</figure><p>As you can imagine if you only have one view in an X-ray every object
in the line of radiation from the generator is superimposed on every
object below it. Even in the days of film X-rays often two views would
be made. In the case of chest X-rays these could be a
posteroanterior(PA) view and a lateral view. In the case of joints the
views may be specific, however remember that in each view objects in the
same line between the generator and receptor will be superimposed.</p>
<p><img src="../fig/knee_gallery.jpeg" alt="Knee series." class="figure"><em>image
courtesy of Radiopaedia, author and ID on image</em></p>
<p>Modern x-rays are born digital. No actual “film” is produced, rather
a DICOM file which typically contains arrays in JPEG files.</p>
<p>We could use the metaphor of a wrapped present here. The DICOM file
contains metadata around the image data, wrapping it. The image data
itself is a bunch of 2D-arrays, but these have been organized to a
specific shape - they are “boxed” by JPEG files. JPEG is a container
format. There are JPEG files (emphasis on the plural) in a single DICOM
file which typically contain images of the same body part with different
angles of acquisition.</p>
<p>We can take x-rays from any angle and even do them repeatedly, and
this allows for fluoroscopy. Because fluoroscopy adds a time dimension
to X-ray the data becomes 3-dimensional, possessing an X, Y and time
dimension. Below is a fluoroscopy image of a patient swallowing
barium.</p>
<p><img src="../fig/fluoro.gif" alt="Fluorsocopy." class="figure"><em>image courtesy of
Ptrump16, CC BY-SA 4.0 <a href="https://creativecommons.org/licenses/by-sa/4.0" class="external-link uri">https://creativecommons.org/licenses/by-sa/4.0</a>, via
Wikimedia Commons</em></p>
<p>Fluoroscopy images are stored in a DICOM but can be displayed as
movies because they are typically cine-files. Cine- is a file format
that lets you store images in sequence with a frame rate.</p>
</section><section><h2 class="section-heading" id="computed-tomography-and-tomosynthesis">Computed Tomography and Tomosynthesis<a class="anchor" aria-label="anchor" href="#computed-tomography-and-tomosynthesis"></a>
</h2>
<hr class="half-width">
<p>There are several kinds of tomography. This technique produces
3D-images, made of voxels, that allow us to see structures within a
subject. CTs are extremely common, and helpful for many diagnostic
questions, but have certain costs in terms of not only time and money,
but also radiation to patients.</p>
<p>CTs and tomosynthetic images are produced with similar technology.
One key difference is that in a CT the image is based on a 360 degree
capture of the signal. You can conceptualize this as a spinning donut
with the generator and receptor opposite to each other. The raw data of
a CT is a <a href="reference.html#sinogram">sinogram</a>. Only by
processing this data do we get what most people would recognize as a CT.
At this level of processing there are already choices effecting the data
we get. Let’s examine two ways to process our sinograms:</p>
<div class="codewrapper sourceCode" id="cb1">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-2"><a href="#cb1-2" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-3"><a href="#cb1-3" tabindex="-1"></a><span class="im">from</span> skimage.transform <span class="im">import</span> iradon</span>
<span id="cb1-4"><a href="#cb1-4" tabindex="-1"></a><span class="im">from</span> skimage.transform <span class="im">import</span> iradon_sart</span>
<span id="cb1-5"><a href="#cb1-5" tabindex="-1"></a></span>
<span id="cb1-6"><a href="#cb1-6" tabindex="-1"></a><span class="co"># load a sinogram of a simple phantom of the head</span></span>
<span id="cb1-7"><a href="#cb1-7" tabindex="-1"></a>sino <span class="op">=</span> np.load(<span class="st">'data/Schepp_Logan_sinogram.npy'</span>)</span>
<span id="cb1-8"><a href="#cb1-8" tabindex="-1"></a><span class="co"># make a filtered back projection reconstruction</span></span>
<span id="cb1-9"><a href="#cb1-9" tabindex="-1"></a>theta <span class="op">=</span> np.linspace(<span class="fl">0.0</span>, <span class="fl">180.0</span>, <span class="bu">max</span>(sino.shape), endpoint<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb1-10"><a href="#cb1-10" tabindex="-1"></a>reconstruction_fbp <span class="op">=</span> iradon(sino, theta<span class="op">=</span>theta, filter_name<span class="op">=</span><span class="st">'ramp'</span>)</span>
<span id="cb1-11"><a href="#cb1-11" tabindex="-1"></a><span class="co"># make a reconstruction with Simultaneous Algebraic Reconstruction Technique</span></span>
<span id="cb1-12"><a href="#cb1-12" tabindex="-1"></a>reconstruction_sart <span class="op">=</span> iradon_sart(sino, theta<span class="op">=</span>theta)</span>
<span id="cb1-13"><a href="#cb1-13" tabindex="-1"></a><span class="co"># plot with matplotlib</span></span>
<span id="cb1-14"><a href="#cb1-14" tabindex="-1"></a>fig, (ax1, ax2, ax3, ax4) <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">4</span>, figsize<span class="op">=</span>(<span class="dv">9</span>, <span class="dv">5</span>), sharex<span class="op">=</span><span class="va">True</span>, sharey<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb1-15"><a href="#cb1-15" tabindex="-1"></a>ax1.set_title(<span class="st">"Sinogram"</span>)</span>
<span id="cb1-16"><a href="#cb1-16" tabindex="-1"></a>ax1.imshow(sino, cmap<span class="op">=</span>plt.cm.Greys_r,)</span>
<span id="cb1-17"><a href="#cb1-17" tabindex="-1"></a>ax2.set_title(<span class="st">"Reconstruction</span><span class="ch">\n</span><span class="st">Filtered back projection"</span>)</span>
<span id="cb1-18"><a href="#cb1-18" tabindex="-1"></a>ax2.imshow(reconstruction_fbp, cmap<span class="op">=</span>plt.cm.Greys_r)</span>
<span id="cb1-19"><a href="#cb1-19" tabindex="-1"></a>ax3.set_title(<span class="st">"Reconstruction</span><span class="ch">\n</span><span class="st">SART"</span>)</span>
<span id="cb1-20"><a href="#cb1-20" tabindex="-1"></a>ax3.imshow(reconstruction_sart, cmap<span class="op">=</span>plt.cm.Greys_r)</span>
<span id="cb1-21"><a href="#cb1-21" tabindex="-1"></a>ax4.set_title(<span class="st">"Difference</span><span class="ch">\n</span><span class="st"> between reconstructions"</span>)</span>
<span id="cb1-22"><a href="#cb1-22" tabindex="-1"></a>ax4.imshow(reconstruction_sart <span class="op">-</span> reconstruction_fbp, cmap<span class="op">=</span>plt.cm.Greys_r)</span>
<span id="cb1-23"><a href="#cb1-23" tabindex="-1"></a></span>
<span id="cb1-24"><a href="#cb1-24" tabindex="-1"></a>plt.show()</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code></code></pre>
</div>
<figure><img src="../fig/output_sinogram_plus.png" alt="Graph of sinogram and processed images." class="figure mx-auto d-block"><div class="figcaption">singogram and processed images.</div>
</figure><p><em>Images generated from the <a href="https://doi.org/10.1109/TNS.1974.6499235" class="external-link">Shepp–Logan phantom</a>
</em></p>
<p>While you may get an already processed CT (Some commercial machines
come with proprietary reconstruction algorithms which will already have
been executed), it is not uncommon to get CTs as DICOM CT projection
data (DICOM-CT-PD) files which can then be processed before viewing, or
in some cases stored off as other file types. As shown in the code above
there is more than one way to process the data into a radiologist
friendly CT. Filtered Back Projection or Algebraic Reconstruction
methods are shown but there are other methods such as iterative
reconstruction, convolution back projection and even deep learning based
methods.</p>
<p>Tomosynthesis makes X-ray based images using a limited angle instead
of going all the way around the patient as in CT. The data from a
tomosynthetic image is then processed so that you get multiple angles
visible. This gets around the issue of overlapping objects in a plain
film X-ray. In both the case of CT and tomosynthesis, the image output
is made by processing the originally acquired data. Although most
researchers work with already processed images, it is important to keep
in mind that in theory the originally acquired data can be processed in
a variety of ways.</p>
</section><section><h2 class="section-heading" id="ultrasounds">Ultrasounds<a class="anchor" aria-label="anchor" href="#ultrasounds"></a>
</h2>
<hr class="half-width">
<p>Ultrasounds can produce multiple complex types of images. Ultrasound
use high frequency sound waves, sent and captured from a piezoelectric
probe (also known as a transducer) to get images.</p>
<p>Just as different tissues attenuate radiation differently, different
tissues attenuate the sound waves differently. To be more precise
different tissues reflect and absorb the waves differently and this can
help us create images after some processing of the signal.</p>
<p>These images can be captured in rapid succession over time, so they
can be saved as cine-files inside DICOMs. On the other hand, the
sonographer can choose to record only a single ‘frame’, in which case a
2D-array will ultimately be saved.</p>
<p>Typically, sonographers produce a lot of <a href="reference.html#b">B-mode</a> images, but B-mode is far from the
only type of ultrasounds. M-mode or motion mode, like the cine-files in
B-mode, can also capture motion, but puts it into a a single 2D-array of
one line of the image over time. In the compound image below you can see
a B-mode 2D-image and an M-mode made on the line in it.</p>
<figure><img src="../fig/MItral_Valve_M_Mode.jpg" alt="Mitral valve prolapse." class="figure mx-auto d-block"><div class="figcaption">Image of mitral valve prolapse from Cafer
Zorkun, MD, PhD on wikidoc.org with creative commons lisence.</div>
</figure><div id="what-are-some-disadvantages-to-ultrasounds-in-terms-of-computational-analysis" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<div id="what-are-some-disadvantages-to-ultrasounds-in-terms-of-computational-analysis" class="callout-inner">
<h3 class="callout-title">What Are Some Disadvantages to Ultrasounds in Terms of Computational Analysis?</h3>
<div class="callout-content">
<p>Ultrasounds images are operator-dependent, often with embedded
patient data, and the settings and patients’ positions can vary
widely.</p>
</div>
</div>
</div>
<div id="challenge-how-to-reduce-these-problems" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="challenge-how-to-reduce-these-problems" class="callout-inner">
<h3 class="callout-title">Challenge: How to Reduce These Problems?</h3>
<div class="callout-content">
<p>How can we optimize research involving ultrasounds in terms of the
challenges above?</p>
</div>
</div>
</div>
<div id="accordionSolution1" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution1" aria-expanded="false" aria-controls="collapseSolution1">
  <h4 class="accordion-header" id="headingSolution1"> Show me the solution </h4>
</button>
<div id="collapseSolution1" class="accordion-collapse collapse" aria-labelledby="headingSolution1" data-bs-parent="#accordionSolution1">
<div class="accordion-body">
<p>Possible solutions include:</p>
<ul>
<li>Reviewing metadata on existing images so it can be matched by
machine type</li>
<li>Training technicians to use standardized settings only</li>
<li>Creating a machine set only on one power setting</li>
<li>Image harmonization/normalization algorithms</li>
</ul>
</div>
</div>
</div>
</div>
</section><section><h2 class="section-heading" id="magnetic-resonance-imaging">Magnetic Resonance Imaging<a class="anchor" aria-label="anchor" href="#magnetic-resonance-imaging"></a>
</h2>
<hr class="half-width">
<p>MRIs are images made by utilizing some fairly complicated physics in
terms of what we can do to protons (abundant in human tissue) with
magnets and radiofrequency waves, and how we capture their signal.
Different ordering and timing of radiofrequency pulses and different
magnetic gradients give us different MRI sequences. The actual signal on
an anatomical MRI needs to be processed typically via Fourier transforms
and some other computational work before it is recognizable as anatomy.
The raw data is reffered to as the k-space data, and this can be kept in
vendor specific formats or open common formats, e.g., ISMRMRD
(International Society of Magnetic Resonance MR Raw Data). In practice,
we rarely use the k-space data (unless perhaps we are medical
physicists) for research on medical pathology. Nonetheless researchers
in new sequences for MRI will be very interested in such data, and
typically getting the fastest transformations of it possible. There are
many ways the raw data could be transformed or used to produce an MRI.
While an inverse Fourier transform is typical, a Hartley transform could
be used, and some scientists even use deep learning based methods. Let’s
look at k-space with a viridis color map:</p>
<figure><img src="../fig/k-space.png" alt="K-space." class="figure mx-auto d-block"><div class="figcaption">k-space image.</div>
</figure><p><em>Sourced from the FastMRI data set of NYU, 2024 (Knoll et al
Radiol Artif Intell. 2020 Jan 29;2(1):e190007. doi:
10.1148/ryai.2020190007.https://pubs.rsna.org/doi/10.1148/ryai.2020190007
and the arXiv paper, <a href="https://arxiv.org/abs/1811.08839" class="external-link uri">https://arxiv.org/abs/1811.08839</a>.)</em></p>
<p>Let’s do an example of a k-space transform</p>
<div class="codewrapper sourceCode" id="cb3">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" tabindex="-1"></a>slice_kspace <span class="op">=</span> np.load(<span class="st">'data/slice_kspace.npy'</span>)</span>
<span id="cb3-2"><a href="#cb3-2" tabindex="-1"></a><span class="co"># show shape</span></span>
<span id="cb3-3"><a href="#cb3-3" tabindex="-1"></a><span class="bu">print</span>(slice_kspace.shape)</span>
<span id="cb3-4"><a href="#cb3-4" tabindex="-1"></a><span class="co"># show type</span></span>
<span id="cb3-5"><a href="#cb3-5" tabindex="-1"></a><span class="bu">print</span>(<span class="bu">type</span>(slice_kspace))</span>
<span id="cb3-6"><a href="#cb3-6" tabindex="-1"></a><span class="co"># print type of an example pixel</span></span>
<span id="cb3-7"><a href="#cb3-7" tabindex="-1"></a><span class="bu">print</span>(<span class="bu">type</span>(slice_kspace[<span class="dv">3</span>,<span class="dv">3</span>]))</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>(640, 368)
&lt;class 'numpy.ndarray'&gt;
&lt;class 'numpy.complex64'&gt;</code></pre>
</div>
<p>Note we have an array that contains numbers with an imaginary element
therefore the type is complex. We can extract and graph the real and
imaginary parts, and also graph a transformation:</p>
<div class="codewrapper sourceCode" id="cb5">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" tabindex="-1"></a>real_from_slice_kspace <span class="op">=</span> slice_kspace.real</span>
<span id="cb5-2"><a href="#cb5-2" tabindex="-1"></a>imaginary_from_slice_kspace <span class="op">=</span> slice_kspace.imag</span>
<span id="cb5-3"><a href="#cb5-3" tabindex="-1"></a><span class="co"># make an inverse fourier</span></span>
<span id="cb5-4"><a href="#cb5-4" tabindex="-1"></a><span class="kw">def</span> inverse_fft2_shift(kspace):</span>
<span id="cb5-5"><a href="#cb5-5" tabindex="-1"></a>    <span class="cf">return</span> np.fft.fftshift(np.fft.ifft2(np.fft.ifftshift(kspace, axes<span class="op">=</span>(<span class="op">-</span><span class="dv">2</span>,<span class="op">-</span><span class="dv">1</span>)), norm<span class="op">=</span><span class="st">'ortho'</span>),axes<span class="op">=</span>(<span class="op">-</span><span class="dv">2</span>,<span class="op">-</span><span class="dv">1</span>))</span>
<span id="cb5-6"><a href="#cb5-6" tabindex="-1"></a><span class="co"># graph both    </span></span>
<span id="cb5-7"><a href="#cb5-7" tabindex="-1"></a>fig, (ax1, ax2, ax3) <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">3</span>, figsize<span class="op">=</span>(<span class="dv">9</span>, <span class="dv">5</span>), sharex<span class="op">=</span><span class="va">True</span>, sharey<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb5-8"><a href="#cb5-8" tabindex="-1"></a>ax1.set_title(<span class="st">"K-space real part"</span>)</span>
<span id="cb5-9"><a href="#cb5-9" tabindex="-1"></a>ax1.imshow(real_from_slice_kspace, cmap<span class="op">=</span>plt.cm.Greys_r,)</span>
<span id="cb5-10"><a href="#cb5-10" tabindex="-1"></a>ax2.set_title(<span class="st">"K-space imaginary part"</span>)</span>
<span id="cb5-11"><a href="#cb5-11" tabindex="-1"></a>ax2.imshow(imaginary_from_slice_kspace, cmap<span class="op">=</span>plt.cm.Greys_r,)</span>
<span id="cb5-12"><a href="#cb5-12" tabindex="-1"></a>ax3.set_title(<span class="st">"Transformed K-space"</span>)</span>
<span id="cb5-13"><a href="#cb5-13" tabindex="-1"></a>ax3.imshow(np.<span class="bu">abs</span>(inverse_fft2_shift(slice_kspace)), cmap<span class="op">=</span>plt.cm.Greys_r)</span></code></pre>
</div>
<figure><img src="../fig/kspacetransform.png" alt="Graph of k space and processed images." class="figure mx-auto d-block"><div class="figcaption">K space and processed images.</div>
</figure><p>Hopefully you can see that our K-space like our sinogram is not so
human-readable, and the transformed image is recognizable as a knee. A
transformed type of image, one a radiologist will be able to read, is
often what we are given. This final product we are used to looking at is
such a post-processed 3D-array wrapped inside a DICOM file. We can
transform the image, and parts of the metadata, to a variety of file
types commonly used in research. These file types will be covered in
more detail later in the course.</p>
<div id="ct-versus-mri" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<div id="ct-versus-mri" class="callout-inner">
<h3 class="callout-title">CT versus MRI</h3>
<div class="callout-content">
<p>After processing both CT and standard MRIs will give a 3D image.
However, it is important to understand that there are differences. An
standard MRI sequence will give better differentiation between various
soft tissues, wheras a CT will provide better images of bones. CTs can
be acquired more quickly and cheaply, but have the hidden cost of
radiation.</p>
</div>
</div>
</div>
<div id="challenge-ct-mris-and-artifacts" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="challenge-ct-mris-and-artifacts" class="callout-inner">
<h3 class="callout-title">Challenge: CT, MRIs and Artifacts?</h3>
<div class="callout-content">
<p>You are researching tumors in adults. You team up with the best
radiologist you can find and ask for imaging. She hands you DICOM files
of some patient MRIs and CTs, and states “These are exactly the images I
use. I have checked that they are all without artifacts” You have the
images straight from the radiologist, could there potentially be any
artifacts?</p>
</div>
</div>
</div>
<div id="accordionSolution2" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution2" aria-expanded="false" aria-controls="collapseSolution2">
  <h4 class="accordion-header" id="headingSolution2"> Show me the solution </h4>
</button>
<div id="collapseSolution2" class="accordion-collapse collapse" aria-labelledby="headingSolution2" data-bs-parent="#accordionSolution2">
<div class="accordion-body">
<p>In an absolute total sense, they could have artifacts. Both CT and
MRI are reconstructed from original data, and the reconstruction will
introduce artifacts. The radiologist thinks of artifacts as things like
motion blurring from when the patient moves or wrap-around in MRIs when
the field of view was set too small. These are obvious to the human eye.
However technically every reconstruction algorithm can potentially
introduce tiny artifacts not visible to the human eye in the
reconstruction.</p>
</div>
</div>
</div>
</div>
</section><section><h2 class="section-heading" id="other-image-types">Other Image Types<a class="anchor" aria-label="anchor" href="#other-image-types"></a>
</h2>
<hr class="half-width">
<p>Nuclear medicine images scans and pathology images are also broadly
available inside hospitals.</p>
<p>Nuclear medicine images e.g. PET and SPECT can be 2D or 3D images
based on a signal of a radiotracer given to the patient. When the
radiotracer is consumed it lets off gamma rays which are then used as a
signal. This type of image can be extremely useful in processes like
looking for metastases. While 3D images <a href="reference.html#registration">registered</a> with a CT or MRI give
anatomic precision, in some cases a 2D image awnsers basic questions. In
the image below a 2D bone scan shows metastasis from prostate
cancer.</p>
<figure><img src="../fig/Prostate-mets-102.jpg" alt="Nuclear Medicine Image." class="figure mx-auto d-block"><div class="figcaption">Nuclear medicine image.</div>
</figure><p><em>sourced from RadsWiki, CC BY-SA 3.0 <a href="https://creativecommons.org/licenses/by-sa/3.0" class="external-link uri">https://creativecommons.org/licenses/by-sa/3.0</a>, via
Wikimedia Commons</em></p>
<p>Pathology is currently undergoing a revolution of digitalization, and
a typical file format has not emerged yet. Pathology images may be
DICOM, but could also be stored as specific kinds of TIFF files or other
file types. Pathology is an entire medical discipline in which various
types of images are used, both 2D, 3D and often multi-channel i.e. in
color. However, one classic type of pathology image is that of a stained
tissue slide seen by a microscope. With various stains we can see what
is going on in a tissue on a cellular level. In the image below you can
see macrophages that have come to sorround actinomyces in someone’s
lung.</p>
<figure><img src="../fig/Actinomycosis.jpg" alt="Pathology Image." class="figure mx-auto d-block"><div class="figcaption">Pathology image.</div>
</figure><p><em>sourced from By Yale Rosen from USA - Actinomycosis, CC BY-SA
2.0, <a href="https://commons.wikimedia.org/w/index.php?curid=31127755" class="external-link uri">https://commons.wikimedia.org/w/index.php?curid=31127755</a></em></p>
<p>Beyond the more common types of imaging, researchers are actively
looking into new forms of imaging. Some add new information to old
modalities, like contrast-enhanced ultrasounds. Other new forms of
imaging are novel in terms of the signal, such as terahertz imaging,
which uses a previously ‘unused’ part of the electomagnetic radiation
spectrum. As you might guess, the more novel the imaging, usually the
less consolidation there is around file types and how they are
organized. It is useful to remember that all these file types, whether
on established image types or novel ones, are sorts of ‘containers’ for
the ‘payload’ of the actual images which are the arrays. Often we simply
need to know how to get the payload array out of its container and/or
where to find certain metadata.</p>
<p>There is less standardization around file formats of certain types of
imaging.</p>
<p>For example, while typical radiological images have settled in how
they are recorded in DICOM, more novel sequences, such as arterial
spin-labeled ones, do not have a standardized way of how they are
recorded in it. Some newer forms of imaging such as electrical impedence
tomography use entirely different kinds of technologies and signals than
anything already existing. When it comes to truly novel imaging types
there can be no standardization at all.</p>
<div id="standard-image-types" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<div id="standard-image-types" class="callout-inner">
<h3 class="callout-title">Standard Image types</h3>
<div class="callout-content">
<table class="table">
<colgroup>
<col width="20%">
<col width="31%">
<col width="21%">
<col width="15%">
<col width="11%">
</colgroup>
<thead><tr class="header">
<th>Type</th>
<th>Signal</th>
<th>Standard File</th>
<th>Image file</th>
<th>Image array</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>X-ray</td>
<td>ionizing radiation</td>
<td>DICOM</td>
<td>JPEG 2000</td>
<td>2D arrays</td>
</tr>
<tr class="even">
<td>Standard CT</td>
<td>ionizing radiation</td>
<td>DICOM</td>
<td>Raw or compressed voxel data</td>
<td>3D arrays</td>
</tr>
<tr class="odd">
<td>Ultrasound (B-mode)</td>
<td>high frequency sound</td>
<td>DICOM</td>
<td>CINE</td>
<td>2D array or 4D tensors</td>
</tr>
<tr class="even">
<td>MRI (spin or gradient echo)</td>
<td>patient’s molecules</td>
<td>DICOM</td>
<td>Raw or compressed voxel data</td>
<td>3D arrays</td>
</tr>
<tr class="odd">
<td>Digital Pathology slides</td>
<td>light through stained tissue</td>
<td>no consensus</td>
<td>often converted to TIFF</td>
<td>multichannel 2D or 3D arrays</td>
</tr>
</tbody>
</table>
<p>The above table is a drastic simplification as there are always cases
where people use novel files or novel designs. There are also many newer
technologies like 4D CT. Nonetheless it is useful to know some of the
typical data structures you will usually find.</p>
</div>
</div>
</div>
<div id="keypoints1" class="callout keypoints">
<div class="callout-square">
<i class="callout-icon" data-feather="key"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Key Points</h3>
<div class="callout-content">
<ul>
<li>Each imaging modality provides distinct sets of information</li>
<li>In computational imaging, images are essentially arrays, although
embedded in additional data structures</li>
<li>Many images we may get e.g. MRIs and CTs have already been processed
with some algorithms to make them human readable</li>
<li>Research should be thoughtfully designed, taking into account the
constraints and capabilities inherent in human capacities</li>
<li>We can expect the emergence of additional imaging modalities in the
future</li>
</ul>
</div>
</div>
</div>
<!--
Place links that you need to refer to multiple times across pages here. Delete
any links that you are not going to use.
 -->
</section></section><section id="aio-mri"><p>Content from <a href="mri.html">Working with MRI</a></p>
<hr>
<p>Last updated on 2024-09-09 |

        <a href="https://github.com/esciencecenter-digital-skills/medical-image-processing/edit/main/episodes/mri.md" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<p>Estimated time: <i aria-hidden="true" data-feather="clock"></i> 70 minutes</p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul>
<li>What kinds of MRI are there?</li>
<li>How are MRI data represented digitally?</li>
<li>How should I organize and structure files for neuroimaging MRI
data?</li>
</ul>
</div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul>
<li>Show common kinds of MRI imaging used in research</li>
<li>Show the most common file formats for MRI</li>
<li>Introduce MRI coordinate systems</li>
<li>Load an MRI scan into Python and explain how the data is stored</li>
<li>View and manipulate image data</li>
<li>Explain what BIDS is</li>
<li>Explain advantages of working with Nifti and BIDS</li>
<li>Show a method to convert from DICOM to BIDS/NIfTI</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<section><h2 class="section-heading" id="introduction">Introduction<a class="anchor" aria-label="anchor" href="#introduction"></a>
</h2>
<hr class="half-width">
<p>This lesson is heavily based on existing lessons from Carpentries;
namely:</p>
<ol style="list-style-type: decimal">
<li><a href="https://carpentries-incubator.github.io/SDC-BIDS-IntroMRI/" class="external-link">Introduction
to Working with MRI Data in Python</a></li>
<li><a href="https://carpentries-incubator.github.io/SDC-BIDS-dMRI/" class="external-link">Introduction
to dMRI</a></li>
<li><a href="https://carpentries-incubator.github.io/SDC-BIDS-fMRI/" class="external-link">Functional
Neuroimaging Analysis in Python</a></li>
</ol>
<p>We will not cover all the material from these lessons, but instead
provide an overview of the key points.</p>
</section><section><h2 class="section-heading" id="types-of-mr-scans">Types of MR Scans<a class="anchor" aria-label="anchor" href="#types-of-mr-scans"></a>
</h2>
<hr class="half-width">
<div class="section level3">
<h3 id="anatomical">Anatomical<a class="anchor" aria-label="anchor" href="#anatomical"></a>
</h3>
<figure><img src="../fig/t1t2flairbrain.jpg" alt="FLAIR brain" width="width: 30vw;" class="figure mx-auto d-block"></figure><p><em>Sourced from <a href="https://case.edu/med/neurology/NR/MRI%20Basics.htm" class="external-link">https://case.edu/med/neurology/NR/MRI%20Basics.htm</a></em></p>
<ul>
<li>3D images of anatomy</li>
<li>Different tissue types produce different intensities</li>
<li>Different sequences produce different intensities for various
phenomena and tissues</li>
</ul>
</div>
<div class="section level3">
<h3 id="functional">Functional<a class="anchor" aria-label="anchor" href="#functional"></a>
</h3>
<figure><img src="../fig/bold.gif" alt="FMRI" class="figure mx-auto d-block"></figure><figure><img src="../fig/fmri_timeseries.png" alt="FMRI timeseries" class="figure mx-auto d-block"></figure><p><em>Sourced from Wagner and Lindquist, 2015</em></p>
<ul>
<li>Reveals blood oxygen level-dependant (BOLD) signal</li>
<li>Four dimensional image (x, y, z and time)</li>
</ul>
</div>
<div class="section level3">
<h3 id="diffusion">Diffusion<a class="anchor" aria-label="anchor" href="#diffusion"></a>
</h3>
<div style="display: flex; justify-content: center;">
<p><img src="../fig/dwi.gif" alt="DWI" style="width: 45%; margin-right: 10px;" class="figure"><img src="../fig/dwi_tracts.png" alt="DWI tracts" style="width: 45%; margin-left: 10px;" class="figure"></p>
</div>
<p><em>Sourced from <a href="https://brainsuite.org/processing/diffusion/tractography/" class="external-link">http://brainsuite.org/processing/diffusion/tractography/</a></em></p>
<ul>
<li>Measures diffusion of water in order to model tissue
microstructure</li>
<li>Four dimensional images (x, y, z + direction of diffusion)</li>
<li>Has parameters about the strength of the diffusion “gradient” and
its direction in <code>.bval</code> and <code>.bvec</code> files</li>
</ul>
</div>
<div class="section level3">
<h3 id="other-types-of-mri">Other Types of MRI<a class="anchor" aria-label="anchor" href="#other-types-of-mri"></a>
</h3>
<p>Perfusion weighted imaging includes relatively novel sequences such
as dynamic contrast-enhanced MR perfusion, dynamic susceptibility
contrast MR perfusion, and arterial spin labelled perfusion.</p>
<p>MRI can also be used for spectroscopy, but this will not be covered
here as it does not produce traditional images.</p>
</div>
</section><section><h2 class="section-heading" id="common-mri-file-formats">Common MRI File Formats<a class="anchor" aria-label="anchor" href="#common-mri-file-formats"></a>
</h2>
<hr class="half-width">
<table class="table">
<colgroup>
<col width="13%">
<col width="17%">
<col width="55%">
<col width="13%">
</colgroup>
<thead><tr class="header">
<th>Format Name</th>
<th>File Extension</th>
<th>Origin/Group</th>
<th>More info</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>DICOM</td>
<td>none or <code>.dc</code>
</td>
<td>ACR/NEMA Consortium</td>
<td><a href="https://www.dicomstandard.org/" class="external-link uri">https://www.dicomstandard.org/</a></td>
</tr>
<tr class="even">
<td>Analyze</td>
<td>
<code>.img</code>/<code>.hdr</code>
</td>
<td>Analyze Software, Mayo Clinic</td>
<td><a href="https://eeg.sourceforge.net/ANALYZE75.pdf" class="external-link uri">https://eeg.sourceforge.net/ANALYZE75.pdf</a></td>
</tr>
<tr class="odd">
<td>NIfTI</td>
<td><code>.nii</code></td>
<td>Neuroimaging Informatics Technology Initiative</td>
<td><a href="https://brainder.org/2012/09/23/the-nifti-file-format/" class="external-link uri">https://brainder.org/2012/09/23/the-nifti-file-format/</a></td>
</tr>
<tr class="even">
<td>MINC</td>
<td><code>.mnc</code></td>
<td>Montreal Neurological Institute</td>
<td><a href="https://www.mcgill.ca/bic/software/minc" class="external-link uri">https://www.mcgill.ca/bic/software/minc</a></td>
</tr>
<tr class="odd">
<td>NRRD</td>
<td><code>.nrrd</code></td>
<td></td>
<td><a href="https://teem.sourceforge.net/nrrd/format.html" class="external-link uri">https://teem.sourceforge.net/nrrd/format.html</a></td>
</tr>
</tbody>
</table>
<p>From the MRI scanner, images are initially collected and put in the
DICOM format but can be converted to these other formats to make working
with the data easier.</p>
<p>In a later episode, we will delve deeper into DICOM data, which
includes various information such as the patient’s name. In this
episode, we will focus on accessing the images.</p>
<p>NIfTI is one of the most ubiquitous file formats for storing
neuroimaging data. We can convert DICOM data to NIfTI using <a href="https://github.com/rordenlab/dcm2niix" class="external-link">dcm2niix</a> software.</p>
<p>We can learn how to run <code>dcm2niix</code> by taking a look at its
help menu:</p>
<div class="codewrapper sourceCode" id="cb1">
<h3 class="code-label">BASH<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode bash" tabindex="0"><code class="sourceCode bash"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a><span class="ex">dcm2niix</span> <span class="at">-help</span></span></code></pre>
</div>
<p>One of the advantages of working with <code>dcm2niix</code> is that
it can be used to create Brain Imaging Data Structure (BIDS) files,
since it outputs a NIfTI and a JSON with metadata ready to fit into the
BIDS standard. <a href="https://bids.neuroimaging.io/" class="external-link">BIDS</a> is a
widely adopted standard of how data from neuroimaging research can be
organized. The organization of data and files is crucial for seamless
collaboration across research groups and even between individual
researchers. Some pipelines assume your data is organized in BIDS
structure, and these are sometimes called <a href="https://bids-apps.neuroimaging.io/apps/" class="external-link">BIDS Apps</a>.</p>
<p>Some of the more popular examples are:</p>
<ul>
<li><code>fmriprep</code></li>
<li><code>freesurfer</code></li>
<li><code>micapipe</code></li>
<li><code>SPM</code></li>
<li><code>MRtrix3_connectome</code></li>
</ul>
<p>We recommend the <a href="https://bids-standard.github.io/bids-starter-kit/#" class="external-link">BIDS
starter-kit website</a> for learning the basics of this standard.</p>
<p>Next, we’ll cover some details on working with NIfTI files.</p>
</section><section><h2 class="section-heading" id="reading-nifti-images">Reading NIfTI Images<a class="anchor" aria-label="anchor" href="#reading-nifti-images"></a>
</h2>
<hr class="half-width">
<p><a href="https://nipy.org/nibabel/" class="external-link">NiBabel</a> is a Python package
for reading and writing neuroimaging data. To learn more about how
NiBabel handles NIfTIs, refer to the <a href="https://nipy.org/nibabel/nifti_images.html" class="external-link">NiBabel documentation
on working with NIfTIs</a>, which this episode heavily references.</p>
<div class="codewrapper sourceCode" id="cb2">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" tabindex="-1"></a><span class="im">import</span> nibabel <span class="im">as</span> nib</span></code></pre>
</div>
<p>First, use the <code>load()</code> function to create a
<code>NiBabel</code> image object from a NIfTI file.</p>
<div class="codewrapper sourceCode" id="cb3">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" tabindex="-1"></a>t2_img <span class="op">=</span> nib.load(<span class="st">"data/mri//OBJECT_phantom_T2W_TSE_Cor_14_1.nii"</span>)</span></code></pre>
</div>
<p>When loading a NIfTI file with <code>NiBabel</code>, you get a
specialized data object that includes all the information stored in the
file. Each piece of information is referred to as an
<strong>attribute</strong> in Python’s terminology. To view all these
attributes, simply type <code>t2_img.</code> followed by <kbd>Tab</kbd>.
Today, we’ll focus on discussing mainly two attributes
(<code>header</code> and <code>affine</code>) and one method
(<code>get_fdata</code>).</p>
<div class="section level3">
<h3 id="header">1. <a href="https://nipy.org/nibabel/nibabel_images.html#the-image-header" class="external-link">Header</a>
<a class="anchor" aria-label="anchor" href="#header"></a>
</h3>
<p>It contains metadata about the image, including image dimensions,
data type, and more.</p>
<div class="codewrapper sourceCode" id="cb4">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" tabindex="-1"></a>t2_hdr <span class="op">=</span> t2_img.header</span>
<span id="cb4-2"><a href="#cb4-2" tabindex="-1"></a><span class="bu">print</span>(t2_hdr)</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>&lt;class 'nibabel.nifti1.Nifti1Header'&gt; object, endian='&lt;'
sizeof_hdr      : 348
data_type       : b''
db_name         : b''
extents         : 0
session_error   : 0
regular         : b''
dim_info        : 0
dim             : [  3 432 432  30   1   1   1   1]
intent_p1       : 0.0
intent_p2       : 0.0
intent_p3       : 0.0
intent_code     : none
datatype        : int16
bitpix          : 16
slice_start     : 0
pixdim          : [1.        0.9259259 0.9259259 5.7360578 0.        0.        0.
 0.       ]
vox_offset      : 0.0
scl_slope       : nan
scl_inter       : nan
slice_end       : 29
slice_code      : unknown
xyzt_units      : 2
cal_max         : 0.0
cal_min         : 0.0
slice_duration  : 0.0
toffset         : 0.0
glmax           : 0
glmin           : 0
descrip         : b'Philips Healthcare Ingenia 5.4.1 '
aux_file        : b''
qform_code      : scanner
sform_code      : unknown
quatern_b       : 0.008265011
quatern_c       : 0.7070585
quatern_d       : -0.7070585
qoffset_x       : 180.81993
qoffset_y       : 21.169691
qoffset_z       : 384.01007
srow_x          : [1. 0. 0. 0.]
srow_y          : [0. 1. 0. 0.]
srow_z          : [0. 0. 1. 0.]
intent_name     : b''
magic           : b'n+1'</code></pre>
</div>
<p><code>t2_hdr</code> is a Python <strong>dictionary</strong>, i.e. a
container that hold pairs of objects - keys and values. Let’s take a
look at all of the keys.</p>
<p>Similar to <code>t2_img</code>, in which attributes can be accessed
by typing <code>t2_img.</code> followed by <kbd>Tab</kbd>, you can do
the same with <code>t2_hdr</code>.</p>
<p>In particular, we’ll be using a <strong>method</strong> belonging to
<code>t2_hdr</code> that will allow you to view the keys associated with
it.</p>
<div class="codewrapper sourceCode" id="cb6">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" tabindex="-1"></a><span class="bu">print</span>(t2_hdr.keys())</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>['sizeof_hdr',
 'data_type',
 'db_name',
 'extents',
 'session_error',
 'regular',
 'dim_info',
 'dim',
 'intent_p1',
 'intent_p2',
 'intent_p3',
 'intent_code',
 'datatype',
 'bitpix',
 'slice_start',
 'pixdim',
 'vox_offset',
 'scl_slope',
 'scl_inter',
 'slice_end',
 'slice_code',
 'xyzt_units',
 'cal_max',
 'cal_min',
 'slice_duration',
 'toffset',
 'glmax',
 'glmin',
 'descrip',
 'aux_file',
 'qform_code',
 'sform_code',
 'quatern_b',
 'quatern_c',
 'quatern_d',
 'qoffset_x',
 'qoffset_y',
 'qoffset_z',
 'srow_x',
 'srow_y',
 'srow_z',
 'intent_name',
 'magic']</code></pre>
</div>
<p>Notice that <strong>methods</strong> require you to include () at the
end of them when you call them whereas <strong>attributes</strong> do
not. The key difference between a method and an attribute is:</p>
<ul>
<li>Attributes are <em>variables</em> belonging to an object and
containing information about their properties or characteristics</li>
<li>Methods are functions that belong to an object and operate on its
attributes. They differ from regular functions by implicitly receiving
the object (<code>self</code>) as their first argument.</li>
</ul>
<p>When you type in <code>t2_img.</code> followed by <kbd>Tab</kbd>, you
may see that attributes are highlighted in orange and methods
highlighted in blue.</p>
<p>The output above is a list of <strong>keys</strong> you can use to
access <strong>values</strong> of <code>t2_hdr</code>. We can access the
value stored by a given key by typing:</p>
<div class="codewrapper sourceCode" id="cb8">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" tabindex="-1"></a><span class="bu">print</span>(t2_hdr[<span class="st">'&lt;key_name&gt;'</span>])</span></code></pre>
</div>
<div id="challenge-extract-values-from-the-nifti-header" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="challenge-extract-values-from-the-nifti-header" class="callout-inner">
<h3 class="callout-title">Challenge: Extract Values from the NIfTI Header</h3>
<div class="callout-content">
<p>Extract the ‘pixdim’ field from the NiFTI header of the loaded
image.</p>
</div>
</div>
</div>
<div id="accordionSolution1" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution1" aria-expanded="false" aria-controls="collapseSolution1">
  <h4 class="accordion-header" id="headingSolution1"> Show me the solution </h4>
</button>
<div id="collapseSolution1" class="accordion-collapse collapse" aria-labelledby="headingSolution1" data-bs-parent="#accordionSolution1">
<div class="accordion-body">
<div class="codewrapper sourceCode" id="cb9">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" tabindex="-1"></a><span class="bu">print</span>(t2_hdr[<span class="st">'pixdim'</span>])</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>array([1. , 0.9259259, 0.9259259, 5.7360578, 0. , 0. , 0. , 0. ], dtype=float32)</code></pre>
</div>
</div>
</div>
</div>
</div>
</div>
<div class="section level3">
<h3 id="data">2. Data<a class="anchor" aria-label="anchor" href="#data"></a>
</h3>
<p>As you’ve seen above, the header contains useful information that
gives us information about the properties (metadata) associated with the
MR data we’ve loaded in. Now we’ll move in to loading the actual
<em>image data itself</em>. We can achieve this by using the method
called <code>t2_img.get_fdata()</code>:</p>
<div class="codewrapper sourceCode" id="cb11">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" tabindex="-1"></a>t2_data <span class="op">=</span> t2_img.get_fdata()</span>
<span id="cb11-2"><a href="#cb11-2" tabindex="-1"></a><span class="bu">print</span>(t2_data)</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>array([[[0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        ...,
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.]],

       [[0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        ...,
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.]],

       ...,

       [[0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        ...,
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.]]])</code></pre>
</div>
<p>The initial observation you might make is the prevalence of zeros in
the image. This abundance of zeros might prompt concerns about the
presence of any discernible content in the picture. However, when
working with radiological images, it’s important to keep in mind that
these images frequently contain areas of air surrounding the objects of
interest, which appear as black space.</p>
<p>What type of data is this exactly in a computational sense? We can
determine this by calling the <code>type()</code> function on
<code>t2_data</code>:</p>
<div class="codewrapper sourceCode" id="cb13">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" tabindex="-1"></a><span class="bu">print</span>(<span class="bu">type</span>(t2_data))</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code><span><span class="va">numpy.ndarray</span></span></code></pre>
</div>
<p>The data is stored as a multidimensional <strong>array</strong>,
which can also be accessed through the file’s <code>dataobj</code>
property:</p>
<div class="codewrapper sourceCode" id="cb15">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" tabindex="-1"></a>t2_img.dataobj</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>&lt;nibabel.arrayproxy.ArrayProxy at 0x20c63b5a4a0&gt;</code></pre>
</div>
<div id="challenge-check-out-attributes-of-the-array" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="challenge-check-out-attributes-of-the-array" class="callout-inner">
<h3 class="callout-title">Challenge: Check Out Attributes of the Array</h3>
<div class="callout-content">
<p>How can we see the number of dimensions in the <code>t2_data</code>
array? Once again, all of the attributes of the array can be seen by
typing <code>t2_data.</code> followed by <kbd>Tab</kbd>.</p>
</div>
</div>
</div>
<div id="accordionSolution2" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution2" aria-expanded="false" aria-controls="collapseSolution2">
  <h4 class="accordion-header" id="headingSolution2"> Show me the solution </h4>
</button>
<div id="collapseSolution2" class="accordion-collapse collapse" aria-labelledby="headingSolution2" data-bs-parent="#accordionSolution2">
<div class="accordion-body">
<div class="codewrapper sourceCode" id="cb17">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" tabindex="-1"></a><span class="bu">print</span>(t2_data.ndim)</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code><span><span class="fl">3</span></span></code></pre>
</div>
<p><code>t2_data</code> contains 3 dimensions. You can think of the data
as a 3D version of a picture (more accurately, a volume).</p>
<figure><img src="../fig/numpy_arrays.png" alt="Numpy arrays" width="70%;" class="figure mx-auto d-block"></figure>
</div>
</div>
</div>
</div>
<div id="challenge-check-out-attributes-of-the-array" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="challenge-check-out-attributes-of-the-array" class="callout-inner">
<h3 class="callout-title">Challenge: Check Out Attributes of the
Array<em> (continued)</em>
</h3>
<div class="callout-content">
<p>Remember typical 2D pictures are made out of <strong>pixels</strong>,
but a 3D MR image is made up of 3D cubes called
<strong>voxels</strong>.</p>
<figure><img src="../fig/mri_slices.jpg" alt="MRI slices" width="70%;" class="figure mx-auto d-block"></figure><p>What is the shape of the image?</p>
</div>
</div>
</div>
<div id="accordionSolution3" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution3" aria-expanded="false" aria-controls="collapseSolution3">
  <h4 class="accordion-header" id="headingSolution3"> Show me the solution </h4>
</button>
<div id="collapseSolution3" class="accordion-collapse collapse" aria-labelledby="headingSolution3" data-bs-parent="#accordionSolution3">
<div class="accordion-body">
<div class="codewrapper sourceCode" id="cb19">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" tabindex="-1"></a><span class="bu">print</span>(t2_data.shape)</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>(432, 432, 30)</code></pre>
</div>
</div>
</div>
</div>
</div>
<p>The three numbers given here represent the number of values <em>along
a respective dimension (x,y,z)</em>. This image was scanned in 30
slices, each with a resolution of 432 x 432 voxels. That means there are
<code>30 * 432 * 432 = 5,598,720</code> voxels in total!</p>
<p>Let’s see the type of data inside of the array.</p>
<div class="codewrapper sourceCode" id="cb21">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" tabindex="-1"></a><span class="bu">print</span>(t2_data.dtype)</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code><span><span class="fu">dtype</span><span class="op">(</span><span class="st">'float64'</span><span class="op">)</span></span></code></pre>
</div>
<p>This tells us that each element in the array (or voxel) is a
floating-point number.<br>
The data type of an image controls the range of possible intensities. As
the number of possible values increases, the amount of space the image
takes up in memory also increases.</p>
<div class="codewrapper sourceCode" id="cb23">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb23-2"><a href="#cb23-2" tabindex="-1"></a><span class="bu">print</span>(np.<span class="bu">min</span>(t2_data))</span>
<span id="cb23-3"><a href="#cb23-3" tabindex="-1"></a><span class="bu">print</span>(np.<span class="bu">max</span>(t2_data))</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code><span><span class="fl">0.0</span></span>
<span><span class="fl">630641.0785522461</span></span></code></pre>
</div>
<p>For our data, the range of intensity values goes from 0 (black) to
more positive digits (whiter).</p>
<p>To examine the value of a specific voxel, you can access it using its
indices. For example, if you have a 3D array <code>data</code>, you can
retrieve the value of a voxel at coordinates (x, y, z) with the
following syntax:</p>
<div class="codewrapper sourceCode" id="cb25">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" tabindex="-1"></a>value <span class="op">=</span> data[x, y, z]</span></code></pre>
</div>
<p>This will give you the value stored at the voxel located at the
specified index <code>(x, y, z)</code>. Make sure that the indices are
within the bounds of the array dimensions.</p>
<p>To inspect the value of a voxel at coordinates (9, 19, 2), you can
use the following code:</p>
<div class="codewrapper sourceCode" id="cb26">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" tabindex="-1"></a><span class="bu">print</span>(t2_data[<span class="dv">9</span>, <span class="dv">19</span>, <span class="dv">2</span>])</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code><span><span class="fl">0.</span></span></code></pre>
</div>
<p>This command retrieves and prints the intensity value at the
specified voxel. The output represents the signal intensity at that
particular location.</p>
<p>Next, we will explore how to extract and visualize larger regions of
interest, such as slices or arrays of voxels, for more comprehensive
analysis.</p>
</div>
</section><section><h2 class="section-heading" id="working-with-image-data">Working with Image Data<a class="anchor" aria-label="anchor" href="#working-with-image-data"></a>
</h2>
<hr class="half-width">
<p>Slicing does exactly what it seems to imply. Given a 3D volume,
slicing involves extracting a 2D <strong>slice</strong> from our
data.</p>
<figure><img src="../fig/T1w.gif" alt="T1 weighted" class="figure mx-auto d-block"></figure><p>From left to right: sagittal, coronal and axial slices of a
brain.</p>
<p>Let’s select the 10th slice in the z-axis of our data:</p>
<div class="codewrapper sourceCode" id="cb28">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" tabindex="-1"></a>z_slice <span class="op">=</span> t2_data[:, :, <span class="dv">9</span>]</span></code></pre>
</div>
<p>This is similar to the indexing we did before to select a single
voxel. However, instead of providing a value for each axis, the
<code>:</code> indicates that we want to grab <em>all</em> values from
that particular axis.</p>
<div id="challenge-slicing-mri-data" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="challenge-slicing-mri-data" class="callout-inner">
<h3 class="callout-title">Challenge: Slicing MRI Data</h3>
<div class="callout-content">
<p>Select the 20th slice along the y-axis and store it in a variable.
Then, select the 4th slice along the x-axis and store it in another
variable.</p>
</div>
</div>
</div>
<div id="accordionSolution4" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution4" aria-expanded="false" aria-controls="collapseSolution4">
  <h4 class="accordion-header" id="headingSolution4"> Show me the solution </h4>
</button>
<div id="collapseSolution4" class="accordion-collapse collapse" aria-labelledby="headingSolution4" data-bs-parent="#accordionSolution4">
<div class="accordion-body">
<div class="codewrapper sourceCode" id="cb29">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" tabindex="-1"></a>y_slice <span class="op">=</span> t2_data[:, <span class="dv">19</span>, :]</span>
<span id="cb29-2"><a href="#cb29-2" tabindex="-1"></a>x_slice <span class="op">=</span> t2_data[<span class="dv">3</span>, :, :]</span></code></pre>
</div>
</div>
</div>
</div>
</div>
<p>We’ve been slicing and dicing images but we have no idea what they
look like. In the next section we’ll show you one way you can visualize
it all together.</p>
</section><section><h2 class="section-heading" id="visualizing-the-data">Visualizing the Data<a class="anchor" aria-label="anchor" href="#visualizing-the-data"></a>
</h2>
<hr class="half-width">
<p>We previously inspected the signal intensity of the voxel at
coordinates (10,20,3). Let’s see what out data looks like when we slice
it at this location. We’ve already indexed the data at each x-, y-, and
z-axis. Let’s use <code>matplotlib</code>:</p>
<div class="codewrapper sourceCode" id="cb30">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb30-2"><a href="#cb30-2" tabindex="-1"></a><span class="op">%</span>matplotlib inline</span>
<span id="cb30-3"><a href="#cb30-3" tabindex="-1"></a></span>
<span id="cb30-4"><a href="#cb30-4" tabindex="-1"></a>slices <span class="op">=</span> [x_slice, y_slice, z_slice]</span>
<span id="cb30-5"><a href="#cb30-5" tabindex="-1"></a></span>
<span id="cb30-6"><a href="#cb30-6" tabindex="-1"></a>fig, axes <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="bu">len</span>(slices))</span>
<span id="cb30-7"><a href="#cb30-7" tabindex="-1"></a><span class="cf">for</span> i, <span class="bu">slice</span> <span class="kw">in</span> <span class="bu">enumerate</span>(slices):</span>
<span id="cb30-8"><a href="#cb30-8" tabindex="-1"></a>    axes[i].imshow(<span class="bu">slice</span>.T, cmap<span class="op">=</span><span class="st">"gray"</span>, origin<span class="op">=</span><span class="st">"lower"</span>)</span></code></pre>
</div>
<p>Now, we’re shifting our focus away from discussing our data to
address the final crucial attribute of a NIfTI.</p>
<div class="section level3">
<h3 id="affine">3. <a href="https://nipy.org/nibabel/coordinate_systems.html" class="external-link">Affine</a>
<a class="anchor" aria-label="anchor" href="#affine"></a>
</h3>
<p>The final important piece of metadata associated with an image file
is the <strong>affine matrix</strong>, which indicates the position of
the image array data in a reference space.</p>
<p>Below is the affine matrix for our data:</p>
<div class="codewrapper sourceCode" id="cb31">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" tabindex="-1"></a>t2_affine <span class="op">=</span> t2_img.affine</span>
<span id="cb31-2"><a href="#cb31-2" tabindex="-1"></a><span class="bu">print</span>(t2_affine)</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>array([[-9.25672975e-01,  2.16410652e-02, -1.74031337e-05,
         1.80819931e+02],
       [ 2.80924864e-06, -3.28338569e-08, -5.73605776e+00,
         2.11696911e+01],
       [-2.16410652e-02, -9.25672975e-01, -2.03403855e-07,
         3.84010071e+02],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         1.00000000e+00]])</code></pre>
</div>
<p>To explain this concept, recall that we referred to coordinates in
our data as (x,y,z) coordinates such that:</p>
<ul>
<li>x is the first dimension of <code>t2_data</code>
</li>
<li>y is the second dimension of <code>t2_data</code>
</li>
<li>z is the third dimension of <code>t2_data</code>
</li>
</ul>
<p>Although this tells us how to access our data in terms of voxels in a
3D volume, it doesn’t tell us much about the actual dimensions in our
data (centimetres, right or left, up or down, back or front). The affine
matrix allows us to translate between <em>voxel coordinates</em> in
(x,y,z) and <em>world space coordinates</em> in (left/right, bottom/top,
back/front). An important thing to note is that in reality in which
order you have:</p>
<ul>
<li>Left/right</li>
<li>Bottom/top</li>
<li>Back/front</li>
</ul>
<p>Depends on how you’ve constructed the affine matrix; thankfully there
is in depth coverage of the issue <a href="https://nipy.org/nibabel/coordinate_systems.html" class="external-link">the nibabel
documentation</a> For most of the the data we’re dealing with we use a
RAS coordinate system so it always refers to:</p>
<ul>
<li>Right</li>
<li>Anterior</li>
<li>Superior</li>
</ul>
<p>Here we must note a practical point. Radiologists and nuclear
medicine specialists like to look at images in a certain layout. The
patient’s right side will be on the physical left of the image. This is
a display convention that is the opposite of how a lot of NIfTIs are set
up by scientists. If you want your results to be used by actual medical
specialists, you probably need to translate your images to thier
conventions. Remember medical specialists may have to read hundreds of
images a day, so they want thier process streamlined, not to worry about
flipping around images so they can understand them.</p>
<p>Applying the affine matrix (<code>t2_affine</code>) is done by using
a <em>linear map</em> (matrix multiplication) on voxel coordinates
(defined in <code>t2_data</code>).</p>
<figure><img src="../fig/coordinate_systems.png" alt="Coordinate system" class="figure mx-auto d-block"></figure><p>The concept of an affine matrix may seem confusing at first but
essentially it allows us to figure out real world distances and
locations.</p>
<p>If we want to know what the distances between these two voxels are in
terms of real world distances (millimetres), this information cannot be
derived from using voxel coordinates, and so we need the <strong>affine
matrix</strong>.</p>
<p>NIfTI images, by definition, have an affine with the voxel
coordinates relating to the real world coordinates in RAS+ encoded
space. So here the affine matrix we’ll be using will be encoded in
<strong>RAS</strong>. That means once we apply the matrix our
coordinates are <code>(Right, Anterior, Superior)</code>.</p>
<ul>
<li>In the R axis, positive values mean move right, negative values mean
move left</li>
<li>In the A axis, positive values mean move forward, negative values
mean move posterior</li>
<li>In the S axis, positive values mean move up, negative values mean
move inferior</li>
</ul>
<p>Increasing a coordinate value in the first dimension corresponds to
moving to the right of the person being scanned, and so on.</p>
</div>
</section><section><h2 class="section-heading" id="functional-mri-data">Functional MRI Data<a class="anchor" aria-label="anchor" href="#functional-mri-data"></a>
</h2>
<hr class="half-width">
<p>A fundamental difference between many MRI sequences and fMRI is the
inclusion of a time dimension in fMRI. Essentially, fMRI captures a
signal in each voxel of the imaged object over time. We can visualize
this as shown below:</p>
<figure><img src="../fig/4D_array_time.png" alt="4D array time" width="70%;" class="figure mx-auto d-block"></figure><p>Unfortunately, any signal will contain some noise, and fMRI data is
inherently noisy, particularly due to head movements. While our primary
interest is in grey matter brain cells, signals from other cells and
structures can also be detected. Various filtering and processing
techniques are employed to clean up fMRI data. Despite the challenges in
interpreting this type of imaging, the effort has led to numerous
positive outcomes for the neuroimaging community. For example, <a href="https://github.com/nipreps/fmriprep" class="external-link">fMRIPrep</a> has set a
standard across new modalities, leading to the broader concept of <a href="https://www.nipreps.org/" class="external-link">nipreps</a>. Notably,
<code>fmriprep</code> remains the go-to package for handling the
complexities of fMRI data processing.</p>
<figure><img src="../fig/nipreps-chart.png" alt="Nipreps chart" width="70%;" class="figure mx-auto d-block"></figure><p><em>Sourced from <a href="https://www.nipreps.org/" class="external-link">https://www.nipreps.org/</a></em></p>
<div id="nipreps-and-beyond" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<div id="nipreps-and-beyond" class="callout-inner">
<h3 class="callout-title">Nipreps and Beyond:</h3>
<div class="callout-content">
<ul>
<li>There are many, many packages for medical image analysis</li>
<li>There are known pre-built pipelines with possibilities in
python</li>
<li>Your pipeline will probably begin with cleaning and preparing
data</li>
<li>You can mix and match parts of pipelines with NiPype</li>
</ul>
</div>
</div>
</div>
<p>If you are less interested in coding, but still need it to accomplish
your research goals, it can be worthwhile to use packages that are well
known, as it is easier to find various forms of documentation and help.
For this reason <a href="https://github.com/nilearn/nilearn" class="external-link">NIlearn</a>
is a library to consider for fMRI data.</p>
<div id="advantages-of-nilearn" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<div id="advantages-of-nilearn" class="callout-inner">
<h3 class="callout-title">Advantages of NIlearn:</h3>
<div class="callout-content">
<ul>
<li>Fully free and open source</li>
<li>Extremely popular</li>
<li>Allows Python coding</li>
<li>Implementations of many state-of-the art algorithms</li>
<li>Works on Nibabel objects</li>
</ul>
</div>
</div>
</div>
</section><section><h2 class="section-heading" id="diffusion-mri-data">Diffusion MRI Data<a class="anchor" aria-label="anchor" href="#diffusion-mri-data"></a>
</h2>
<hr class="half-width">
<p>Diffusion MRIs have additional data when compared to anatomical
MRIs.</p>
<p>Diffusion sequences are sensitive to the signals from the random,
microscropic motion (i.e. diffusion) of water protons. The diffusion of
water in anatomical structures is restricted due to barriers (e.g. cell
membranes), resulting in a preferred direction of diffusion
(anisotropy). A typical diffusion MRI scan will acquire multiple volumes
with varying magnetic fields which are sensitive to diffusion along a
particular direction and result in diffusion-weighted images.</p>
<p>In addition to the acquired images, two files are collected as part
of the diffusion dataset, known as the b-vectors and b-values. The
b-value (file suffix <code>.bval</code>) is the diffusion-sensitizing
factor, and reflects the diffusion gradient timing and strength. The
b-vector (file suffix <code>.bvec</code>) corresponds to the direction
with which diffusion was measured. Together, these two files define the
diffusion MRI measurement as a set of gradient directions and
corresponding amplitudes, and are necessary to calculate useful measures
of the microscopic properties.</p>
<p>Just like fMRI, diffusion MRI data does not typically come off the
scanner ready to be analyzed, as there can be many things that might
need to be corrected before analysis. To illustrate what the
preprocessing step may look like, here is an example preprocessing
workflow from QSIPrep (Cieslak et al, 2020):</p>
<figure><img src="../fig/dmri_preprocess_steps.jpg" alt="dMRI preprocess steps" width="70%;" class="figure mx-auto d-block"></figure><p>Depending open what you want to do with your imaging you may use a
pre-contructed pipeline only, or you may want to code. A strong possible
library for coding with diffusion images is the <a href="https://dipy.org/index.html#" class="external-link">Diffusion Imaging in Python
(DIPY)</a> package.</p>
<div id="adantages-of-dipy" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<div id="adantages-of-dipy" class="callout-inner">
<h3 class="callout-title">Adantages of DIPY:</h3>
<div class="callout-content">
<ul>
<li>Fully free and open source</li>
<li>Allows Python coding</li>
<li>Implementations of many state-of-the art algorithms</li>
<li>Has methods for diffusion tensor imaging</li>
<li>High performance with many algorithms actually implemented in Cython
under the hood</li>
</ul>
</div>
</div>
</div>
<p>Diffusion tensor imaging (DTI) is a technique that uses diffusion of
water as a signal for axonal organization. Tractography is a group of
techniques to visualize neural tracts using data collected by DTI.</p>
<div id="tractography" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<div id="tractography" class="callout-inner">
<h3 class="callout-title">Tractography</h3>
<div class="callout-content">
<p>Tractography is a reconstruction technique used to visually represent
neural fiber tracts using data collected by diffusion MRI. Tractography
models axonal trajectories as ‘streamlines’ from local directional
information. There are several families methods for tractopraphy. No
known methods is exact and perfect, they all have biases and
limitations. The streamlines generated by a tractography method and the
required meta-data are usually saved into files called tractograms.</p>
</div>
</div>
</div>
<div id="keypoints1" class="callout keypoints">
<div class="callout-square">
<i class="callout-icon" data-feather="key"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Key Points</h3>
<div class="callout-content">
<ul>
<li>Imaging MRIs commonly used for research can be anatomical,
functional or diffusion</li>
<li>MRIs can be converted from DICOMs to NIfTIs</li>
<li>BIDS is a standard about organizing neuroimaging data</li>
<li>NIfTI images contain a header, which describes the contents, and the
data</li>
<li>The position of the NIfTI data in space is determined by the affine
matrix</li>
<li>NIfTI data is a multi-dimensional array of values</li>
<li>Functional MRIs and Diffusion MRIs require heavy
(pre)processing</li>
<li>Functional MRIs have time dimension</li>
<li>Diffusion MRI has b-values and b-vectors</li>
<li>There are many various tractography methods, each with
imperfections</li>
</ul>
</div>
</div>
</div>
<!--
Place links that you need to refer to multiple times across pages here. Delete
any links that you are not going to use.
 -->
</section></section><section id="aio-simpleitk"><p>Content from <a href="simpleitk.html">Registration and Segmentation with SITK</a></p>
<hr>
<p>Last updated on 2024-10-08 |

        <a href="https://github.com/esciencecenter-digital-skills/medical-image-processing/edit/main/episodes/simpleitk.md" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<p>Estimated time: <i aria-hidden="true" data-feather="clock"></i> 150 minutes</p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul>
<li>What are SITK Images?</li>
<li>How can registration be implemented in SITK?</li>
<li>How can I segment an image in SITK?</li>
</ul>
</div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul>
<li>Explain how to perform basic operations on SITK Images</li>
<li>Explain when registration can be needed and how to register images
with SITK</li>
<li>Become familiar with basic segmentation algorithms available in
SITK</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<section><h2 class="section-heading" id="introduction">Introduction<a class="anchor" aria-label="anchor" href="#introduction"></a>
</h2>
<hr class="half-width">
<p>In the realm of medical imaging analysis, registration and
segmentation play crucial roles. Registration aligns images, enabling
the fusion of data or the tracking of changes over time. Segmentation
identifies and labels objects of interest within images. Automation is
essential due to high clinical demand, driving the development of robust
algorithms for both processes. Moreover, many advanced segmentation
techniques utilize image registration, whether implicitly or
explicitly.</p>
<p><a href="https://github.com/SimpleITK/SimpleITK" class="external-link">SimpleITK</a> (SITK)
is a simplified programming interface to the algorithms and data
structures of the <a href="https://itk.org/" class="external-link">Insight Toolkit</a> (ITK)
for segmentation, registration and advanced image analysis, available in
many programming languages (e.g., C++, Python, R, Java, C#, Lua,
Ruby).</p>
<figure><img src="../fig/sitk.png" alt="SITK logo." class="figure mx-auto d-block"></figure><p>SITK is part of the <a href="https://insightsoftwareconsortium.org/" class="external-link">Insight Software
Consortium</a> a non-profit educational consortium dedicated to
promoting and maintaining open-source, freely available software for
medical image analysis. Its copyright is held by <a href="https://numfocus.org/" class="external-link">NumFOCUS</a>, and the software is
distributed under the <a href="https://github.com/SimpleITK/SimpleITK/blob/master/LICENSE" class="external-link">Apache
License 2.0</a>.</p>
<p>In this episode, we use a hands-on approach utilizing Python to show
how to use SITK for performing registration and segmentation tasks in
medical imaging use cases.</p>
</section><section><h2 class="section-heading" id="fundamental-concepts">Fundamental Concepts<a class="anchor" aria-label="anchor" href="#fundamental-concepts"></a>
</h2>
<hr class="half-width">
<p>In this section, we’ll cover some fundamental image processing
operations using SITK, such as reading and writing images, accessing
pixel values, and resampling images.</p>
<div class="section level3">
<h3 id="images">Images<a class="anchor" aria-label="anchor" href="#images"></a>
</h3>
<p>The fundamental tenet of an image in ITK and consequentially in SITK
is that an image is defined by a set of points on a grid occupying a
<strong>physical region in space</strong> . This significantly differs
from many other image analysis libraries that treat an image as an array
which has two implications: (1) pixel/voxel spacing is assumed to be
isotropic and (2) there is no notion of an image’s location in physical
space.</p>
<p>SITK images are multi-dimensional (the default configuration includes
images from two dimensional up to five dimensional) and can be a scalar,
labelmap (scalar with run length encoding), complex value or have an
arbitrary number of scalar channels (also known as a vector image). The
region in physical space which an image occupies is defined by the
image’s:</p>
<ol style="list-style-type: decimal">
<li>Origin (vector like type) - location in the world coordinate system
of the voxel with all zero indexes.</li>
<li>Spacing (vector like type) - distance between pixels along each of
the dimensions.</li>
<li>Size (vector like type) - number of pixels in each dimension.</li>
<li>Direction cosine matrix (vector like type representing matrix in row
major order) - direction of each of the axes corresponding to the matrix
columns.</li>
</ol>
<p>The following figure illustrates these concepts.</p>
<figure><img src="../fig/sitk_origin_spacing.png" alt="SITK Image." class="figure mx-auto d-block"><div class="figcaption">An image in SITK occupies a region in physical
space which is defined by its meta-data (origin, size, spacing, and
direction cosine matrix). Note that the image’s physical extent starts
half a voxel before the origin and ends half a voxel beyond the last
voxel.</div>
</figure><p>In SITK, when we construct an image we specify its dimensionality,
size and pixel type, all other components are set to <strong>reasonable
default values</strong>:</p>
<ol style="list-style-type: decimal">
<li>Origin - all zeros.</li>
<li>Spacing - all ones.</li>
<li>Direction - identity.</li>
<li>Intensities in all channels - all zero.</li>
</ol>
<p>The tenet that images occupy a spatial location in the physical world
has to do with the original application domain of ITK and SITK, medical
imaging. In that domain images represent anatomical structures with
metric sizes and spatial locations. Additionally, the spacing between
voxels is often non-isotropic (most commonly the spacing along the
inferior-superior/foot-to-head direction is larger). Viewers that treat
images as an array will display a distorted image as shown below:</p>
<figure><img src="../fig/isotropic_vs_non_isotropic.png" alt="Isotropic vs non-isotropic images." class="figure mx-auto d-block"><div class="figcaption">The same image displayed with a viewer that is
not aware of spatial meta-data (left image) and one that is aware (right
image). The image’s pixel spacing is (0.97656, 2.0)mm.</div>
</figure><p>As an image is also defined by its spatial location, two images with
the same pixel data and spacing may not be considered equivalent. Think
of two CT scans of the same patient acquired at different sites. The
following figure illustrates the notion of spatial location in the
physical world, the two images are considered different even though the
intensity values and pixel spacing are the same.</p>
<figure><img src="../fig/spatial_relationship.png" alt="Spatial relationship in images." class="figure mx-auto d-block"><div class="figcaption">Two images with exactly the same pixel data,
positioned in the world coordinate system. In SITK these are not
considered the same image, because they occupy different spatial
locations.</div>
</figure><p>As SITK images occupy a physical region in space, the quantities
defining this region have metric units (cm, mm, etc.). In general SITK
assumes units are in millimeters (historical reasons, due to DICOM
standard). In practice SITK is not aware of the specific units
associated with each image, it just assumes that they are consistent.
Thus, it is up to you the developer to ensure that all of the images you
read and created are using the same units.</p>
<p>A SITK image can have an arbitrary number of channels with the
content of the channels being a scalar or complex value. This is
determined when an image is created.</p>
<p>In the medical domain, many image types have a single scalar channel
(e.g. CT, US). Another common image type is a three channel image where
each channel has scalar values in [0,255], often people refer to such an
image as an RGB image. This terminology implies that the three channels
should be interpreted using the RGB color space. In some cases you can
have the same image type, but the channel values represent another color
space, such as HSV (it decouples the color and intensity information and
is a bit more invariant to illumination changes). SITK has no concept of
color space, thus in both cases it will simply view a pixel value as a
3-tuple.</p>
<p>Let’s read an example of human brain CT, and let’s explore it with
SITK.</p>
<div class="codewrapper sourceCode" id="cb1">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a><span class="op">%</span>matplotlib inline</span>
<span id="cb1-2"><a href="#cb1-2" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-3"><a href="#cb1-3" tabindex="-1"></a><span class="im">import</span> SimpleITK <span class="im">as</span> sitk</span>
<span id="cb1-4"><a href="#cb1-4" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" tabindex="-1"></a>img_volume <span class="op">=</span> sitk.ReadImage(<span class="st">"data/sitk/A1_grayT1.nrrd"</span>)</span>
<span id="cb1-6"><a href="#cb1-6" tabindex="-1"></a></span>
<span id="cb1-7"><a href="#cb1-7" tabindex="-1"></a><span class="bu">print</span>(<span class="bu">type</span>(img_volume))</span>
<span id="cb1-8"><a href="#cb1-8" tabindex="-1"></a><span class="bu">print</span>(img_volume.GetOrigin())</span>
<span id="cb1-9"><a href="#cb1-9" tabindex="-1"></a><span class="bu">print</span>(img_volume.GetSpacing())</span>
<span id="cb1-10"><a href="#cb1-10" tabindex="-1"></a><span class="bu">print</span>(img_volume.GetDirection())</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>&lt;class 'SimpleITK.SimpleITK.Image'&gt;
(-77.625, -107.625, 119.625)
(0.75, 0.75, 0.75)
(0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, -1.0, 0.0)</code></pre>
</div>
<p>The function <code>sitk.ReadImage</code> loads the file as a
<code>SimpleITK.SimpleITK.Image</code> instance, and then we can access
useful methods to get an idea of how the image/s contained in the file
is/are. The size of the image’s dimensions have explicit accessors:</p>
<div class="codewrapper sourceCode" id="cb3">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" tabindex="-1"></a><span class="bu">print</span>(img_volume.GetSize())</span>
<span id="cb3-2"><a href="#cb3-2" tabindex="-1"></a><span class="bu">print</span>(img_volume.GetWidth())</span>
<span id="cb3-3"><a href="#cb3-3" tabindex="-1"></a><span class="bu">print</span>(img_volume.GetHeight())</span>
<span id="cb3-4"><a href="#cb3-4" tabindex="-1"></a><span class="bu">print</span>(img_volume.GetDepth())</span>
<span id="cb3-5"><a href="#cb3-5" tabindex="-1"></a><span class="bu">print</span>(img_volume.GetNumberOfComponentsPerPixel())</span>
<span id="cb3-6"><a href="#cb3-6" tabindex="-1"></a><span class="bu">print</span>(img_volume.GetPixelIDTypeAsString())</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>(288, 320, 208)
288
320
208
1
32-bit float</code></pre>
</div>
<p>Just inspecting these accessors, we deduce that the file contains a
volume made of 208 images, each made of 288x320 pixels and one channel
only (grayscale).</p>
<div id="sitk-conventions" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<div id="sitk-conventions" class="callout-inner">
<h3 class="callout-title">SITK Conventions</h3>
<div class="callout-content">
<ul>
<li>Image access is in x,y,z order, <code>image.GetPixel(x,y,z)</code>
or <code>image[x,y,z]</code>, with zero based indexing.</li>
<li>If the output of an ITK filter has non-zero starting index, then the
index will be set to 0, and the origin adjusted accordingly.</li>
</ul>
</div>
</div>
</div>
<div id="displaying-images" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<div id="displaying-images" class="callout-inner">
<h3 class="callout-title">Displaying Images</h3>
<div class="callout-content">
<p>While SITK does not do visualization, it does contain a built in
<code>Show</code> method. This function writes the image out to disk and
than launches a program for visualization. By default it is configured
to use ImageJ, because it is readily supports all the image types which
SITK has and load very quickly.</p>
<div class="codewrapper sourceCode" id="cb5">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" tabindex="-1"></a>sitk.Show?</span></code></pre>
</div>
<p>SITK provides two options for invoking an external viewer, use a
procedural interface (the <code>Show</code> function) or an object
oriented one. For more details about them, please refer to <a href="https://insightsoftwareconsortium.github.io/SimpleITK-Notebooks/Python_html/04_Image_Display.html" class="external-link">this
notebook</a> from the official documentation.</p>
<p>In this episode we will convert SITK images to <code>numpy</code>
arrays, and we will plot them as such.</p>
</div>
</div>
</div>
<div class="section level4">
<h4 id="images-as-arrays">Images as Arrays<a class="anchor" aria-label="anchor" href="#images-as-arrays"></a>
</h4>
<p>We have two options for converting from SITK to a <code>numpy</code>
array:</p>
<ul>
<li>
<code>GetArrayFromImage()</code>: returns a copy of the image data.
You can then freely modify the data as it has no effect on the original
SITK image.</li>
<li>
<code>GetArrayViewFromImage()</code>: returns a view on the image
data which is useful for display in a memory efficient manner. You
cannot modify the data and <strong>the view will be invalid if the
original SITK image is deleted</strong>.</li>
</ul>
<p>The order of index and dimensions need careful attention during
conversion. ITK’s Image class has a <code>GetPixel</code> which takes an
ITK Index object as an argument, which is ordered as (x,y,z). This is
the convention that SITK’s Image class uses for the
<code>GetPixel</code> method and slicing operator as well. In
<code>numpy</code>, an array is indexed in the opposite order (z,y,x).
Also note that the access to channels is different. In SITK you do not
access the channel directly, rather the pixel value representing all
channels for the specific pixel is returned and you then access the
channel for that pixel. In the numpy array you are accessing the channel
directly. Let’s see this in an example:</p>
<div class="codewrapper sourceCode" id="cb6">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb6-2"><a href="#cb6-2" tabindex="-1"></a></span>
<span id="cb6-3"><a href="#cb6-3" tabindex="-1"></a>multi_channel_3Dimage <span class="op">=</span> sitk.Image([<span class="dv">2</span>, <span class="dv">4</span>, <span class="dv">8</span>], sitk.sitkVectorFloat32, <span class="dv">5</span>)</span>
<span id="cb6-4"><a href="#cb6-4" tabindex="-1"></a>x <span class="op">=</span> multi_channel_3Dimage.GetWidth() <span class="op">-</span> <span class="dv">1</span></span>
<span id="cb6-5"><a href="#cb6-5" tabindex="-1"></a>y <span class="op">=</span> multi_channel_3Dimage.GetHeight() <span class="op">-</span> <span class="dv">1</span></span>
<span id="cb6-6"><a href="#cb6-6" tabindex="-1"></a>z <span class="op">=</span> multi_channel_3Dimage.GetDepth() <span class="op">-</span> <span class="dv">1</span></span>
<span id="cb6-7"><a href="#cb6-7" tabindex="-1"></a>multi_channel_3Dimage[x, y, z] <span class="op">=</span> np.random.random(</span>
<span id="cb6-8"><a href="#cb6-8" tabindex="-1"></a>    multi_channel_3Dimage.GetNumberOfComponentsPerPixel()</span>
<span id="cb6-9"><a href="#cb6-9" tabindex="-1"></a>)</span>
<span id="cb6-10"><a href="#cb6-10" tabindex="-1"></a></span>
<span id="cb6-11"><a href="#cb6-11" tabindex="-1"></a>nda <span class="op">=</span> sitk.GetArrayFromImage(multi_channel_3Dimage)</span>
<span id="cb6-12"><a href="#cb6-12" tabindex="-1"></a></span>
<span id="cb6-13"><a href="#cb6-13" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Image size: "</span> <span class="op">+</span> <span class="bu">str</span>(multi_channel_3Dimage.GetSize()))</span>
<span id="cb6-14"><a href="#cb6-14" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Numpy array size: "</span> <span class="op">+</span> <span class="bu">str</span>(nda.shape))</span>
<span id="cb6-15"><a href="#cb6-15" tabindex="-1"></a></span>
<span id="cb6-16"><a href="#cb6-16" tabindex="-1"></a><span class="co"># Notice the index order and channel access are different:</span></span>
<span id="cb6-17"><a href="#cb6-17" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"First channel value in image: "</span> <span class="op">+</span> <span class="bu">str</span>(multi_channel_3Dimage[x, y, z][<span class="dv">0</span>]))</span>
<span id="cb6-18"><a href="#cb6-18" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"First channel value in numpy array: "</span> <span class="op">+</span> <span class="bu">str</span>(nda[z, y, x, <span class="dv">0</span>]))</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>Image size: (2, 4, 8)
Numpy array size: (8, 4, 2, 5)
First channel value in image: 0.5384010076522827
First channel value in numpy array: 0.538401</code></pre>
</div>
<p>Going back to the loaded file, let’s plot the array version of the
volume slice from the middle of the stack, along the z axis, using
different color maps:</p>
<div class="codewrapper sourceCode" id="cb8">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" tabindex="-1"></a>npa <span class="op">=</span> sitk.GetArrayViewFromImage(img_volume)</span>
<span id="cb8-2"><a href="#cb8-2" tabindex="-1"></a></span>
<span id="cb8-3"><a href="#cb8-3" tabindex="-1"></a><span class="co"># Display the image slice from the middle of the stack, z axis</span></span>
<span id="cb8-4"><a href="#cb8-4" tabindex="-1"></a>z <span class="op">=</span> <span class="bu">int</span>(img_volume.GetDepth() <span class="op">/</span> <span class="dv">2</span>)</span>
<span id="cb8-5"><a href="#cb8-5" tabindex="-1"></a>npa_zslice <span class="op">=</span> sitk.GetArrayViewFromImage(img_volume)[z, :, :]</span>
<span id="cb8-6"><a href="#cb8-6" tabindex="-1"></a></span>
<span id="cb8-7"><a href="#cb8-7" tabindex="-1"></a><span class="co"># Three plots displaying the same data, how do we deal with the high dynamic range?</span></span>
<span id="cb8-8"><a href="#cb8-8" tabindex="-1"></a>fig <span class="op">=</span> plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">3</span>))</span>
<span id="cb8-9"><a href="#cb8-9" tabindex="-1"></a></span>
<span id="cb8-10"><a href="#cb8-10" tabindex="-1"></a>fig.add_subplot(<span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">1</span>)</span>
<span id="cb8-11"><a href="#cb8-11" tabindex="-1"></a>plt.imshow(npa_zslice)</span>
<span id="cb8-12"><a href="#cb8-12" tabindex="-1"></a>plt.title(<span class="st">"default colormap"</span>, fontsize<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb8-13"><a href="#cb8-13" tabindex="-1"></a>plt.axis(<span class="st">"off"</span>)</span>
<span id="cb8-14"><a href="#cb8-14" tabindex="-1"></a></span>
<span id="cb8-15"><a href="#cb8-15" tabindex="-1"></a>fig.add_subplot(<span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">2</span>)</span>
<span id="cb8-16"><a href="#cb8-16" tabindex="-1"></a>plt.imshow(npa_zslice, cmap<span class="op">=</span>plt.cm.Greys_r)</span>
<span id="cb8-17"><a href="#cb8-17" tabindex="-1"></a>plt.title(<span class="st">"grey colormap"</span>, fontsize<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb8-18"><a href="#cb8-18" tabindex="-1"></a>plt.axis(<span class="st">"off"</span>)</span>
<span id="cb8-19"><a href="#cb8-19" tabindex="-1"></a></span>
<span id="cb8-20"><a href="#cb8-20" tabindex="-1"></a>fig.add_subplot(<span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">3</span>)</span>
<span id="cb8-21"><a href="#cb8-21" tabindex="-1"></a>plt.title(</span>
<span id="cb8-22"><a href="#cb8-22" tabindex="-1"></a>    <span class="st">"grey colormap,</span><span class="ch">\n</span><span class="st"> scaling based on volumetric min and max values"</span>, fontsize<span class="op">=</span><span class="dv">10</span></span>
<span id="cb8-23"><a href="#cb8-23" tabindex="-1"></a>)</span>
<span id="cb8-24"><a href="#cb8-24" tabindex="-1"></a>plt.imshow(npa_zslice, cmap<span class="op">=</span>plt.cm.Greys_r, vmin<span class="op">=</span>npa.<span class="bu">min</span>(), vmax<span class="op">=</span>npa.<span class="bu">max</span>())</span>
<span id="cb8-25"><a href="#cb8-25" tabindex="-1"></a>plt.axis(<span class="st">"off"</span>)</span></code></pre>
</div>
<figure><img src="../fig/slice_cmaps.png" alt="Slice and cmaps example." class="figure mx-auto d-block"></figure><p>We can also do the reverse, i.e. converting a <code>numpy</code>
array to the SITK Image:</p>
<div class="codewrapper sourceCode" id="cb9">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" tabindex="-1"></a>img_zslice <span class="op">=</span> sitk.GetImageFromArray(npa_zslice)</span>
<span id="cb9-2"><a href="#cb9-2" tabindex="-1"></a><span class="bu">print</span>(<span class="bu">type</span>(img_zslice))</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>&lt;class 'SimpleITK.SimpleITK.Image'&gt;</code></pre>
</div>
<p>We can also plot multiple slices at the same time, for better
inspecting the volume:</p>
<div class="codewrapper sourceCode" id="cb11">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" tabindex="-1"></a>img_xslices <span class="op">=</span> [img_volume[x,:,:] <span class="cf">for</span> x <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">50</span>, <span class="dv">200</span>, <span class="dv">30</span>)]</span>
<span id="cb11-2"><a href="#cb11-2" tabindex="-1"></a>img_yslices <span class="op">=</span> [img_volume[:,y,:] <span class="cf">for</span> y <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">50</span>, <span class="dv">200</span>, <span class="dv">30</span>)]</span>
<span id="cb11-3"><a href="#cb11-3" tabindex="-1"></a>img_zslices <span class="op">=</span> [img_volume[:,:,z] <span class="cf">for</span> z <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">50</span>, <span class="dv">200</span>, <span class="dv">30</span>)]</span>
<span id="cb11-4"><a href="#cb11-4" tabindex="-1"></a></span>
<span id="cb11-5"><a href="#cb11-5" tabindex="-1"></a>tile_x <span class="op">=</span> sitk.Tile(img_xslices, [<span class="dv">1</span>,<span class="dv">0</span>])</span>
<span id="cb11-6"><a href="#cb11-6" tabindex="-1"></a>tile_y <span class="op">=</span> sitk.Tile(img_yslices, [<span class="dv">1</span>,<span class="dv">0</span>])</span>
<span id="cb11-7"><a href="#cb11-7" tabindex="-1"></a>tile_z <span class="op">=</span> sitk.Tile(img_zslices, [<span class="dv">1</span>,<span class="dv">0</span>])</span>
<span id="cb11-8"><a href="#cb11-8" tabindex="-1"></a></span>
<span id="cb11-9"><a href="#cb11-9" tabindex="-1"></a>nda_xslices <span class="op">=</span> sitk.GetArrayViewFromImage(tile_x)</span>
<span id="cb11-10"><a href="#cb11-10" tabindex="-1"></a>nda_yslices <span class="op">=</span> sitk.GetArrayViewFromImage(tile_y)</span>
<span id="cb11-11"><a href="#cb11-11" tabindex="-1"></a>nda_zslices <span class="op">=</span> sitk.GetArrayViewFromImage(tile_z)</span>
<span id="cb11-12"><a href="#cb11-12" tabindex="-1"></a></span>
<span id="cb11-13"><a href="#cb11-13" tabindex="-1"></a>fig, (ax1, ax2, ax3) <span class="op">=</span> plt.subplots(<span class="dv">1</span>,<span class="dv">3</span>)</span>
<span id="cb11-14"><a href="#cb11-14" tabindex="-1"></a>ax1.imshow(nda_xslices, cmap<span class="op">=</span>plt.cm.Greys_r)</span>
<span id="cb11-15"><a href="#cb11-15" tabindex="-1"></a>ax2.imshow(nda_yslices, cmap<span class="op">=</span>plt.cm.Greys_r)</span>
<span id="cb11-16"><a href="#cb11-16" tabindex="-1"></a>ax3.imshow(nda_zslices, cmap<span class="op">=</span>plt.cm.Greys_r)</span>
<span id="cb11-17"><a href="#cb11-17" tabindex="-1"></a>ax1.set_title(<span class="st">'X slices'</span>)</span>
<span id="cb11-18"><a href="#cb11-18" tabindex="-1"></a>ax2.set_title(<span class="st">'Y slices'</span>)</span>
<span id="cb11-19"><a href="#cb11-19" tabindex="-1"></a>ax3.set_title(<span class="st">'Z slices'</span>)</span></code></pre>
</div>
<figure><img src="../fig/iso_slices.png" alt="Multiple slices example." class="figure mx-auto d-block"></figure><p>Operations like slice indexing, cropping, flipping, … can be
performed on SITK images very similarly as it is usually done in
<code>numpy</code>. Note that slicing of SITK images returns a copy of
the image data, similarly to slicing Python lists, and differently from
the “view” returned by slicing <code>numpy</code> arrays.</p>
<div class="codewrapper sourceCode" id="cb12">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" tabindex="-1"></a>n_slice <span class="op">=</span> <span class="dv">150</span></span>
<span id="cb12-2"><a href="#cb12-2" tabindex="-1"></a></span>
<span id="cb12-3"><a href="#cb12-3" tabindex="-1"></a><span class="co"># Original slice</span></span>
<span id="cb12-4"><a href="#cb12-4" tabindex="-1"></a>plt.imshow(sitk.GetArrayViewFromImage(img_volume[:, :, n_slice]), cmap<span class="op">=</span><span class="st">"gray"</span>)</span></code></pre>
</div>
<div class="codewrapper sourceCode" id="cb13">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" tabindex="-1"></a><span class="co"># Cropping</span></span>
<span id="cb13-2"><a href="#cb13-2" tabindex="-1"></a>plt.imshow(sitk.GetArrayViewFromImage(img_volume[:, :<span class="dv">100</span>, n_slice]), cmap<span class="op">=</span><span class="st">"gray"</span>)</span></code></pre>
</div>
<div class="codewrapper sourceCode" id="cb14">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" tabindex="-1"></a><span class="co"># Flipping</span></span>
<span id="cb14-2"><a href="#cb14-2" tabindex="-1"></a>plt.imshow(sitk.GetArrayViewFromImage(img_volume[:, ::<span class="op">-</span><span class="dv">1</span>, n_slice]), cmap<span class="op">=</span><span class="st">"gray"</span>)</span></code></pre>
</div>
<div class="codewrapper sourceCode" id="cb15">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" tabindex="-1"></a><span class="co"># Subsampling</span></span>
<span id="cb15-2"><a href="#cb15-2" tabindex="-1"></a>plt.imshow(sitk.GetArrayViewFromImage(img_volume[:, ::<span class="dv">3</span>, n_slice]), cmap<span class="op">=</span><span class="st">"gray"</span>)</span></code></pre>
</div>
<div class="codewrapper sourceCode" id="cb16">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" tabindex="-1"></a><span class="co"># Comparative operators</span></span>
<span id="cb16-2"><a href="#cb16-2" tabindex="-1"></a>plt.imshow(sitk.GetArrayViewFromImage(img_volume[:, :, n_slice] <span class="op">&gt;</span> <span class="dv">90</span>), cmap<span class="op">=</span><span class="st">"gray"</span>)</span></code></pre>
</div>
<div class="codewrapper sourceCode" id="cb17">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" tabindex="-1"></a><span class="co"># Draw a square</span></span>
<span id="cb17-2"><a href="#cb17-2" tabindex="-1"></a>draw_square <span class="op">=</span> sitk.GetArrayFromImage(img_volume[:, :, n_slice])</span>
<span id="cb17-3"><a href="#cb17-3" tabindex="-1"></a>draw_square[<span class="dv">0</span>:<span class="dv">100</span>,<span class="dv">0</span>:<span class="dv">100</span>] <span class="op">=</span> draw_square.<span class="bu">max</span>()</span>
<span id="cb17-4"><a href="#cb17-4" tabindex="-1"></a>plt.imshow(draw_square, cmap<span class="op">=</span><span class="st">"gray"</span>)</span></code></pre>
</div>
<figure><img src="../fig/sitk_operations.png" alt="Operations examples." class="figure mx-auto d-block"></figure><p>Another example of operation we can perform is creating a grid mask
and apply it to an image:</p>
<div class="codewrapper sourceCode" id="cb18">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" tabindex="-1"></a>img_zslice <span class="op">=</span> img_volume[:, :, n_slice]</span>
<span id="cb18-2"><a href="#cb18-2" tabindex="-1"></a><span class="co"># Create a grid and use as mask</span></span>
<span id="cb18-3"><a href="#cb18-3" tabindex="-1"></a>grid_image <span class="op">=</span> sitk.GridSource(</span>
<span id="cb18-4"><a href="#cb18-4" tabindex="-1"></a>    outputPixelType<span class="op">=</span>sitk.sitkUInt16,</span>
<span id="cb18-5"><a href="#cb18-5" tabindex="-1"></a>    size<span class="op">=</span>img_zslice.GetSize(),</span>
<span id="cb18-6"><a href="#cb18-6" tabindex="-1"></a>    sigma<span class="op">=</span>(<span class="fl">0.1</span>, <span class="fl">0.1</span>),</span>
<span id="cb18-7"><a href="#cb18-7" tabindex="-1"></a>    gridSpacing<span class="op">=</span>(<span class="fl">20.0</span>, <span class="fl">20.0</span>),</span>
<span id="cb18-8"><a href="#cb18-8" tabindex="-1"></a>)</span>
<span id="cb18-9"><a href="#cb18-9" tabindex="-1"></a></span>
<span id="cb18-10"><a href="#cb18-10" tabindex="-1"></a><span class="co"># zero out the values in the original image that correspond to</span></span>
<span id="cb18-11"><a href="#cb18-11" tabindex="-1"></a><span class="co"># the grid lines in the grid_image</span></span>
<span id="cb18-12"><a href="#cb18-12" tabindex="-1"></a>img_zslice[grid_image <span class="op">==</span> <span class="dv">0</span>] <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb18-13"><a href="#cb18-13" tabindex="-1"></a>nda <span class="op">=</span> sitk.GetArrayViewFromImage(img_zslice)</span>
<span id="cb18-14"><a href="#cb18-14" tabindex="-1"></a>plt.imshow(nda, cmap<span class="op">=</span><span class="st">"gray"</span>)</span></code></pre>
</div>
<div id="challenge-fix-the-error-optional" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="challenge-fix-the-error-optional" class="callout-inner">
<h3 class="callout-title">Challenge: Fix the Error (Optional)</h3>
<div class="callout-content">
<p>By running the lines of code above for masking the slice with the
grid, you will get an error. Can you guess what is it about?</p>
</div>
</div>
</div>
<div id="accordionSolution1" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution1" aria-expanded="false" aria-controls="collapseSolution1">
  <h4 class="accordion-header" id="headingSolution1"> Show me the solution </h4>
</button>
<div id="collapseSolution1" class="accordion-collapse collapse" data-bs-parent="#accordionSolution1" aria-labelledby="headingSolution1">
<div class="accordion-body">
<p>By default, SITK creates images centered in the origin, which all
ones spacing. We need to have the same values in the grid and in the
image in order to superimpose the first to the latter.</p>
<div class="codewrapper sourceCode" id="cb19">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" tabindex="-1"></a>grid_image.SetOrigin(img_zslice.GetOrigin())</span>
<span id="cb19-2"><a href="#cb19-2" tabindex="-1"></a>grid_image.SetSpacing(img_zslice.GetSpacing())</span></code></pre>
</div>
<p>Add these two lines to the code above, after the creation of the
grid. Everything should work fine now. Remember that making such changes
to an image already containing data should be done cautiously.</p>
<figure><img src="../fig/slice_grid.png" alt="Slice with grid mask." class="figure mx-auto d-block"></figure>
</div>
</div>
</div>
</div>
</div>
<div class="section level4">
<h4 id="meta-dictionaries">Meta-dictionaries<a class="anchor" aria-label="anchor" href="#meta-dictionaries"></a>
</h4>
<p>SITK can read (and write) images stored in a single file, or a set of
files (e.g. DICOM series).</p>
<p>Images stored in the DICOM format have a meta-data dictionary
associated with them, which is populated with the DICOM tags. When a
DICOM series is read as a single image, the meta-data information is not
available since DICOM tags are specific to each file. If you need the
meta-data, you have three options:</p>
<ol style="list-style-type: decimal">
<li><p>Using the object oriented interface’s <a href="https://simpleitk.org/doxygen/latest/html/classitk_1_1simple_1_1ImageSeriesReader.html" class="external-link">ImageSeriesReader</a>
class, configure it to load the tags using the
<code>MetaDataDictionaryArrayUpdateOn</code> method and possibly the
<code>LoadPrivateTagsOn</code> method if you need the private tags. Once
the series is read you can access the meta-data from the series reader
using the <code>GetMetaDataKeys</code>, <code>HasMetaDataKey</code>, and
<code>GetMetaData</code>.</p></li>
<li><p>Using the object oriented interface’s <a href="https://simpleitk.org/doxygen/latest/html/classitk_1_1simple_1_1ImageFileReader.html" class="external-link">ImageFileReader</a>,
set a specific slice’s file name and only read it’s meta-data using the
<code>ReadImageInformation</code> method which only reads the meta-data
but not the bulk pixel information. Once the meta-data is read you can
access it from the file reader using the <code>GetMetaDataKeys</code>,
<code>HasMetaDataKey</code>, and <code>GetMetaData</code>.</p></li>
<li><p>Using the object oriented interface’s <a href="https://simpleitk.org/doxygen/latest/html/classitk_1_1simple_1_1ImageFileReader.html" class="external-link">ImageFileReader</a>,
set a specific slice’s file name and read it. Or using the procedural
interface’s, <a href="https://simpleitk.org/doxygen/latest/html/namespaceitk_1_1simple.html#ae3b678b5b043c5a8c93aa616d5ee574c" class="external-link">ReadImage</a>
function, read a specific file. You can then access the meta-data
directly from the <a href="https://simpleitk.org/doxygen/latest/html/classitk_1_1simple_1_1Image.html" class="external-link">Image</a>
using the <code>GetMetaDataKeys</code>, <code>HasMetaDataKey</code>, and
<code>GetMetaData</code>.</p></li>
</ol>
<p><strong>Note</strong>: When reading an image series, via the
<code>ImageSeriesReader</code> or via the procedural
<code>ReadImage</code> interface, the images in the list are assumed to
be ordered correctly (<code>GetGDCMSeriesFileNames</code> ensures this
for DICOM). If the order is incorrect, the image will be read, but its
spacing and possibly the direction cosine matrix will be incorrect.</p>
<p>Let’s read in a digital x-ray image saved in a DICOM file format, and
let’s print the metadata’s keys:</p>
<div class="codewrapper sourceCode" id="cb20">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" tabindex="-1"></a>img_xray <span class="op">=</span> sitk.ReadImage(<span class="st">'data/sitk/digital_xray.dcm'</span>)</span>
<span id="cb20-2"><a href="#cb20-2" tabindex="-1"></a></span>
<span id="cb20-3"><a href="#cb20-3" tabindex="-1"></a><span class="cf">for</span> key <span class="kw">in</span> img_xray.GetMetaDataKeys():</span>
<span id="cb20-4"><a href="#cb20-4" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f'"</span><span class="sc">{</span>key<span class="sc">}</span><span class="ss">":"</span><span class="sc">{</span>img_xray<span class="sc">.</span>GetMetaData(key)<span class="sc">}</span><span class="ss">"'</span>)</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code><span><span class="st">"0008|0005"</span><span class="op">:</span><span class="st">"ISO_IR 100"</span></span>
<span><span class="st">"0008|0012"</span><span class="op">:</span><span class="st">"20180713"</span></span>
<span><span class="st">"0008|0013"</span><span class="op">:</span><span class="st">"233245.431"</span></span>
<span><span class="st">"0008|0016"</span><span class="op">:</span><span class="st">"1.2.840.10008.5.1.4.1.1.7"</span></span>
<span><span class="st">"0008|0018"</span><span class="op">:</span><span class="st">"2.25.225981116244996633889747723103230447077"</span></span>
<span><span class="st">"0008|0020"</span><span class="op">:</span><span class="st">"20160208"</span></span>
<span><span class="st">"0008|0060"</span><span class="op">:</span><span class="st">"XC"</span></span>
<span><span class="st">"0020|000d"</span><span class="op">:</span><span class="st">"2.25.59412532821774470466514450673479191872"</span></span>
<span><span class="st">"0020|000e"</span><span class="op">:</span><span class="st">"2.25.149748537243964334146339164752570719260"</span></span>
<span><span class="st">"0028|0002"</span><span class="op">:</span><span class="st">"3"</span></span>
<span><span class="st">"0028|0004"</span><span class="op">:</span><span class="st">"YBR_FULL_422"</span></span>
<span><span class="st">"0028|0006"</span><span class="op">:</span><span class="st">"0"</span></span>
<span><span class="st">"0028|0010"</span><span class="op">:</span><span class="st">"357"</span></span>
<span><span class="st">"0028|0011"</span><span class="op">:</span><span class="st">"371"</span></span>
<span><span class="st">"0028|0100"</span><span class="op">:</span><span class="st">"8"</span></span>
<span><span class="st">"0028|0101"</span><span class="op">:</span><span class="st">"8"</span></span>
<span><span class="st">"0028|0102"</span><span class="op">:</span><span class="st">"7"</span></span>
<span><span class="st">"0028|0103"</span><span class="op">:</span><span class="st">"0"</span></span>
<span><span class="st">"ITK_original_direction"</span><span class="op">:</span><span class="st">"[UNKNOWN_PRINT_CHARACTERISTICS]</span></span>
<span><span class="st">"</span></span>
<span><span class="st">"ITK_original_spacing"</span><span class="op">:</span><span class="st">"[UNKNOWN_PRINT_CHARACTERISTICS]</span></span>
<span><span class="st">"</span></span></code></pre>
</div>
<div id="reading-a-dicom" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<div id="reading-a-dicom" class="callout-inner">
<h3 class="callout-title">Reading a DICOM:</h3>
<div class="callout-content">
<ul>
<li>Many libraries allow you to read DICOM metadata.</li>
<li>PyDICOM will be explored for this task later.</li>
<li>If you are already using SITK, you will usually not need an extra
library to get DICOM metadata.</li>
<li>Many libraries have some basic DICOM functionality, check the
documentation before adding extra dependencies.</li>
</ul>
</div>
</div>
</div>
<p>Generally speaking, SITK represents color images as multi-channel
images independent of a <a href="https://en.wikipedia.org/wiki/Color_space" class="external-link">color space</a>. It is
up to you to interpret the channels correctly based on additional color
space knowledge prior to using them for display or any other
purpose.</p>
<p>Things to note: 1. When using SITK to read a color DICOM image, the
channel values will be transformed to the RGB color space. 2. When using
SITK to read a scalar image, it is assumed that the lowest intensity
value is black and highest white. If the photometric interpretation tag
is MONOCHROME2 (lowest value displayed as black) nothing is done. If it
is MONOCHROME1 (lowest value displayed as white), the pixel values are
inverted.</p>
<div class="codewrapper sourceCode" id="cb22">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'Image Modality: </span><span class="sc">{</span>img_xray<span class="sc">.</span>GetMetaData(<span class="st">"0008|0060"</span>)<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb22-2"><a href="#cb22-2" tabindex="-1"></a><span class="bu">print</span>(img_xray.GetNumberOfComponentsPerPixel())</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>Image Modality: XC
3</code></pre>
</div>
<p>“0008|0060” is a code indicating the modality, and “XC” stays for
“External-camera Photography”.</p>
<div id="grayscale-images-stored-as-srgb" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<div id="grayscale-images-stored-as-srgb" class="callout-inner">
<h3 class="callout-title">Grayscale Images Stored as sRGB</h3>
<div class="callout-content">
<p>“digital_xray.dcm” image is sRGB, even if an x-ray should be a single
channel gray scale image. In some cases looks may be deceiving. Gray
scale images are not always stored as a single channel image. In some
cases an image that looks like a gray scale image is actually a three
channel image with the intensity values repeated in each of the
channels. Even worse, some gray scale images can be four channel images
with the channels representing RGBA and the alpha channel set to all
255. This can result in a significant waste of memory and computation
time. Always become familiar with your data.</p>
<p>We can <a href="https://en.wikipedia.org/wiki/Grayscale#Converting_color_to_grayscale" class="external-link">convert
sRGB to gray scale</a>:</p>
<div class="codewrapper sourceCode" id="cb24">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" tabindex="-1"></a>nda_xray <span class="op">=</span> sitk.GetArrayViewFromImage(img_xray)</span>
<span id="cb24-2"><a href="#cb24-2" tabindex="-1"></a>plt.imshow(np.squeeze(nda_xray))</span></code></pre>
</div>
<figure><img src="../fig/xray_srgb.png" alt="Digital x-ray image." class="figure mx-auto d-block"></figure><div class="codewrapper sourceCode" id="cb25">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" tabindex="-1"></a><span class="kw">def</span> srgb2gray(image):</span>
<span id="cb25-2"><a href="#cb25-2" tabindex="-1"></a>    <span class="co"># Convert sRGB image to gray scale and rescale results to [0,255]</span></span>
<span id="cb25-3"><a href="#cb25-3" tabindex="-1"></a>    channels <span class="op">=</span> [</span>
<span id="cb25-4"><a href="#cb25-4" tabindex="-1"></a>        sitk.VectorIndexSelectionCast(image, i, sitk.sitkFloat32)</span>
<span id="cb25-5"><a href="#cb25-5" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(image.GetNumberOfComponentsPerPixel())</span>
<span id="cb25-6"><a href="#cb25-6" tabindex="-1"></a>    ]</span>
<span id="cb25-7"><a href="#cb25-7" tabindex="-1"></a>    <span class="co"># linear mapping</span></span>
<span id="cb25-8"><a href="#cb25-8" tabindex="-1"></a>    I <span class="op">=</span> <span class="dv">1</span> <span class="op">/</span> <span class="fl">255.0</span> <span class="op">*</span> (<span class="fl">0.2126</span> <span class="op">*</span> channels[<span class="dv">0</span>] <span class="op">+</span> <span class="fl">0.7152</span> <span class="op">*</span> channels[<span class="dv">1</span>] <span class="op">+</span> <span class="fl">0.0722</span> <span class="op">*</span> channels[<span class="dv">2</span>])</span>
<span id="cb25-9"><a href="#cb25-9" tabindex="-1"></a>    <span class="co"># nonlinear gamma correction</span></span>
<span id="cb25-10"><a href="#cb25-10" tabindex="-1"></a>    I <span class="op">=</span> (</span>
<span id="cb25-11"><a href="#cb25-11" tabindex="-1"></a>        I <span class="op">*</span> sitk.Cast(I <span class="op">&lt;=</span> <span class="fl">0.0031308</span>, sitk.sitkFloat32) <span class="op">*</span> <span class="fl">12.92</span></span>
<span id="cb25-12"><a href="#cb25-12" tabindex="-1"></a>        <span class="op">+</span> I <span class="op">**</span> (<span class="dv">1</span> <span class="op">/</span> <span class="fl">2.4</span>) <span class="op">*</span> sitk.Cast(I <span class="op">&gt;</span> <span class="fl">0.0031308</span>, sitk.sitkFloat32) <span class="op">*</span> <span class="fl">1.055</span></span>
<span id="cb25-13"><a href="#cb25-13" tabindex="-1"></a>        <span class="op">-</span> <span class="fl">0.055</span></span>
<span id="cb25-14"><a href="#cb25-14" tabindex="-1"></a>    )</span>
<span id="cb25-15"><a href="#cb25-15" tabindex="-1"></a>    <span class="cf">return</span> sitk.Cast(sitk.RescaleIntensity(I), sitk.sitkUInt8)</span>
<span id="cb25-16"><a href="#cb25-16" tabindex="-1"></a></span>
<span id="cb25-17"><a href="#cb25-17" tabindex="-1"></a>img_xray_gray <span class="op">=</span> srgb2gray(img_xray)</span>
<span id="cb25-18"><a href="#cb25-18" tabindex="-1"></a>nda <span class="op">=</span> sitk.GetArrayViewFromImage(img_xray_gray)</span>
<span id="cb25-19"><a href="#cb25-19" tabindex="-1"></a>plt.imshow(np.squeeze(nda), cmap<span class="op">=</span><span class="st">"gray"</span>)</span></code></pre>
</div>
<figure><img src="../fig/xray_gs.png" alt="Grayscale x-ray image." class="figure mx-auto d-block"></figure>
</div>
</div>
</div>
</div>
</div>
<div class="section level3">
<h3 id="transforms">Transforms<a class="anchor" aria-label="anchor" href="#transforms"></a>
</h3>
<p>SITK supports two types of spatial transforms, ones with a global
(unbounded) spatial domain and ones with a bounded spatial domain.
Points in SITK are mapped by the transform using the
<code>TransformPoint</code> method.</p>
<p>All <strong>global domain transforms</strong> are of the form:</p>
<p><span class="math inline">\(T(x) = A(x - c) + t + c\)</span></p>
<p>The nomenclature used in the documentation refers to the components
of the transformations as follows:</p>
<ul>
<li>Matrix - the matrix <span class="math inline">\(A\)</span>.</li>
<li>Center - the point <span class="math inline">\(c\)</span>.</li>
<li>Translation - the vector <span class="math inline">\(t\)</span>.</li>
<li>Offset - the expression <span class="math inline">\(t + c -
Ac\)</span>.</li>
</ul>
<p>A variety of global 2D and 3D transformations are available
(translation, rotation, rigid, similarity, affine…). Some of these
transformations are available with various parameterizations which are
useful for registration purposes.</p>
<p>The second type of spatial transformation, <strong>bounded domain
transformations</strong>, are defined to be identity outside their
domain. These include the B-spline deformable transformation, often
referred to as Free-Form Deformation, and the displacement field
transformation.</p>
<p>The B-spline transform uses a grid of control points to represent a
spline based transformation. To specify the transformation the user
defines the number of control points and the spatial region which they
overlap. The spline order can also be set, though the default of cubic
is appropriate in most cases. The displacement field transformation uses
a dense set of vectors representing displacement in a bounded spatial
domain. It has no implicit constraints on transformation continuity or
smoothness.</p>
<p>Finally, SITK supports a <strong>composite transformation</strong>
with either a bounded or global domain. This transformation represents
multiple transformations applied one after the other <span class="math inline">\(T_0(T_1(T_2(...T_n(p))))\)</span>. The semantics
are stack based, that is, first in last applied:</p>
<div class="codewrapper sourceCode" id="cb26">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" tabindex="-1"></a>composite_transform <span class="op">=</span> CompositeTransform([T0, T1])</span>
<span id="cb26-2"><a href="#cb26-2" tabindex="-1"></a>composite_transform.AddTransform(T2)</span></code></pre>
</div>
<p>For more details about SITK transformation types and examples, see <a href="https://insightsoftwareconsortium.github.io/SimpleITK-Notebooks/Python_html/22_Transforms.html" class="external-link">this
tutorial notebook</a>.</p>
</div>
<div class="section level3">
<h3 id="resampling">Resampling<a class="anchor" aria-label="anchor" href="#resampling"></a>
</h3>
<p>Resampling, as the verb implies, is the action of sampling an image,
which itself is a sampling of an original continuous signal.</p>
<p>Generally speaking, resampling in SITK involves four components:</p>
<ol style="list-style-type: decimal">
<li>Image - the image we resample, given in coordinate system <span class="math inline">\(m\)</span>.</li>
<li>Resampling grid - a regular grid of points given in coordinate
system <span class="math inline">\(f\)</span> which will be mapped to
coordinate system <span class="math inline">\(m\)</span>.</li>
<li>Transformation <span class="math inline">\(T_f^m\)</span> - maps
points from coordinate system <span class="math inline">\(f\)</span> to
coordinate system <span class="math inline">\(m\)</span>, <span class="math inline">\(^mp=T_f^m(^fp)\)</span>.</li>
<li>Interpolator - method for obtaining the intensity values at
arbitrary points in coordinate system <span class="math inline">\(m\)</span> from the values of the points defined
by the Image.</li>
</ol>
<p>While SITK provides a large number of interpolation methods, the two
most commonly used are sitkLinear and sitkNearestNeighbor. The former is
used for most interpolation tasks and is a compromise between accuracy
and computational efficiency. The later is used to interpolate labeled
images representing a segmentation. It is the only interpolation
approach which will not introduce new labels into the result.</p>
<p>The SITK interface includes three variants for specifying the
resampling grid:</p>
<ol style="list-style-type: decimal">
<li>Use the same grid as defined by the resampled image.</li>
<li>Provide a second, reference, image which defines the grid.</li>
<li>Specify the grid using: size, origin, spacing, and direction cosine
matrix.</li>
</ol>
<p>Points that are mapped outside of the resampled image’s spatial
extent in physical space are set to a constant pixel value which you
provide (default is zero).</p>
<p>It is not uncommon to end up with an empty (all black) image after
resampling. This is due to:</p>
<ol style="list-style-type: decimal">
<li>Using wrong settings for the resampling grid (not too common, but
does happen).</li>
<li>Using the inverse of the transformation <span class="math inline">\(T_f^m\)</span>. This is a relatively common error,
which is readily addressed by invoking the transformation’s
<code>GetInverse</code> method.</li>
</ol>
<p>Let’s try to plot multiple slices across different axis for the image
“training_001_mr_T1.mha”.</p>
<div class="codewrapper sourceCode" id="cb27">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" tabindex="-1"></a>img_volume <span class="op">=</span> sitk.ReadImage(<span class="st">"data/sitk/training_001_mr_T1.mha"</span>)</span>
<span id="cb27-2"><a href="#cb27-2" tabindex="-1"></a><span class="bu">print</span>(img_volume.GetSize())</span>
<span id="cb27-3"><a href="#cb27-3" tabindex="-1"></a><span class="bu">print</span>(img_volume.GetSpacing())</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>(256, 256, 26)
(1.25, 1.25, 4.0)</code></pre>
</div>
<p>We can plot the slices as we did before:</p>
<div class="codewrapper sourceCode" id="cb29">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" tabindex="-1"></a>img_xslices <span class="op">=</span> [img_volume[x,:,:] <span class="cf">for</span> x <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">50</span>, <span class="dv">200</span>, <span class="dv">30</span>)]</span>
<span id="cb29-2"><a href="#cb29-2" tabindex="-1"></a>img_yslices <span class="op">=</span> [img_volume[:,y,:] <span class="cf">for</span> y <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">50</span>, <span class="dv">200</span>, <span class="dv">30</span>)]</span>
<span id="cb29-3"><a href="#cb29-3" tabindex="-1"></a>img_zslices <span class="op">=</span> [img_volume[:,:,z] <span class="cf">for</span> z <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, <span class="dv">25</span>, <span class="dv">3</span>)]</span>
<span id="cb29-4"><a href="#cb29-4" tabindex="-1"></a></span>
<span id="cb29-5"><a href="#cb29-5" tabindex="-1"></a>tile_x <span class="op">=</span> sitk.Tile(img_xslices, [<span class="dv">1</span>,<span class="dv">0</span>])</span>
<span id="cb29-6"><a href="#cb29-6" tabindex="-1"></a>tile_y <span class="op">=</span> sitk.Tile(img_yslices, [<span class="dv">1</span>,<span class="dv">0</span>])</span>
<span id="cb29-7"><a href="#cb29-7" tabindex="-1"></a>tile_z <span class="op">=</span> sitk.Tile(img_zslices, [<span class="dv">1</span>,<span class="dv">0</span>])</span>
<span id="cb29-8"><a href="#cb29-8" tabindex="-1"></a></span>
<span id="cb29-9"><a href="#cb29-9" tabindex="-1"></a>nda_xslices <span class="op">=</span> sitk.GetArrayViewFromImage(tile_x)</span>
<span id="cb29-10"><a href="#cb29-10" tabindex="-1"></a>nda_yslices <span class="op">=</span> sitk.GetArrayViewFromImage(tile_y)</span>
<span id="cb29-11"><a href="#cb29-11" tabindex="-1"></a>nda_zslices <span class="op">=</span> sitk.GetArrayViewFromImage(tile_z)</span>
<span id="cb29-12"><a href="#cb29-12" tabindex="-1"></a></span>
<span id="cb29-13"><a href="#cb29-13" tabindex="-1"></a>fig, (ax1, ax2, ax3) <span class="op">=</span> plt.subplots(<span class="dv">1</span>,<span class="dv">3</span>)</span>
<span id="cb29-14"><a href="#cb29-14" tabindex="-1"></a>ax1.imshow(nda_xslices, cmap<span class="op">=</span>plt.cm.Greys_r)</span>
<span id="cb29-15"><a href="#cb29-15" tabindex="-1"></a>ax2.imshow(nda_yslices, cmap<span class="op">=</span>plt.cm.Greys_r)</span>
<span id="cb29-16"><a href="#cb29-16" tabindex="-1"></a>ax3.imshow(nda_zslices, cmap<span class="op">=</span>plt.cm.Greys_r)</span>
<span id="cb29-17"><a href="#cb29-17" tabindex="-1"></a>ax1.set_title(<span class="st">'X slices'</span>)</span>
<span id="cb29-18"><a href="#cb29-18" tabindex="-1"></a>ax2.set_title(<span class="st">'Y slices'</span>)</span>
<span id="cb29-19"><a href="#cb29-19" tabindex="-1"></a>ax3.set_title(<span class="st">'Z slices'</span>)</span></code></pre>
</div>
<figure><img src="../fig/non-iso_slices.png" alt="Non-isotropic slices example." class="figure mx-auto d-block"></figure><div id="challenge-distorted-images" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="challenge-distorted-images" class="callout-inner">
<h3 class="callout-title">Challenge: Distorted Images</h3>
<div class="callout-content">
<p>What is the main difference with the first image we plotted
(“A1_grayT1.nrrd”)?</p>
</div>
</div>
</div>
<div id="accordionSolution2" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution2" aria-expanded="false" aria-controls="collapseSolution2">
  <h4 class="accordion-header" id="headingSolution2"> Show me the solution </h4>
</button>
<div id="collapseSolution2" class="accordion-collapse collapse" data-bs-parent="#accordionSolution2" aria-labelledby="headingSolution2">
<div class="accordion-body">
<p>In this case, there are only 26 images in the volume and the spacing
between voxels is non-isotropic, and in particular it is the same across
x- and y-axis, but it differs across the z-axis.</p>
</div>
</div>
</div>
</div>
<p>We can fix the distortion by resampling the volume along the z-axis,
which has a different spacing (i.e., 4mm), and make it match with the
other two spacing measures (i.e., 1.25mm):</p>
<div class="codewrapper sourceCode" id="cb30">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" tabindex="-1"></a><span class="kw">def</span> resample_img(image, out_spacing<span class="op">=</span>[<span class="fl">1.25</span>, <span class="fl">1.25</span>, <span class="fl">1.25</span>]):</span>
<span id="cb30-2"><a href="#cb30-2" tabindex="-1"></a>    </span>
<span id="cb30-3"><a href="#cb30-3" tabindex="-1"></a>    <span class="co"># Resample images to 1.25mm spacing</span></span>
<span id="cb30-4"><a href="#cb30-4" tabindex="-1"></a>    original_spacing <span class="op">=</span> image.GetSpacing()</span>
<span id="cb30-5"><a href="#cb30-5" tabindex="-1"></a>    original_size <span class="op">=</span> image.GetSize()</span>
<span id="cb30-6"><a href="#cb30-6" tabindex="-1"></a></span>
<span id="cb30-7"><a href="#cb30-7" tabindex="-1"></a>    out_size <span class="op">=</span> [</span>
<span id="cb30-8"><a href="#cb30-8" tabindex="-1"></a>        <span class="bu">int</span>(np.<span class="bu">round</span>(original_size[<span class="dv">0</span>] <span class="op">*</span> (original_spacing[<span class="dv">0</span>] <span class="op">/</span> out_spacing[<span class="dv">0</span>]))),</span>
<span id="cb30-9"><a href="#cb30-9" tabindex="-1"></a>        <span class="bu">int</span>(np.<span class="bu">round</span>(original_size[<span class="dv">1</span>] <span class="op">*</span> (original_spacing[<span class="dv">1</span>] <span class="op">/</span> out_spacing[<span class="dv">1</span>]))),</span>
<span id="cb30-10"><a href="#cb30-10" tabindex="-1"></a>        <span class="bu">int</span>(np.<span class="bu">round</span>(original_size[<span class="dv">2</span>] <span class="op">*</span> (original_spacing[<span class="dv">2</span>] <span class="op">/</span> out_spacing[<span class="dv">2</span>])))]</span>
<span id="cb30-11"><a href="#cb30-11" tabindex="-1"></a></span>
<span id="cb30-12"><a href="#cb30-12" tabindex="-1"></a>    resample <span class="op">=</span> sitk.ResampleImageFilter()</span>
<span id="cb30-13"><a href="#cb30-13" tabindex="-1"></a>    resample.SetOutputSpacing(out_spacing)</span>
<span id="cb30-14"><a href="#cb30-14" tabindex="-1"></a>    resample.SetSize(out_size)</span>
<span id="cb30-15"><a href="#cb30-15" tabindex="-1"></a>    resample.SetOutputDirection(image.GetDirection())</span>
<span id="cb30-16"><a href="#cb30-16" tabindex="-1"></a>    resample.SetOutputOrigin(image.GetOrigin())</span>
<span id="cb30-17"><a href="#cb30-17" tabindex="-1"></a>    resample.SetTransform(sitk.Transform())</span>
<span id="cb30-18"><a href="#cb30-18" tabindex="-1"></a>    resample.SetDefaultPixelValue(image.GetPixelIDValue())</span>
<span id="cb30-19"><a href="#cb30-19" tabindex="-1"></a>    resample.SetInterpolator(sitk.sitkBSpline)</span>
<span id="cb30-20"><a href="#cb30-20" tabindex="-1"></a></span>
<span id="cb30-21"><a href="#cb30-21" tabindex="-1"></a>    <span class="cf">return</span> resample.Execute(image)</span>
<span id="cb30-22"><a href="#cb30-22" tabindex="-1"></a></span>
<span id="cb30-23"><a href="#cb30-23" tabindex="-1"></a>resampled_sitk_img <span class="op">=</span> resample_img(img_volume)</span>
<span id="cb30-24"><a href="#cb30-24" tabindex="-1"></a></span>
<span id="cb30-25"><a href="#cb30-25" tabindex="-1"></a><span class="bu">print</span>(resampled_sitk_img.GetSize())</span>
<span id="cb30-26"><a href="#cb30-26" tabindex="-1"></a><span class="bu">print</span>(resampled_sitk_img.GetSpacing())</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>(256, 256, 83)
(1.25, 1.25, 1.25)</code></pre>
</div>
</div>
</section><section><h2 class="section-heading" id="registration">Registration<a class="anchor" aria-label="anchor" href="#registration"></a>
</h2>
<hr class="half-width">
<p>Image registration involves spatially transforming the source/moving
image(s) to align with the target image. More specifically, the goal of
registration is to estimate the transformation which maps points from
one image to the corresponding points in another image. The
transformation estimated via registration is said to map points from the
<strong>fixed image</strong> (target image) coordinate system to the
<strong>moving image</strong> (source image) coordinate system.</p>
<div id="many-ways-to-do-registration" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<div id="many-ways-to-do-registration" class="callout-inner">
<h3 class="callout-title">Many Ways to Do Registration:</h3>
<div class="callout-content">
<ul>
<li>Several libraries offer built-in registration functionalities.</li>
<li>Registration can be performed with <a href="https://dipy.org/index.html" class="external-link">DIPY</a> (mentioned in the MRI
episode), specifically for diffusion imaging using the
<code>SymmetricDiffeomorphicRegistration</code> functionality.</li>
<li>
<a href="https://nipy.org/nibabel/" class="external-link">NiBabel</a> includes a function
in its processing module that allows resampling or conforming one NIfTI
image into the space of another.</li>
<li>SITK provides robust registration capabilities and allows you to
code your own registration algorithms.</li>
<li>To maintain cleaner and more efficient code, it’s advisable to use
as few libraries as possible and avoid those that may create
conflicts.</li>
</ul>
</div>
</div>
</div>
<p>SITK provides a configurable multi-resolution registration framework,
implemented in the <a href="https://simpleitk.org/doxygen/latest/html/classitk_1_1simple_1_1ImageRegistrationMethod.html" class="external-link">ImageRegistrationMethod</a>
class. In addition, a number of variations of the Demons registration
algorithm are implemented independently from this class as they do not
fit into the framework.</p>
<p>The task of registration is formulated using non-linear optimization
which requires an initial estimate. The two most common initialization
approaches are (1) Use the identity transform (a.k.a. forgot to
initialize). (2) Align the physical centers of the two images (see <a href="https://simpleitk.org/doxygen/latest/html/classitk_1_1simple_1_1CenteredTransformInitializerFilter.html" class="external-link">CenteredTransformInitializerFilter</a>).
If after initialization there is no overlap between the images,
registration will fail. The closer the initialization transformation is
to the actual transformation, the higher the probability of convergence
to the correct solution.</p>
<p>If your registration involves the use of a global domain transform
(<a href="https://simpleitk.readthedocs.io/en/master/fundamentalConcepts.html#lbl-transforms" class="external-link">described
here</a>), you should also set an appropriate center of rotation. In
many cases you want the center of rotation to be the physical center of
the fixed image (the CenteredTransformCenteredTransformInitializerFilter
ensures this). This is of significant importance for registration
convergence due to the non-linear nature of rotation. When the center of
rotation is far from our physical region of interest (ROI), a small
rotational angle results in a large displacement. Think of moving the
pivot/fulcrum point of a <a href="https://en.wikipedia.org/wiki/Lever" class="external-link">lever</a>. For the same
rotation angle, the farther you are from the fulcrum the larger the
displacement. For numerical stability we do not want our computations to
be sensitive to very small variations in the rotation angle, thus the
ideal center of rotation is the point which minimizes the distance to
the farthest point in our ROI:</p>
<p><span class="math inline">\(p_{center} = arg_{p_{rotation}} min dist
(p_{rotation}, \{p_{roi}\})\)</span></p>
<p>Without additional knowledge we can only assume that the ROI is the
whole fixed image. If your ROI is only in a sub region of the image, a
more appropriate point would be the center of the oriented bounding box
of that ROI.</p>
<p>To create a specific registration instance using the
ImageRegistrationMethod you need to select several components which
together define the registration instance:</p>
<ol style="list-style-type: decimal">
<li>Transformation
<ul>
<li>It defines the mapping between the two images.</li>
</ul>
</li>
<li>Similarity metric
<ul>
<li>It reflects the relationship between the intensities of the images
(identity, affine, stochastic…).</li>
</ul>
</li>
<li>Optimizer.
<ul>
<li>When selecting the optimizer you will also need to configure it
(e.g. set the number of iterations).</li>
</ul>
</li>
<li>Interpolator.
<ul>
<li>In most cases linear interpolation, the default setting, is
sufficient.</li>
</ul>
</li>
</ol>
<p>Let’s see now an example where we want to use registration for
aligning two volumes relative to the same patient, one being a CT scan
and the second being a MRI sequence T1-weighted scan. We first read the
images, casting the pixel type to that required for registration
(Float32 or Float64) and look at them:</p>
<div class="codewrapper sourceCode" id="cb32">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" tabindex="-1"></a><span class="im">from</span> ipywidgets <span class="im">import</span> interact, fixed</span>
<span id="cb32-2"><a href="#cb32-2" tabindex="-1"></a><span class="im">from</span> IPython.display <span class="im">import</span> clear_output</span>
<span id="cb32-3"><a href="#cb32-3" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb32-4"><a href="#cb32-4" tabindex="-1"></a></span>
<span id="cb32-5"><a href="#cb32-5" tabindex="-1"></a>OUTPUT_DIR <span class="op">=</span> <span class="st">"data/sitk/"</span></span>
<span id="cb32-6"><a href="#cb32-6" tabindex="-1"></a>fixed_image <span class="op">=</span>  sitk.ReadImage(<span class="ss">f"</span><span class="sc">{</span>OUTPUT_DIR<span class="sc">}</span><span class="ss">training_001_ct.mha"</span>, sitk.sitkFloat32)</span>
<span id="cb32-7"><a href="#cb32-7" tabindex="-1"></a>moving_image <span class="op">=</span> sitk.ReadImage(<span class="ss">f"</span><span class="sc">{</span>OUTPUT_DIR<span class="sc">}</span><span class="ss">training_001_mr_T1.mha"</span>, sitk.sitkFloat32)</span>
<span id="cb32-8"><a href="#cb32-8" tabindex="-1"></a></span>
<span id="cb32-9"><a href="#cb32-9" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Origin for fixed image: </span><span class="sc">{</span>fixed_image<span class="sc">.</span>GetOrigin()<span class="sc">}</span><span class="ss">, moving image: </span><span class="sc">{</span>moving_image<span class="sc">.</span>GetOrigin()<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb32-10"><a href="#cb32-10" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Spacing for fixed image: </span><span class="sc">{</span>fixed_image<span class="sc">.</span>GetSpacing()<span class="sc">}</span><span class="ss">, moving image: </span><span class="sc">{</span>moving_image<span class="sc">.</span>GetSpacing()<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb32-11"><a href="#cb32-11" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Size for fixed image: </span><span class="sc">{</span>fixed_image<span class="sc">.</span>GetSize()<span class="sc">}</span><span class="ss">, moving image: </span><span class="sc">{</span>moving_image<span class="sc">.</span>GetSize()<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb32-12"><a href="#cb32-12" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Number Of Components Per Pixel for fixed image: </span><span class="sc">{</span>fixed_image<span class="sc">.</span>GetNumberOfComponentsPerPixel()<span class="sc">}</span><span class="ss">, moving image: </span><span class="sc">{</span>moving_image<span class="sc">.</span>GetNumberOfComponentsPerPixel()<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb32-13"><a href="#cb32-13" tabindex="-1"></a></span>
<span id="cb32-14"><a href="#cb32-14" tabindex="-1"></a><span class="co"># Callback invoked by the interact IPython method for scrolling through the image stacks of</span></span>
<span id="cb32-15"><a href="#cb32-15" tabindex="-1"></a><span class="co"># the two images (moving and fixed).</span></span>
<span id="cb32-16"><a href="#cb32-16" tabindex="-1"></a><span class="kw">def</span> display_images(fixed_image_z, moving_image_z, fixed_npa, moving_npa):</span>
<span id="cb32-17"><a href="#cb32-17" tabindex="-1"></a>    <span class="co"># Create a figure with two subplots and the specified size.</span></span>
<span id="cb32-18"><a href="#cb32-18" tabindex="-1"></a>    plt.subplots(<span class="dv">1</span>,<span class="dv">2</span>,figsize<span class="op">=</span>(<span class="dv">10</span>,<span class="dv">8</span>))</span>
<span id="cb32-19"><a href="#cb32-19" tabindex="-1"></a>    </span>
<span id="cb32-20"><a href="#cb32-20" tabindex="-1"></a>    <span class="co"># Draw the fixed image in the first subplot.</span></span>
<span id="cb32-21"><a href="#cb32-21" tabindex="-1"></a>    plt.subplot(<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">1</span>)</span>
<span id="cb32-22"><a href="#cb32-22" tabindex="-1"></a>    plt.imshow(fixed_npa[fixed_image_z,:,:],cmap<span class="op">=</span>plt.cm.Greys_r)</span>
<span id="cb32-23"><a href="#cb32-23" tabindex="-1"></a>    plt.title(<span class="st">'fixed/target image'</span>)</span>
<span id="cb32-24"><a href="#cb32-24" tabindex="-1"></a>    plt.axis(<span class="st">'off'</span>)</span>
<span id="cb32-25"><a href="#cb32-25" tabindex="-1"></a>    </span>
<span id="cb32-26"><a href="#cb32-26" tabindex="-1"></a>    <span class="co"># Draw the moving image in the second subplot.</span></span>
<span id="cb32-27"><a href="#cb32-27" tabindex="-1"></a>    plt.subplot(<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">2</span>)</span>
<span id="cb32-28"><a href="#cb32-28" tabindex="-1"></a>    plt.imshow(moving_npa[moving_image_z,:,:],cmap<span class="op">=</span>plt.cm.Greys_r)</span>
<span id="cb32-29"><a href="#cb32-29" tabindex="-1"></a>    plt.title(<span class="st">'moving/source image'</span>)</span>
<span id="cb32-30"><a href="#cb32-30" tabindex="-1"></a>    plt.axis(<span class="st">'off'</span>)</span>
<span id="cb32-31"><a href="#cb32-31" tabindex="-1"></a>    </span>
<span id="cb32-32"><a href="#cb32-32" tabindex="-1"></a>    plt.show()</span>
<span id="cb32-33"><a href="#cb32-33" tabindex="-1"></a></span>
<span id="cb32-34"><a href="#cb32-34" tabindex="-1"></a>interact(</span>
<span id="cb32-35"><a href="#cb32-35" tabindex="-1"></a>    display_images,</span>
<span id="cb32-36"><a href="#cb32-36" tabindex="-1"></a>    fixed_image_z <span class="op">=</span> (<span class="dv">0</span>,fixed_image.GetSize()[<span class="dv">2</span>]<span class="op">-</span><span class="dv">1</span>),</span>
<span id="cb32-37"><a href="#cb32-37" tabindex="-1"></a>    moving_image_z <span class="op">=</span> (<span class="dv">0</span>,moving_image.GetSize()[<span class="dv">2</span>]<span class="op">-</span><span class="dv">1</span>),</span>
<span id="cb32-38"><a href="#cb32-38" tabindex="-1"></a>    fixed_npa <span class="op">=</span> fixed(sitk.GetArrayViewFromImage(fixed_image)),</span>
<span id="cb32-39"><a href="#cb32-39" tabindex="-1"></a>    moving_npa <span class="op">=</span> fixed(sitk.GetArrayViewFromImage(moving_image)))</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>Origin for fixed image: (0.0, 0.0, 0.0), moving image: (0.0, 0.0, 0.0)
Spacing for fixed image: (0.653595, 0.653595, 4.0), moving image: (1.25, 1.25, 4.0)
Size for fixed image: (512, 512, 29), moving image: (256, 256, 26)
Number Of Components Per Pixel for fixed image: 1, moving image: 1</code></pre>
</div>
<figure><img src="../fig/ct_mri_registration.png" alt="CT and MRI volumes before being aligned." class="figure mx-auto d-block"></figure><p>We can use the <code>CenteredTransformInitializer</code> to align the
centers of the two volumes and set the center of rotation to the center
of the fixed image:</p>
<div class="codewrapper sourceCode" id="cb34">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" tabindex="-1"></a><span class="co"># Callback invoked by the IPython interact method for scrolling and</span></span>
<span id="cb34-2"><a href="#cb34-2" tabindex="-1"></a><span class="co"># modifying the alpha blending of an image stack of two images that</span></span>
<span id="cb34-3"><a href="#cb34-3" tabindex="-1"></a><span class="co"># occupy the same physical space. </span></span>
<span id="cb34-4"><a href="#cb34-4" tabindex="-1"></a><span class="kw">def</span> display_images_with_alpha(image_z, alpha, fixed, moving):</span>
<span id="cb34-5"><a href="#cb34-5" tabindex="-1"></a>    img <span class="op">=</span> (<span class="fl">1.0</span> <span class="op">-</span> alpha)<span class="op">*</span>fixed[:,:,image_z] <span class="op">+</span> alpha<span class="op">*</span>moving[:,:,image_z] </span>
<span id="cb34-6"><a href="#cb34-6" tabindex="-1"></a>    plt.imshow(sitk.GetArrayViewFromImage(img),cmap<span class="op">=</span>plt.cm.Greys_r)</span>
<span id="cb34-7"><a href="#cb34-7" tabindex="-1"></a>    plt.axis(<span class="st">'off'</span>)</span>
<span id="cb34-8"><a href="#cb34-8" tabindex="-1"></a>    plt.show()</span>
<span id="cb34-9"><a href="#cb34-9" tabindex="-1"></a></span>
<span id="cb34-10"><a href="#cb34-10" tabindex="-1"></a>initial_transform <span class="op">=</span> sitk.CenteredTransformInitializer(fixed_image, </span>
<span id="cb34-11"><a href="#cb34-11" tabindex="-1"></a>                                                      moving_image, </span>
<span id="cb34-12"><a href="#cb34-12" tabindex="-1"></a>                                                      sitk.Euler3DTransform(), </span>
<span id="cb34-13"><a href="#cb34-13" tabindex="-1"></a>                                                      sitk.CenteredTransformInitializerFilter.GEOMETRY)</span>
<span id="cb34-14"><a href="#cb34-14" tabindex="-1"></a></span>
<span id="cb34-15"><a href="#cb34-15" tabindex="-1"></a>moving_resampled <span class="op">=</span> sitk.Resample(moving_image, fixed_image, initial_transform, sitk.sitkLinear, <span class="fl">0.0</span>, moving_image.GetPixelID())</span>
<span id="cb34-16"><a href="#cb34-16" tabindex="-1"></a></span>
<span id="cb34-17"><a href="#cb34-17" tabindex="-1"></a>interact(</span>
<span id="cb34-18"><a href="#cb34-18" tabindex="-1"></a>    display_images_with_alpha,</span>
<span id="cb34-19"><a href="#cb34-19" tabindex="-1"></a>    image_z <span class="op">=</span> (<span class="dv">0</span>,fixed_image.GetSize()[<span class="dv">2</span>]<span class="op">-</span><span class="dv">1</span>),</span>
<span id="cb34-20"><a href="#cb34-20" tabindex="-1"></a>    alpha <span class="op">=</span> (<span class="fl">0.0</span>,<span class="fl">1.0</span>,<span class="fl">0.05</span>),</span>
<span id="cb34-21"><a href="#cb34-21" tabindex="-1"></a>    fixed <span class="op">=</span> fixed(fixed_image),</span>
<span id="cb34-22"><a href="#cb34-22" tabindex="-1"></a>    moving <span class="op">=</span> fixed(moving_resampled))</span></code></pre>
</div>
<figure><img src="../fig/ct_mri_registration2.png" alt="CT and MRI volumes overimposed." class="figure mx-auto d-block"></figure><p>The specific registration task at hand estimates a 3D rigid
transformation between images of different modalities. There are
multiple components from each group (optimizers, similarity metrics,
interpolators) that are appropriate for the task. Note that each
component selection requires setting some parameter values. We have made
the following choices:</p>
<ul>
<li>Similarity metric, mutual information (Mattes MI):
<ul>
<li>Number of histogram bins, 50.</li>
<li>Sampling strategy, random.</li>
<li>Sampling percentage, 1%.</li>
</ul>
</li>
<li>Interpolator, <code>sitkLinear</code>.</li>
<li>Optimizer, gradient descent:
<ul>
<li>Learning rate, step size along traversal direction in parameter
space, 1.0.</li>
<li>Number of iterations, maximal number of iterations, 100.</li>
<li>Convergence minimum value, value used for convergence checking in
conjunction with the energy profile of the similarity metric that is
estimated in the given window size, 1e-6.</li>
<li>Convergence window size, number of values of the similarity metric
which are used to estimate the energy profile of the similarity metric,
10.</li>
</ul>
</li>
</ul>
<p>We perform registration using the settings given above, and by taking
advantage of the built in multi-resolution framework, we use a three
tier pyramid.</p>
<p>In this example we plot the similarity metric’s value during
registration. Note that the change of scales in the multi-resolution
framework is readily visible.</p>
<div class="codewrapper sourceCode" id="cb35">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" tabindex="-1"></a><span class="co"># Callback invoked when the StartEvent happens, sets up our new data.</span></span>
<span id="cb35-2"><a href="#cb35-2" tabindex="-1"></a><span class="kw">def</span> start_plot():</span>
<span id="cb35-3"><a href="#cb35-3" tabindex="-1"></a>    <span class="kw">global</span> metric_values, multires_iterations</span>
<span id="cb35-4"><a href="#cb35-4" tabindex="-1"></a>    </span>
<span id="cb35-5"><a href="#cb35-5" tabindex="-1"></a>    metric_values <span class="op">=</span> []</span>
<span id="cb35-6"><a href="#cb35-6" tabindex="-1"></a>    multires_iterations <span class="op">=</span> []</span>
<span id="cb35-7"><a href="#cb35-7" tabindex="-1"></a></span>
<span id="cb35-8"><a href="#cb35-8" tabindex="-1"></a><span class="co"># Callback invoked when the EndEvent happens, do cleanup of data and figure.</span></span>
<span id="cb35-9"><a href="#cb35-9" tabindex="-1"></a><span class="kw">def</span> end_plot():</span>
<span id="cb35-10"><a href="#cb35-10" tabindex="-1"></a>    <span class="kw">global</span> metric_values, multires_iterations</span>
<span id="cb35-11"><a href="#cb35-11" tabindex="-1"></a>    </span>
<span id="cb35-12"><a href="#cb35-12" tabindex="-1"></a>    <span class="kw">del</span> metric_values</span>
<span id="cb35-13"><a href="#cb35-13" tabindex="-1"></a>    <span class="kw">del</span> multires_iterations</span>
<span id="cb35-14"><a href="#cb35-14" tabindex="-1"></a>    <span class="co"># Close figure, we don't want to get a duplicate of the plot latter on.</span></span>
<span id="cb35-15"><a href="#cb35-15" tabindex="-1"></a>    plt.close()</span>
<span id="cb35-16"><a href="#cb35-16" tabindex="-1"></a></span>
<span id="cb35-17"><a href="#cb35-17" tabindex="-1"></a><span class="co"># Callback invoked when the sitkMultiResolutionIterationEvent happens, update the index into the </span></span>
<span id="cb35-18"><a href="#cb35-18" tabindex="-1"></a><span class="co"># metric_values list. </span></span>
<span id="cb35-19"><a href="#cb35-19" tabindex="-1"></a><span class="kw">def</span> update_multires_iterations():</span>
<span id="cb35-20"><a href="#cb35-20" tabindex="-1"></a>    <span class="kw">global</span> metric_values, multires_iterations</span>
<span id="cb35-21"><a href="#cb35-21" tabindex="-1"></a>    multires_iterations.append(<span class="bu">len</span>(metric_values))</span>
<span id="cb35-22"><a href="#cb35-22" tabindex="-1"></a></span>
<span id="cb35-23"><a href="#cb35-23" tabindex="-1"></a><span class="co"># Callback invoked when the IterationEvent happens, update our data and display new figure.</span></span>
<span id="cb35-24"><a href="#cb35-24" tabindex="-1"></a><span class="kw">def</span> plot_values(registration_method):</span>
<span id="cb35-25"><a href="#cb35-25" tabindex="-1"></a>    <span class="kw">global</span> metric_values, multires_iterations</span>
<span id="cb35-26"><a href="#cb35-26" tabindex="-1"></a>    </span>
<span id="cb35-27"><a href="#cb35-27" tabindex="-1"></a>    metric_values.append(registration_method.GetMetricValue())                                       </span>
<span id="cb35-28"><a href="#cb35-28" tabindex="-1"></a>    <span class="co"># Clear the output area (wait=True, to reduce flickering), and plot current data</span></span>
<span id="cb35-29"><a href="#cb35-29" tabindex="-1"></a>    clear_output(wait<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb35-30"><a href="#cb35-30" tabindex="-1"></a>    <span class="co"># Plot the similarity metric values</span></span>
<span id="cb35-31"><a href="#cb35-31" tabindex="-1"></a>    plt.plot(metric_values, <span class="st">'r'</span>)</span>
<span id="cb35-32"><a href="#cb35-32" tabindex="-1"></a>    plt.plot(multires_iterations, [metric_values[index] <span class="cf">for</span> index <span class="kw">in</span> multires_iterations], <span class="st">'b*'</span>)</span>
<span id="cb35-33"><a href="#cb35-33" tabindex="-1"></a>    plt.xlabel(<span class="st">'Iteration Number'</span>,fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb35-34"><a href="#cb35-34" tabindex="-1"></a>    plt.ylabel(<span class="st">'Metric Value'</span>,fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb35-35"><a href="#cb35-35" tabindex="-1"></a>    plt.show()</span>
<span id="cb35-36"><a href="#cb35-36" tabindex="-1"></a></span>
<span id="cb35-37"><a href="#cb35-37" tabindex="-1"></a></span>
<span id="cb35-38"><a href="#cb35-38" tabindex="-1"></a>registration_method <span class="op">=</span> sitk.ImageRegistrationMethod()</span>
<span id="cb35-39"><a href="#cb35-39" tabindex="-1"></a></span>
<span id="cb35-40"><a href="#cb35-40" tabindex="-1"></a><span class="co"># Similarity metric settings.</span></span>
<span id="cb35-41"><a href="#cb35-41" tabindex="-1"></a>registration_method.SetMetricAsMattesMutualInformation(numberOfHistogramBins<span class="op">=</span><span class="dv">50</span>)</span>
<span id="cb35-42"><a href="#cb35-42" tabindex="-1"></a>registration_method.SetMetricSamplingStrategy(registration_method.RANDOM)</span>
<span id="cb35-43"><a href="#cb35-43" tabindex="-1"></a>registration_method.SetMetricSamplingPercentage(<span class="fl">0.01</span>)</span>
<span id="cb35-44"><a href="#cb35-44" tabindex="-1"></a></span>
<span id="cb35-45"><a href="#cb35-45" tabindex="-1"></a>registration_method.SetInterpolator(sitk.sitkLinear)</span>
<span id="cb35-46"><a href="#cb35-46" tabindex="-1"></a></span>
<span id="cb35-47"><a href="#cb35-47" tabindex="-1"></a><span class="co"># Optimizer settings.</span></span>
<span id="cb35-48"><a href="#cb35-48" tabindex="-1"></a>registration_method.SetOptimizerAsGradientDescent(learningRate<span class="op">=</span><span class="fl">1.0</span>, numberOfIterations<span class="op">=</span><span class="dv">100</span>, convergenceMinimumValue<span class="op">=</span><span class="fl">1e-6</span>, convergenceWindowSize<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb35-49"><a href="#cb35-49" tabindex="-1"></a>registration_method.SetOptimizerScalesFromPhysicalShift()</span>
<span id="cb35-50"><a href="#cb35-50" tabindex="-1"></a></span>
<span id="cb35-51"><a href="#cb35-51" tabindex="-1"></a><span class="co"># Setup for the multi-resolution framework.            </span></span>
<span id="cb35-52"><a href="#cb35-52" tabindex="-1"></a>registration_method.SetShrinkFactorsPerLevel(shrinkFactors <span class="op">=</span> [<span class="dv">4</span>,<span class="dv">2</span>,<span class="dv">1</span>])</span>
<span id="cb35-53"><a href="#cb35-53" tabindex="-1"></a>registration_method.SetSmoothingSigmasPerLevel(smoothingSigmas<span class="op">=</span>[<span class="dv">2</span>,<span class="dv">1</span>,<span class="dv">0</span>])</span>
<span id="cb35-54"><a href="#cb35-54" tabindex="-1"></a>registration_method.SmoothingSigmasAreSpecifiedInPhysicalUnitsOn()</span>
<span id="cb35-55"><a href="#cb35-55" tabindex="-1"></a></span>
<span id="cb35-56"><a href="#cb35-56" tabindex="-1"></a><span class="co"># Don't optimize in-place, we would possibly like to run this cell multiple times.</span></span>
<span id="cb35-57"><a href="#cb35-57" tabindex="-1"></a>registration_method.SetInitialTransform(initial_transform, inPlace<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb35-58"><a href="#cb35-58" tabindex="-1"></a></span>
<span id="cb35-59"><a href="#cb35-59" tabindex="-1"></a><span class="co"># Connect all of the observers so that we can perform plotting during registration.</span></span>
<span id="cb35-60"><a href="#cb35-60" tabindex="-1"></a>registration_method.AddCommand(sitk.sitkStartEvent, start_plot)</span>
<span id="cb35-61"><a href="#cb35-61" tabindex="-1"></a>registration_method.AddCommand(sitk.sitkEndEvent, end_plot)</span>
<span id="cb35-62"><a href="#cb35-62" tabindex="-1"></a>registration_method.AddCommand(sitk.sitkMultiResolutionIterationEvent, update_multires_iterations) </span>
<span id="cb35-63"><a href="#cb35-63" tabindex="-1"></a>registration_method.AddCommand(sitk.sitkIterationEvent, <span class="kw">lambda</span>: plot_values(registration_method))</span>
<span id="cb35-64"><a href="#cb35-64" tabindex="-1"></a></span>
<span id="cb35-65"><a href="#cb35-65" tabindex="-1"></a>final_transform <span class="op">=</span> registration_method.Execute(sitk.Cast(fixed_image, sitk.sitkFloat32), </span>
<span id="cb35-66"><a href="#cb35-66" tabindex="-1"></a>                                               sitk.Cast(moving_image, sitk.sitkFloat32))</span></code></pre>
</div>
<figure><img src="../fig/reg_metric_iter.png" alt="Metrics across iterations." class="figure mx-auto d-block"></figure><p>Always remember to query why the optimizer terminated. This will help
you understand whether termination is too early, either due to
thresholds being too tight, early termination due to small number of
iterations - <code>numberOfIterations</code>, or too loose, early
termination due to large value for minimal change in similarity measure
- <code>convergenceMinimumValue</code>.</p>
<div class="codewrapper sourceCode" id="cb36">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Final metric value: </span><span class="sc">{0}</span><span class="st">'</span>.<span class="bu">format</span>(registration_method.GetMetricValue()))</span>
<span id="cb36-2"><a href="#cb36-2" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Optimizer</span><span class="ch">\'</span><span class="st">s stopping condition, </span><span class="sc">{0}</span><span class="st">'</span>.<span class="bu">format</span>(registration_method.GetOptimizerStopConditionDescription()))</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>Final metric value: -0.6561600032169457
Optimizer's stopping condition, GradientDescentOptimizerv4Template: Convergence checker passed at iteration 61.</code></pre>
</div>
<p>Now we can visually inspect the results:</p>
<div class="codewrapper sourceCode" id="cb38">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1" tabindex="-1"></a>moving_resampled <span class="op">=</span> sitk.Resample(moving_image, fixed_image, final_transform, sitk.sitkLinear, <span class="fl">0.0</span>, moving_image.GetPixelID())</span>
<span id="cb38-2"><a href="#cb38-2" tabindex="-1"></a></span>
<span id="cb38-3"><a href="#cb38-3" tabindex="-1"></a>interact(</span>
<span id="cb38-4"><a href="#cb38-4" tabindex="-1"></a>    display_images_with_alpha,</span>
<span id="cb38-5"><a href="#cb38-5" tabindex="-1"></a>    image_z <span class="op">=</span> (<span class="dv">0</span>,fixed_image.GetSize()[<span class="dv">2</span>]<span class="op">-</span><span class="dv">1</span>),</span>
<span id="cb38-6"><a href="#cb38-6" tabindex="-1"></a>    alpha <span class="op">=</span> (<span class="fl">0.0</span>,<span class="fl">1.0</span>,<span class="fl">0.05</span>),</span>
<span id="cb38-7"><a href="#cb38-7" tabindex="-1"></a>    fixed <span class="op">=</span> fixed(fixed_image),</span>
<span id="cb38-8"><a href="#cb38-8" tabindex="-1"></a>    moving <span class="op">=</span> fixed(moving_resampled))</span></code></pre>
</div>
<figure><img src="../fig/ct_mri_registration_aligned.png" alt="CT and MRI volumes aligned." class="figure mx-auto d-block"></figure><div class="codewrapper sourceCode" id="cb39">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Origin for fixed image: </span><span class="sc">{</span>fixed_image<span class="sc">.</span>GetOrigin()<span class="sc">}</span><span class="ss">, shifted moving image: </span><span class="sc">{</span>moving_resampled<span class="sc">.</span>GetOrigin()<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb39-2"><a href="#cb39-2" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Spacing for fixed image: </span><span class="sc">{</span>fixed_image<span class="sc">.</span>GetSpacing()<span class="sc">}</span><span class="ss">, shifted moving image: </span><span class="sc">{</span>moving_resampled<span class="sc">.</span>GetSpacing()<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb39-3"><a href="#cb39-3" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Size for fixed image: </span><span class="sc">{</span>fixed_image<span class="sc">.</span>GetSize()<span class="sc">}</span><span class="ss">, shifted moving image: </span><span class="sc">{</span>moving_resampled<span class="sc">.</span>GetSize()<span class="sc">}</span><span class="ss">"</span>)</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>Origin for fixed image: (0.0, 0.0, 0.0), shifted moving image: (0.0, 0.0, 0.0)
Spacing for fixed image: (0.653595, 0.653595, 4.0), shifted moving image: (0.653595, 0.653595, 4.0)
Size for fixed image: (512, 512, 29), shifted moving image: (512, 512, 29)</code></pre>
</div>
<p>If we are satisfied with the results, save them to file.</p>
<div class="codewrapper sourceCode" id="cb41">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1" tabindex="-1"></a>sitk.WriteImage(moving_resampled, os.path.join(OUTPUT_DIR, <span class="st">'RIRE_training_001_mr_T1_resampled.mha'</span>))</span>
<span id="cb41-2"><a href="#cb41-2" tabindex="-1"></a>sitk.WriteTransform(final_transform, os.path.join(OUTPUT_DIR, <span class="st">'RIRE_training_001_CT_2_mr_T1.tfm'</span>))</span></code></pre>
</div>
</section><section><h2 class="section-heading" id="segmentation">Segmentation<a class="anchor" aria-label="anchor" href="#segmentation"></a>
</h2>
<hr class="half-width">
<p>Image segmentation filters process images by dividing them into
meaningful regions. SITK provides a wide range of filters to support
classical segmentation algorithms, including various thresholding
methods and watershed algorithms. The output is typically an image where
different integers represent distinct objects, with 0 often used for the
background and 1 (or sometimes 255) for foreground objects. After
segmenting the data, SITK allows for efficient post-processing, such as
labeling distinct objects and analyzing their shapes.</p>
<p>Let’s start by reading in a T1 MRI scan, on which we will perform
segmentation operations.</p>
<div class="codewrapper sourceCode" id="cb42">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb42-1"><a href="#cb42-1" tabindex="-1"></a><span class="op">%</span>matplotlib inline</span>
<span id="cb42-2"><a href="#cb42-2" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb42-3"><a href="#cb42-3" tabindex="-1"></a><span class="im">from</span> ipywidgets <span class="im">import</span> interact, fixed</span>
<span id="cb42-4"><a href="#cb42-4" tabindex="-1"></a><span class="im">import</span> SimpleITK <span class="im">as</span> sitk</span>
<span id="cb42-5"><a href="#cb42-5" tabindex="-1"></a></span>
<span id="cb42-6"><a href="#cb42-6" tabindex="-1"></a>img_T1 <span class="op">=</span> sitk.ReadImage(<span class="st">"data/sitk/A1_grayT1.nrrd"</span>)</span>
<span id="cb42-7"><a href="#cb42-7" tabindex="-1"></a><span class="co"># To visualize the labels image in RGB with needs a image with 0-255 range</span></span>
<span id="cb42-8"><a href="#cb42-8" tabindex="-1"></a>img_T1_255 <span class="op">=</span> sitk.Cast(sitk.RescaleIntensity(img_T1), sitk.sitkUInt8)</span>
<span id="cb42-9"><a href="#cb42-9" tabindex="-1"></a></span>
<span id="cb42-10"><a href="#cb42-10" tabindex="-1"></a><span class="co"># Callback invoked by the interact IPython method for scrolling through the image stacks of</span></span>
<span id="cb42-11"><a href="#cb42-11" tabindex="-1"></a><span class="co"># a volume image</span></span>
<span id="cb42-12"><a href="#cb42-12" tabindex="-1"></a><span class="kw">def</span> display_images(image_z, npa, title):</span>
<span id="cb42-13"><a href="#cb42-13" tabindex="-1"></a>    plt.imshow(npa[image_z,:,:], cmap<span class="op">=</span>plt.cm.Greys_r)</span>
<span id="cb42-14"><a href="#cb42-14" tabindex="-1"></a>    plt.title(title)</span>
<span id="cb42-15"><a href="#cb42-15" tabindex="-1"></a>    plt.axis(<span class="st">'off'</span>)</span>
<span id="cb42-16"><a href="#cb42-16" tabindex="-1"></a>    plt.show()</span>
<span id="cb42-17"><a href="#cb42-17" tabindex="-1"></a></span>
<span id="cb42-18"><a href="#cb42-18" tabindex="-1"></a>interact(</span>
<span id="cb42-19"><a href="#cb42-19" tabindex="-1"></a>    display_images,</span>
<span id="cb42-20"><a href="#cb42-20" tabindex="-1"></a>    image_z <span class="op">=</span> (<span class="dv">0</span>,img_T1.GetSize()[<span class="dv">2</span>]<span class="op">-</span><span class="dv">1</span>),</span>
<span id="cb42-21"><a href="#cb42-21" tabindex="-1"></a>    npa <span class="op">=</span> fixed(sitk.GetArrayViewFromImage(img_T1)),</span>
<span id="cb42-22"><a href="#cb42-22" tabindex="-1"></a>    title <span class="op">=</span> fixed(<span class="st">'Z slices'</span>))</span></code></pre>
</div>
<figure><img src="../fig/seg_z.png" alt="T1 MRI scan, Z slices." class="figure mx-auto d-block"></figure><div class="section level3">
<h3 id="thresholding">Thresholding<a class="anchor" aria-label="anchor" href="#thresholding"></a>
</h3>
<p>Thresholding is the most basic form of segmentation. It simply labels
the pixels of an image based on the intensity range without respect to
geometry or connectivity.</p>
<div class="codewrapper sourceCode" id="cb43">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1" tabindex="-1"></a><span class="co"># Basic thresholding</span></span>
<span id="cb43-2"><a href="#cb43-2" tabindex="-1"></a>seg <span class="op">=</span> img_T1<span class="op">&gt;</span><span class="dv">200</span></span>
<span id="cb43-3"><a href="#cb43-3" tabindex="-1"></a>seg_img <span class="op">=</span> sitk.LabelOverlay(img_T1_255, seg)</span>
<span id="cb43-4"><a href="#cb43-4" tabindex="-1"></a></span>
<span id="cb43-5"><a href="#cb43-5" tabindex="-1"></a>interact(</span>
<span id="cb43-6"><a href="#cb43-6" tabindex="-1"></a>    display_images,</span>
<span id="cb43-7"><a href="#cb43-7" tabindex="-1"></a>    image_z <span class="op">=</span> (<span class="dv">0</span>,img_T1.GetSize()[<span class="dv">2</span>]<span class="op">-</span><span class="dv">1</span>),</span>
<span id="cb43-8"><a href="#cb43-8" tabindex="-1"></a>    npa <span class="op">=</span> fixed(sitk.GetArrayViewFromImage(seg_img)),</span>
<span id="cb43-9"><a href="#cb43-9" tabindex="-1"></a>    title <span class="op">=</span> fixed(<span class="st">"Basic thresholding"</span>))</span></code></pre>
</div>
<p>Another example using <code>BinaryThreshold</code>:</p>
<div class="codewrapper sourceCode" id="cb44">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb44-1"><a href="#cb44-1" tabindex="-1"></a><span class="co"># Binary thresholding</span></span>
<span id="cb44-2"><a href="#cb44-2" tabindex="-1"></a>seg <span class="op">=</span> sitk.BinaryThreshold(img_T1, lowerThreshold<span class="op">=</span><span class="dv">100</span>, upperThreshold<span class="op">=</span><span class="dv">400</span>, insideValue<span class="op">=</span><span class="dv">1</span>, outsideValue<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb44-3"><a href="#cb44-3" tabindex="-1"></a>seg_img <span class="op">=</span> sitk.LabelOverlay(img_T1_255, seg)</span>
<span id="cb44-4"><a href="#cb44-4" tabindex="-1"></a></span>
<span id="cb44-5"><a href="#cb44-5" tabindex="-1"></a>interact(</span>
<span id="cb44-6"><a href="#cb44-6" tabindex="-1"></a>    display_images,</span>
<span id="cb44-7"><a href="#cb44-7" tabindex="-1"></a>    image_z <span class="op">=</span> (<span class="dv">0</span>,img_T1.GetSize()[<span class="dv">2</span>]<span class="op">-</span><span class="dv">1</span>),</span>
<span id="cb44-8"><a href="#cb44-8" tabindex="-1"></a>    npa <span class="op">=</span> fixed(sitk.GetArrayViewFromImage(seg_img)),</span>
<span id="cb44-9"><a href="#cb44-9" tabindex="-1"></a>    title <span class="op">=</span> fixed(<span class="st">"Binary thresholding"</span>))<span class="op">****</span></span></code></pre>
</div>
<p>ITK has a number of histogram based automatic thresholding filters
including <code>Huang</code>, <code>MaximumEntropy</code>,
<code>Triangle</code>, and the popular Otsu’s method
(<code>OtsuThresholdImageFilter</code>). These methods create a
histogram then use a heuristic to determine a threshold value.</p>
<div class="codewrapper sourceCode" id="cb45">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb45-1"><a href="#cb45-1" tabindex="-1"></a><span class="co"># Otsu Thresholding</span></span>
<span id="cb45-2"><a href="#cb45-2" tabindex="-1"></a>otsu_filter <span class="op">=</span> sitk.OtsuThresholdImageFilter()</span>
<span id="cb45-3"><a href="#cb45-3" tabindex="-1"></a>otsu_filter.SetInsideValue(<span class="dv">0</span>)</span>
<span id="cb45-4"><a href="#cb45-4" tabindex="-1"></a>otsu_filter.SetOutsideValue(<span class="dv">1</span>)</span>
<span id="cb45-5"><a href="#cb45-5" tabindex="-1"></a>seg <span class="op">=</span> otsu_filter.Execute(img_T1)</span>
<span id="cb45-6"><a href="#cb45-6" tabindex="-1"></a>seg_img <span class="op">=</span> sitk.LabelOverlay(img_T1_255, seg)</span>
<span id="cb45-7"><a href="#cb45-7" tabindex="-1"></a></span>
<span id="cb45-8"><a href="#cb45-8" tabindex="-1"></a>interact(</span>
<span id="cb45-9"><a href="#cb45-9" tabindex="-1"></a>    display_images,</span>
<span id="cb45-10"><a href="#cb45-10" tabindex="-1"></a>    image_z <span class="op">=</span> (<span class="dv">0</span>,img_T1.GetSize()[<span class="dv">2</span>]<span class="op">-</span><span class="dv">1</span>),</span>
<span id="cb45-11"><a href="#cb45-11" tabindex="-1"></a>    npa <span class="op">=</span> fixed(sitk.GetArrayViewFromImage(seg_img)),</span>
<span id="cb45-12"><a href="#cb45-12" tabindex="-1"></a>    title <span class="op">=</span> fixed(<span class="st">"Otsu thresholding"</span>))</span>
<span id="cb45-13"><a href="#cb45-13" tabindex="-1"></a></span>
<span id="cb45-14"><a href="#cb45-14" tabindex="-1"></a><span class="bu">print</span>(otsu_filter.GetThreshold() )</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code><span><span class="fl">236.40869140625</span></span></code></pre>
</div>
<figure><img src="../fig/thresholding.png" alt="Basic thresholding methods." class="figure mx-auto d-block"></figure>
</div>
<div class="section level3">
<h3 id="region-growing-segmentation">Region Growing Segmentation<a class="anchor" aria-label="anchor" href="#region-growing-segmentation"></a>
</h3>
<p>The first step of improvement upon the naive thresholding is a class
of algorithms called region growing. The common theme for all these
algorithms is that a voxel’s neighbor is considered to be in the same
class if its intensities are similar to the current voxel. The
definition of similar is what varies:</p>
<ul>
<li>
<a href="https://itk.org/Doxygen/html/classitk_1_1ConnectedThresholdImageFilter.html" class="external-link">ConnectedThreshold</a>:
The neighboring voxel’s intensity is within explicitly specified
thresholds.</li>
<li>
<a href="https://itk.org/Doxygen/html/classitk_1_1ConfidenceConnectedImageFilter.html" class="external-link">ConfidenceConnected</a>:
The neighboring voxel’s intensity is within the implicitly specified
bounds <span class="math inline">\(\mu \pm c \sigma\)</span>, where
<span class="math inline">\(\mu\)</span> is the mean intensity of the
seed points, <span class="math inline">\(\sigma\)</span> their standard
deviation and <span class="math inline">\(c\)</span> a user specified
constant.</li>
<li>
<a href="https://itk.org/Doxygen/html/classitk_1_1VectorConfidenceConnectedImageFilter.html" class="external-link">VectorConfidenceConnected</a>:
A generalization of the previous approach to vector valued images, for
instance multi-spectral images or multi-parametric MRI. The neighboring
voxel’s intensity vector is within the implicitly specified bounds using
the Mahalanobis distance <span class="math inline">\(\sqrt{(x-\mu)^{T\sum{-1}}(x-\mu)}&lt;c\)</span>,
where <span class="math inline">\(\mu\)</span> is the mean of the
vectors at the seed points, <span class="math inline">\(\sum\)</span> is
the covariance matrix and <span class="math inline">\(c\)</span> is a
user specified constant.</li>
<li><a href="https://itk.org/Doxygen/html/classitk_1_1NeighborhoodConnectedImageFilter.html" class="external-link">NeighborhoodConnected</a></li>
</ul>
<p>Let’s imagine that we have to segment the left <a href="https://en.wikipedia.org/wiki/Lateral_ventricles" class="external-link">lateral
ventricle</a> of the brain image we just visualized.</p>
<figure><img src="../fig/lateral_ventricle.gif" alt="Brain lateral ventricle." class="figure mx-auto d-block"></figure><p><a href="https://www.slicer.org/" class="external-link">3D Slicer</a> was used to determine
that index: (132,142,96) was a good seed for the left lateral ventricle.
Let’s first visualize the seed:</p>
<div class="codewrapper sourceCode" id="cb47">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb47-1"><a href="#cb47-1" tabindex="-1"></a><span class="co"># Initial seed</span></span>
<span id="cb47-2"><a href="#cb47-2" tabindex="-1"></a>seed <span class="op">=</span> (<span class="dv">132</span>,<span class="dv">142</span>,<span class="dv">96</span>)</span>
<span id="cb47-3"><a href="#cb47-3" tabindex="-1"></a>seg <span class="op">=</span> sitk.Image(img_T1.GetSize(), sitk.sitkUInt8)</span>
<span id="cb47-4"><a href="#cb47-4" tabindex="-1"></a>seg.CopyInformation(img_T1)</span>
<span id="cb47-5"><a href="#cb47-5" tabindex="-1"></a>seg[seed] <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb47-6"><a href="#cb47-6" tabindex="-1"></a>seg <span class="op">=</span> sitk.BinaryDilate(seg, [<span class="dv">3</span>]<span class="op">*</span>seg.GetDimension())</span>
<span id="cb47-7"><a href="#cb47-7" tabindex="-1"></a>seg_img <span class="op">=</span> sitk.LabelOverlay(img_T1_255, seg)</span>
<span id="cb47-8"><a href="#cb47-8" tabindex="-1"></a></span>
<span id="cb47-9"><a href="#cb47-9" tabindex="-1"></a>interact(</span>
<span id="cb47-10"><a href="#cb47-10" tabindex="-1"></a>    display_images,</span>
<span id="cb47-11"><a href="#cb47-11" tabindex="-1"></a>    image_z <span class="op">=</span> (<span class="dv">0</span>,img_T1.GetSize()[<span class="dv">2</span>]<span class="op">-</span><span class="dv">1</span>),</span>
<span id="cb47-12"><a href="#cb47-12" tabindex="-1"></a>    npa <span class="op">=</span> fixed(sitk.GetArrayViewFromImage(seg_img)),</span>
<span id="cb47-13"><a href="#cb47-13" tabindex="-1"></a>    title <span class="op">=</span> fixed(<span class="st">"Initial seed"</span>))</span></code></pre>
</div>
<figure><img src="../fig/seed.png" alt="Initial seed." class="figure mx-auto d-block"></figure><p>Let’s use <code>ConnectedThreshold</code> functionality:</p>
<div class="codewrapper sourceCode" id="cb48">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb48-1"><a href="#cb48-1" tabindex="-1"></a><span class="co"># Connected Threshold</span></span>
<span id="cb48-2"><a href="#cb48-2" tabindex="-1"></a>seg <span class="op">=</span> sitk.ConnectedThreshold(img_T1, seedList<span class="op">=</span>[seed], lower<span class="op">=</span><span class="dv">100</span>, upper<span class="op">=</span><span class="dv">190</span>)</span>
<span id="cb48-3"><a href="#cb48-3" tabindex="-1"></a></span>
<span id="cb48-4"><a href="#cb48-4" tabindex="-1"></a>seg_img <span class="op">=</span> sitk.LabelOverlay(img_T1_255, seg)</span>
<span id="cb48-5"><a href="#cb48-5" tabindex="-1"></a></span>
<span id="cb48-6"><a href="#cb48-6" tabindex="-1"></a>interact(</span>
<span id="cb48-7"><a href="#cb48-7" tabindex="-1"></a>    display_images,</span>
<span id="cb48-8"><a href="#cb48-8" tabindex="-1"></a>    image_z <span class="op">=</span> (<span class="dv">0</span>,img_T1.GetSize()[<span class="dv">2</span>]<span class="op">-</span><span class="dv">1</span>),</span>
<span id="cb48-9"><a href="#cb48-9" tabindex="-1"></a>    npa <span class="op">=</span> fixed(sitk.GetArrayViewFromImage(seg_img)),</span>
<span id="cb48-10"><a href="#cb48-10" tabindex="-1"></a>    title <span class="op">=</span> fixed(<span class="st">"Connected threshold"</span>))</span></code></pre>
</div>
<p>Improving upon this is the <code>ConfidenceConnected</code> filter,
which uses the initial seed or current segmentation to estimate the
threshold range.</p>
<p>This region growing algorithm allows the user to implicitly specify
the threshold bounds based on the statistics estimated from the seed
points, <span class="math inline">\(\mu \pm c\sigma\)</span>. This
algorithm has some flexibility which you should familiarize yourself
with:</p>
<ul>
<li>The “multiplier” parameter is the constant <span class="math inline">\(c\)</span> from the formula above.</li>
<li>You can specify a region around each seed point
“initialNeighborhoodRadius” from which the statistics are estimated, see
what happens when you set it to zero.</li>
<li>The “numberOfIterations” allows you to rerun the algorithm. In the
first run the bounds are defined by the seed voxels you specified, in
the following iterations <span class="math inline">\(\mu\)</span> and
<span class="math inline">\(\sigma\)</span> are estimated from the
segmented points and the region growing is updated accordingly.</li>
</ul>
<div class="codewrapper sourceCode" id="cb49">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb49-1"><a href="#cb49-1" tabindex="-1"></a>seg <span class="op">=</span> sitk.ConfidenceConnected(img_T1, seedList<span class="op">=</span>[seed],</span>
<span id="cb49-2"><a href="#cb49-2" tabindex="-1"></a>                                   numberOfIterations<span class="op">=</span><span class="dv">0</span>,</span>
<span id="cb49-3"><a href="#cb49-3" tabindex="-1"></a>                                   multiplier<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb49-4"><a href="#cb49-4" tabindex="-1"></a>                                   initialNeighborhoodRadius<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb49-5"><a href="#cb49-5" tabindex="-1"></a>                                   replaceValue<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb49-6"><a href="#cb49-6" tabindex="-1"></a></span>
<span id="cb49-7"><a href="#cb49-7" tabindex="-1"></a>seg_img <span class="op">=</span> sitk.LabelOverlay(img_T1_255, seg)</span>
<span id="cb49-8"><a href="#cb49-8" tabindex="-1"></a></span>
<span id="cb49-9"><a href="#cb49-9" tabindex="-1"></a>interact(</span>
<span id="cb49-10"><a href="#cb49-10" tabindex="-1"></a>    display_images,</span>
<span id="cb49-11"><a href="#cb49-11" tabindex="-1"></a>    image_z<span class="op">=</span>(<span class="dv">0</span>,img_T1.GetSize()[<span class="dv">2</span>]<span class="op">-</span><span class="dv">1</span>),</span>
<span id="cb49-12"><a href="#cb49-12" tabindex="-1"></a>    npa <span class="op">=</span> fixed(sitk.GetArrayViewFromImage(seg_img)),</span>
<span id="cb49-13"><a href="#cb49-13" tabindex="-1"></a>    title<span class="op">=</span>fixed(<span class="st">"Confidence connected"</span>))</span></code></pre>
</div>
<p>Since we have available also another MRI scan of the same patient,
T2-weighted, we can try to further improve the segmentation. We first
load a T2 image from the same person and combine it with the T1 image to
create a vector image. This region growing algorithm is similar to the
previous one, <code>ConfidenceConnected</code>, and allows the user to
implicitly specify the threshold bounds based on the statistics
estimated from the seed points. The main difference is that in this case
we are using the Mahalanobis and not the intensity difference.</p>
<div class="codewrapper sourceCode" id="cb50">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb50-1"><a href="#cb50-1" tabindex="-1"></a>img_T2 <span class="op">=</span> sitk.ReadImage(<span class="st">"data/sitk/A1_grayT2.nrrd"</span>)</span>
<span id="cb50-2"><a href="#cb50-2" tabindex="-1"></a>img_multi <span class="op">=</span> sitk.Compose(img_T1, img_T2)</span>
<span id="cb50-3"><a href="#cb50-3" tabindex="-1"></a>seg <span class="op">=</span> sitk.VectorConfidenceConnected(img_multi, seedList<span class="op">=</span>[seed],</span>
<span id="cb50-4"><a href="#cb50-4" tabindex="-1"></a>                                             numberOfIterations<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb50-5"><a href="#cb50-5" tabindex="-1"></a>                                             multiplier<span class="op">=</span><span class="fl">2.5</span>,</span>
<span id="cb50-6"><a href="#cb50-6" tabindex="-1"></a>                                             initialNeighborhoodRadius<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb50-7"><a href="#cb50-7" tabindex="-1"></a>seg_img <span class="op">=</span> sitk.LabelOverlay(img_T1_255, seg)</span>
<span id="cb50-8"><a href="#cb50-8" tabindex="-1"></a></span>
<span id="cb50-9"><a href="#cb50-9" tabindex="-1"></a>interact(</span>
<span id="cb50-10"><a href="#cb50-10" tabindex="-1"></a>    display_images,</span>
<span id="cb50-11"><a href="#cb50-11" tabindex="-1"></a>    image_z <span class="op">=</span> (<span class="dv">0</span>,img_T1.GetSize()[<span class="dv">2</span>]<span class="op">-</span><span class="dv">1</span>),</span>
<span id="cb50-12"><a href="#cb50-12" tabindex="-1"></a>    npa <span class="op">=</span> fixed(sitk.GetArrayViewFromImage(seg_img)),</span>
<span id="cb50-13"><a href="#cb50-13" tabindex="-1"></a>    title <span class="op">=</span> fixed(<span class="st">"Vector confidence connected"</span>))</span></code></pre>
</div>
<figure><img src="../fig/reg_grow.png" alt="Region growing segmentations." class="figure mx-auto d-block"></figure><div class="section level4">
<h4 id="clean-up">Clean Up<a class="anchor" aria-label="anchor" href="#clean-up"></a>
</h4>
<p>Use of low level segmentation algorithms such as region growing is
often followed by a clean up step. In this step we fill holes and remove
small connected components. Both of these operations are achieved by
using binary morphological operations, opening
(<code>BinaryMorphologicalOpening</code>) to remove small connected
components and closing (<code>BinaryMorphologicalClosing</code>) to fill
holes.</p>
<p>SITK supports several shapes for the structuring elements (kernels)
including:</p>
<ul>
<li><code>sitkAnnulus</code></li>
<li><code>sitkBall</code></li>
<li><code>sitkBox</code></li>
<li><code>sitkCross</code></li>
</ul>
<p>The size of the kernel can be specified as a scalar (same for all
dimensions) or as a vector of values, size per dimension.</p>
<p>The following code cell illustrates the results of such a clean up,
using closing to remove holes in the original segmentation.</p>
<div class="codewrapper sourceCode" id="cb51">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb51-1"><a href="#cb51-1" tabindex="-1"></a>seg <span class="op">=</span> sitk.ConfidenceConnected(img_T1, seedList<span class="op">=</span>[seed],</span>
<span id="cb51-2"><a href="#cb51-2" tabindex="-1"></a>                                   numberOfIterations<span class="op">=</span><span class="dv">0</span>,</span>
<span id="cb51-3"><a href="#cb51-3" tabindex="-1"></a>                                   multiplier<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb51-4"><a href="#cb51-4" tabindex="-1"></a>                                   initialNeighborhoodRadius<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb51-5"><a href="#cb51-5" tabindex="-1"></a>                                   replaceValue<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb51-6"><a href="#cb51-6" tabindex="-1"></a></span>
<span id="cb51-7"><a href="#cb51-7" tabindex="-1"></a>vectorRadius<span class="op">=</span>(<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>)</span>
<span id="cb51-8"><a href="#cb51-8" tabindex="-1"></a>kernel<span class="op">=</span>sitk.sitkBall</span>
<span id="cb51-9"><a href="#cb51-9" tabindex="-1"></a>seg_clean <span class="op">=</span> sitk.BinaryMorphologicalClosing(seg, vectorRadius, kernel)</span>
<span id="cb51-10"><a href="#cb51-10" tabindex="-1"></a>seg_img_clean <span class="op">=</span> sitk.LabelOverlay(img_T1_255, seg_clean)</span>
<span id="cb51-11"><a href="#cb51-11" tabindex="-1"></a></span>
<span id="cb51-12"><a href="#cb51-12" tabindex="-1"></a>interact(</span>
<span id="cb51-13"><a href="#cb51-13" tabindex="-1"></a>    display_images,</span>
<span id="cb51-14"><a href="#cb51-14" tabindex="-1"></a>    image_z <span class="op">=</span> (<span class="dv">0</span>,img_T1.GetSize()[<span class="dv">2</span>]<span class="op">-</span><span class="dv">1</span>),</span>
<span id="cb51-15"><a href="#cb51-15" tabindex="-1"></a>    npa <span class="op">=</span> fixed(sitk.GetArrayViewFromImage(seg_img_clean)),</span>
<span id="cb51-16"><a href="#cb51-16" tabindex="-1"></a>    title <span class="op">=</span> fixed(<span class="st">"Confidence connected after morphological closing"</span>))</span></code></pre>
</div>
<figure><img src="../fig/reg_grow_cleaned.png" alt="Confidence connected after morphological closing." class="figure mx-auto d-block"></figure>
</div>
</div>
<div class="section level3">
<h3 id="level-set-segmentation">Level-Set Segmentation<a class="anchor" aria-label="anchor" href="#level-set-segmentation"></a>
</h3>
<p>There are a variety of <a href="https://en.wikipedia.org/wiki/Level-set_method" class="external-link">level-set
based</a> segmentation filter available in ITK:</p>
<ul>
<li><a href="https://itk.org/Doxygen/html/classitk_1_1GeodesicActiveContourLevelSetImageFilter.html" class="external-link">GeodesicActiveContour</a></li>
<li><a href="https://itk.org/Doxygen/html/classitk_1_1ShapeDetectionLevelSetImageFilter.html" class="external-link">ShapeDetection</a></li>
<li><a href="https://itk.org/Doxygen/html/classitk_1_1ThresholdSegmentationLevelSetImageFilter.html" class="external-link">ThresholdSegmentation</a></li>
<li><a href="https://itk.org/Doxygen/html/classitk_1_1LaplacianSegmentationLevelSetImageFilter.html" class="external-link">LaplacianSegmentation</a></li>
<li><a href="https://itk.org/Doxygen/html/classitk_1_1ScalarChanAndVeseDenseLevelSetImageFilter.html" class="external-link">ScalarChanAndVese</a></li>
</ul>
<p>There is also a <a href="https://itk.org/Doxygen/html/group__ITKLevelSetsv4.html" class="external-link">modular
Level-set framework</a> which allows composition of terms and easy
extension in C++.</p>
<p>First we create a label image from our seed.</p>
<div class="codewrapper sourceCode" id="cb52">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb52-1"><a href="#cb52-1" tabindex="-1"></a>seed <span class="op">=</span> (<span class="dv">132</span>,<span class="dv">142</span>,<span class="dv">96</span>)</span>
<span id="cb52-2"><a href="#cb52-2" tabindex="-1"></a></span>
<span id="cb52-3"><a href="#cb52-3" tabindex="-1"></a>seg <span class="op">=</span> sitk.Image(img_T1.GetSize(), sitk.sitkUInt8)</span>
<span id="cb52-4"><a href="#cb52-4" tabindex="-1"></a>seg.CopyInformation(img_T1)</span>
<span id="cb52-5"><a href="#cb52-5" tabindex="-1"></a>seg[seed] <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb52-6"><a href="#cb52-6" tabindex="-1"></a>seg <span class="op">=</span> sitk.BinaryDilate(seg, [<span class="dv">3</span>]<span class="op">*</span>seg.GetDimension())</span></code></pre>
</div>
<p>Use the seed to estimate a reasonable threshold range.</p>
<div class="codewrapper sourceCode" id="cb53">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb53-1"><a href="#cb53-1" tabindex="-1"></a>stats <span class="op">=</span> sitk.LabelStatisticsImageFilter()</span>
<span id="cb53-2"><a href="#cb53-2" tabindex="-1"></a>stats.Execute(img_T1, seg)</span>
<span id="cb53-3"><a href="#cb53-3" tabindex="-1"></a></span>
<span id="cb53-4"><a href="#cb53-4" tabindex="-1"></a>factor <span class="op">=</span> <span class="fl">3.5</span></span>
<span id="cb53-5"><a href="#cb53-5" tabindex="-1"></a>lower_threshold <span class="op">=</span> stats.GetMean(<span class="dv">1</span>)<span class="op">-</span>factor<span class="op">*</span>stats.GetSigma(<span class="dv">1</span>)</span>
<span id="cb53-6"><a href="#cb53-6" tabindex="-1"></a>upper_threshold <span class="op">=</span> stats.GetMean(<span class="dv">1</span>)<span class="op">+</span>factor<span class="op">*</span>stats.GetSigma(<span class="dv">1</span>)</span>
<span id="cb53-7"><a href="#cb53-7" tabindex="-1"></a><span class="bu">print</span>(lower_threshold,upper_threshold)</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>81.25184541308809 175.0084466827569</code></pre>
</div>
<div class="codewrapper sourceCode" id="cb55">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb55-1"><a href="#cb55-1" tabindex="-1"></a>init_ls <span class="op">=</span> sitk.SignedMaurerDistanceMap(seg, insideIsPositive<span class="op">=</span><span class="va">True</span>, useImageSpacing<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb55-2"><a href="#cb55-2" tabindex="-1"></a>lsFilter <span class="op">=</span> sitk.ThresholdSegmentationLevelSetImageFilter()</span>
<span id="cb55-3"><a href="#cb55-3" tabindex="-1"></a>lsFilter.SetLowerThreshold(lower_threshold)</span>
<span id="cb55-4"><a href="#cb55-4" tabindex="-1"></a>lsFilter.SetUpperThreshold(upper_threshold)</span>
<span id="cb55-5"><a href="#cb55-5" tabindex="-1"></a>lsFilter.SetMaximumRMSError(<span class="fl">0.02</span>)</span>
<span id="cb55-6"><a href="#cb55-6" tabindex="-1"></a>lsFilter.SetNumberOfIterations(<span class="dv">1000</span>)</span>
<span id="cb55-7"><a href="#cb55-7" tabindex="-1"></a>lsFilter.SetCurvatureScaling(<span class="fl">.5</span>)</span>
<span id="cb55-8"><a href="#cb55-8" tabindex="-1"></a>lsFilter.SetPropagationScaling(<span class="dv">1</span>)</span>
<span id="cb55-9"><a href="#cb55-9" tabindex="-1"></a>lsFilter.ReverseExpansionDirectionOn()</span>
<span id="cb55-10"><a href="#cb55-10" tabindex="-1"></a>ls <span class="op">=</span> lsFilter.Execute(init_ls, sitk.Cast(img_T1, sitk.sitkFloat32))</span>
<span id="cb55-11"><a href="#cb55-11" tabindex="-1"></a><span class="bu">print</span>(lsFilter)</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>itk::simple::ThresholdSegmentationLevelSetImageFilter
  LowerThreshold: 81.2518
  UpperThreshold: 175.008
  MaximumRMSError: 0.02
  PropagationScaling: 1
  CurvatureScaling: 0.5
  NumberOfIterations: 1000
  ReverseExpansionDirection: 1
  ElapsedIterations: 119
  RMSChange: 0.0180966
  Debug: 0
  NumberOfThreads: 10
  NumberOfWorkUnits: 0
  Commands: (none)
  ProgressMeasurement: 0.119
  ActiveProcess: (none)</code></pre>
</div>
<div class="codewrapper sourceCode" id="cb57">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb57-1"><a href="#cb57-1" tabindex="-1"></a>seg_img <span class="op">=</span> sitk.LabelOverlay(img_T1_255, ls<span class="op">&gt;</span><span class="dv">0</span>)</span>
<span id="cb57-2"><a href="#cb57-2" tabindex="-1"></a></span>
<span id="cb57-3"><a href="#cb57-3" tabindex="-1"></a>interact(</span>
<span id="cb57-4"><a href="#cb57-4" tabindex="-1"></a>    display_images,</span>
<span id="cb57-5"><a href="#cb57-5" tabindex="-1"></a>    image_z <span class="op">=</span> (<span class="dv">0</span>,img_T1.GetSize()[<span class="dv">2</span>]<span class="op">-</span><span class="dv">1</span>),</span>
<span id="cb57-6"><a href="#cb57-6" tabindex="-1"></a>    npa <span class="op">=</span> fixed(sitk.GetArrayViewFromImage(seg_img)),</span>
<span id="cb57-7"><a href="#cb57-7" tabindex="-1"></a>    title <span class="op">=</span> fixed(<span class="st">"Level set segmentation"</span>))</span></code></pre>
</div>
<figure><img src="../fig/level_set_seg.png" alt="Level-set segmentation." class="figure mx-auto d-block"></figure><div id="challenge-segment-on-the-y-axis" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="challenge-segment-on-the-y-axis" class="callout-inner">
<h3 class="callout-title">Challenge: Segment on the y-Axis</h3>
<div class="callout-content">
<p>Try to segment the lateral ventricle using volume’s slices on y-axis
instead of z-axis.</p>
<p>Hint: Start by editing the <code>display_images</code> function in
order to select the slices on the y-axis.</p>
</div>
</div>
</div>
<div id="accordionSolution3" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution3" aria-expanded="false" aria-controls="collapseSolution3">
  <h4 class="accordion-header" id="headingSolution3"> Show me the solution </h4>
</button>
<div id="collapseSolution3" class="accordion-collapse collapse" data-bs-parent="#accordionSolution3" aria-labelledby="headingSolution3">
<div class="accordion-body">
<div class="codewrapper sourceCode" id="cb58">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb58-1"><a href="#cb58-1" tabindex="-1"></a><span class="kw">def</span> display_images(image_y, npa, title):</span>
<span id="cb58-2"><a href="#cb58-2" tabindex="-1"></a>    plt.imshow(npa[:,image_y,:], cmap<span class="op">=</span>plt.cm.Greys_r)</span>
<span id="cb58-3"><a href="#cb58-3" tabindex="-1"></a>    plt.title(title)</span>
<span id="cb58-4"><a href="#cb58-4" tabindex="-1"></a>    plt.axis(<span class="st">'off'</span>)</span>
<span id="cb58-5"><a href="#cb58-5" tabindex="-1"></a>    plt.show()</span>
<span id="cb58-6"><a href="#cb58-6" tabindex="-1"></a></span>
<span id="cb58-7"><a href="#cb58-7" tabindex="-1"></a><span class="co"># Initial seed; we can reuse the same used for the z-axis slices</span></span>
<span id="cb58-8"><a href="#cb58-8" tabindex="-1"></a>seed <span class="op">=</span> (<span class="dv">132</span>,<span class="dv">142</span>,<span class="dv">96</span>)</span>
<span id="cb58-9"><a href="#cb58-9" tabindex="-1"></a>seg <span class="op">=</span> sitk.Image(img_T1.GetSize(), sitk.sitkUInt8)</span>
<span id="cb58-10"><a href="#cb58-10" tabindex="-1"></a>seg.CopyInformation(img_T1)</span>
<span id="cb58-11"><a href="#cb58-11" tabindex="-1"></a>seg[seed] <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb58-12"><a href="#cb58-12" tabindex="-1"></a>seg <span class="op">=</span> sitk.BinaryDilate(seg, [<span class="dv">3</span>]<span class="op">*</span>seg.GetDimension())</span>
<span id="cb58-13"><a href="#cb58-13" tabindex="-1"></a>seg_img <span class="op">=</span> sitk.LabelOverlay(img_T1_255, seg)</span>
<span id="cb58-14"><a href="#cb58-14" tabindex="-1"></a></span>
<span id="cb58-15"><a href="#cb58-15" tabindex="-1"></a>interact(</span>
<span id="cb58-16"><a href="#cb58-16" tabindex="-1"></a>    display_images,</span>
<span id="cb58-17"><a href="#cb58-17" tabindex="-1"></a>    image_y <span class="op">=</span> (<span class="dv">0</span>,img_T1.GetSize()[<span class="dv">2</span>]<span class="op">-</span><span class="dv">1</span>),</span>
<span id="cb58-18"><a href="#cb58-18" tabindex="-1"></a>    npa <span class="op">=</span> fixed(sitk.GetArrayViewFromImage(seg_img)),</span>
<span id="cb58-19"><a href="#cb58-19" tabindex="-1"></a>    title <span class="op">=</span> fixed(<span class="st">"Initial seed"</span>))</span></code></pre>
</div>
<p>After some attempts, this was the method that gave the best
segmentation results:</p>
<div class="codewrapper sourceCode" id="cb59">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb59-1"><a href="#cb59-1" tabindex="-1"></a>seed <span class="op">=</span> (<span class="dv">132</span>,<span class="dv">142</span>,<span class="dv">96</span>)</span>
<span id="cb59-2"><a href="#cb59-2" tabindex="-1"></a></span>
<span id="cb59-3"><a href="#cb59-3" tabindex="-1"></a>seg <span class="op">=</span> sitk.Image(img_T1.GetSize(), sitk.sitkUInt8)</span>
<span id="cb59-4"><a href="#cb59-4" tabindex="-1"></a>seg.CopyInformation(img_T1)</span>
<span id="cb59-5"><a href="#cb59-5" tabindex="-1"></a>seg[seed] <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb59-6"><a href="#cb59-6" tabindex="-1"></a>seg <span class="op">=</span> sitk.BinaryDilate(seg, [<span class="dv">3</span>]<span class="op">*</span>seg.GetDimension())</span>
<span id="cb59-7"><a href="#cb59-7" tabindex="-1"></a></span>
<span id="cb59-8"><a href="#cb59-8" tabindex="-1"></a>stats <span class="op">=</span> sitk.LabelStatisticsImageFilter()</span>
<span id="cb59-9"><a href="#cb59-9" tabindex="-1"></a>stats.Execute(img_T1, seg)</span>
<span id="cb59-10"><a href="#cb59-10" tabindex="-1"></a></span>
<span id="cb59-11"><a href="#cb59-11" tabindex="-1"></a>factor <span class="op">=</span> <span class="fl">3.5</span></span>
<span id="cb59-12"><a href="#cb59-12" tabindex="-1"></a>lower_threshold <span class="op">=</span> stats.GetMean(<span class="dv">1</span>)<span class="op">-</span>factor<span class="op">*</span>stats.GetSigma(<span class="dv">1</span>)</span>
<span id="cb59-13"><a href="#cb59-13" tabindex="-1"></a>upper_threshold <span class="op">=</span> stats.GetMean(<span class="dv">1</span>)<span class="op">+</span>factor<span class="op">*</span>stats.GetSigma(<span class="dv">1</span>)</span>
<span id="cb59-14"><a href="#cb59-14" tabindex="-1"></a><span class="bu">print</span>(lower_threshold,upper_threshold)</span>
<span id="cb59-15"><a href="#cb59-15" tabindex="-1"></a></span>
<span id="cb59-16"><a href="#cb59-16" tabindex="-1"></a>init_ls <span class="op">=</span> sitk.SignedMaurerDistanceMap(seg, insideIsPositive<span class="op">=</span><span class="va">True</span>, useImageSpacing<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb59-17"><a href="#cb59-17" tabindex="-1"></a>lsFilter <span class="op">=</span> sitk.ThresholdSegmentationLevelSetImageFilter()</span>
<span id="cb59-18"><a href="#cb59-18" tabindex="-1"></a>lsFilter.SetLowerThreshold(lower_threshold)</span>
<span id="cb59-19"><a href="#cb59-19" tabindex="-1"></a>lsFilter.SetUpperThreshold(upper_threshold)</span>
<span id="cb59-20"><a href="#cb59-20" tabindex="-1"></a>lsFilter.SetMaximumRMSError(<span class="fl">0.02</span>)</span>
<span id="cb59-21"><a href="#cb59-21" tabindex="-1"></a>lsFilter.SetNumberOfIterations(<span class="dv">1000</span>)</span>
<span id="cb59-22"><a href="#cb59-22" tabindex="-1"></a>lsFilter.SetCurvatureScaling(<span class="fl">.5</span>)</span>
<span id="cb59-23"><a href="#cb59-23" tabindex="-1"></a>lsFilter.SetPropagationScaling(<span class="dv">1</span>)</span>
<span id="cb59-24"><a href="#cb59-24" tabindex="-1"></a>lsFilter.ReverseExpansionDirectionOn()</span>
<span id="cb59-25"><a href="#cb59-25" tabindex="-1"></a>ls <span class="op">=</span> lsFilter.Execute(init_ls, sitk.Cast(img_T1, sitk.sitkFloat32))</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>81.25184541308809 175.0084466827569</code></pre>
</div>
<div class="codewrapper sourceCode" id="cb61">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb61-1"><a href="#cb61-1" tabindex="-1"></a>seg_img <span class="op">=</span> sitk.LabelOverlay(img_T1_255, ls<span class="op">&gt;</span><span class="dv">0</span>)</span>
<span id="cb61-2"><a href="#cb61-2" tabindex="-1"></a></span>
<span id="cb61-3"><a href="#cb61-3" tabindex="-1"></a>interact(</span>
<span id="cb61-4"><a href="#cb61-4" tabindex="-1"></a>    display_images,</span>
<span id="cb61-5"><a href="#cb61-5" tabindex="-1"></a>    image_y <span class="op">=</span> (<span class="dv">0</span>,img_T1.GetSize()[<span class="dv">2</span>]<span class="op">-</span><span class="dv">1</span>),</span>
<span id="cb61-6"><a href="#cb61-6" tabindex="-1"></a>    npa <span class="op">=</span> fixed(sitk.GetArrayViewFromImage(seg_img)),</span>
<span id="cb61-7"><a href="#cb61-7" tabindex="-1"></a>    title <span class="op">=</span> fixed(<span class="st">"Level set segmentation"</span>))</span></code></pre>
</div>
<figure><img src="../fig/y-axis_seg.png" alt="Y-axis segmentation." class="figure mx-auto d-block"></figure>
</div>
</div>
</div>
</div>
<div id="segmentation-evaluation" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<div id="segmentation-evaluation" class="callout-inner">
<h3 class="callout-title">Segmentation Evaluation</h3>
<div class="callout-content">
<p>Evaluating segmentation algorithms typically involves comparing your
results to reference data.</p>
<p>In the medical field, reference data is usually created through
manual segmentation by an expert. When resources are limited, a single
expert might define this data, though this is less than ideal. If
multiple experts contribute, their inputs can be combined to produce
reference data that more closely approximates the elusive “ground
truth.”</p>
<p>For detailed coding examples on segmentation evaluation, refer to <a href="https://insightsoftwareconsortium.github.io/SimpleITK-Notebooks/Python_html/34_Segmentation_Evaluation.html" class="external-link">this
notebook</a>.</p>
</div>
</div>
</div>
</div>
</section><section><h2 class="section-heading" id="acknowledgements">Acknowledgements<a class="anchor" aria-label="anchor" href="#acknowledgements"></a>
</h2>
<hr class="half-width">
<p>This episode was largely inspired by <a href="https://SITK.org/TUTORIAL/#tutorial" class="external-link">the official SITK
tutorial</a>, which is copyrighted by NumFOCUS and distributed under the
<a href="https://creativecommons.org/licenses/by/4.0/" class="external-link">Creative Commons
Attribution 4.0 International License</a>, and <a href="https://insightsoftwareconsortium.github.io/SITK-Notebooks/" class="external-link">SITK
Notebooks</a>.</p>
<div class="section level3">
<h3 id="additional-resources">Additional Resources<a class="anchor" aria-label="anchor" href="#additional-resources"></a>
</h3>
<p>To really understand the structure of SITK images and how to work
with them, we recommend some hands-on interaction using the <a href="https://github.com/InsightSoftwareConsortium/SimpleITK-Notebooks" class="external-link">SITK
Jupyter notebooks</a> from the SITK official channels. More detailed
information about SITK fundamental concepts can also be found <a href="https://simpleitk.readthedocs.io/en/master/fundamentalConcepts.html#" class="external-link">here</a>.</p>
<p>Code illustrating various aspects of the registration and
segmentation framework can be found in the set of <a href="https://SITK.readthedocs.io/en/master/link_examples.html#lbl-examples" class="external-link">examples</a>
which are part of the SITK distribution and in the SITK <a href="https://insightsoftwareconsortium.github.io/SimpleITK-Notebooks/" class="external-link">Jupyter
notebook repository</a>.</p>
<div id="keypoints1" class="callout keypoints">
<div class="callout-square">
<i class="callout-icon" data-feather="key"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Key Points</h3>
<div class="callout-content">
<ul>
<li>Registration aligns images for data merging or temporal tracking,
while segmentation identifies objects within images, which is critical
for detailed analysis.</li>
<li>SITK simplifies segmentation, registration, and advanced analysis
tasks using ITK algorithms and supporting several programming
languages.</li>
<li>Images in SITK are defined by physical space, unlike array-based
libraries, ensuring accurate spatial representation and metadata
management.</li>
<li>SITK offers global and bounded domain transformations for spatial
manipulation and efficient resampling techniques with various
interpolation options.</li>
<li>Use SITK’s robust capabilities for registration and classical
segmentation methods such as thresholding and region growth, ensuring
efficient analysis of medical images.</li>
</ul>
</div>
</div>
</div>
<!--
Place links that you need to refer to multiple times across pages here. Delete
any links that you are not going to use.
 -->
</div>
</section></section><section id="aio-images_ml"><p>Content from <a href="images_ml.html">Preparing Images for Machine Learning</a></p>
<hr>
<p>Last updated on 2024-09-14 |

        <a href="https://github.com/esciencecenter-digital-skills/medical-image-processing/edit/main/episodes/images_ml.md" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<p>Estimated time: <i aria-hidden="true" data-feather="clock"></i> 125 minutes</p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul>
<li>What are the fundamental steps in preparing images for machine
learning?</li>
<li>What techniques can be used to augment data?</li>
<li>How should data from various sources or collected under different
conditions be handled?</li>
<li>How can we generate features for machine learning using radiomics or
volumetrics?</li>
</ul>
</div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul>
<li>Outline the fundamental steps involved in preparing images for
machine learning</li>
<li>Demonstrate data augmentation techniques using affine
transformations</li>
<li>Discuss the potential pitfalls of data augmentation</li>
<li>Explain the concept of derived features, including radiomics and
other pipeline-derived features</li>
<li>Review image harmonization techniques at both the image and dataset
levels</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<section><h2 class="section-heading" id="basic-steps">Basic Steps<a class="anchor" aria-label="anchor" href="#basic-steps"></a>
</h2>
<hr class="half-width">
<p>Datasets of images can serve as the raw data for machine learning
(ML). While in rare cases they might be ready for use with minimal
effort, in most projects, a significant portion of time is dedicated to
cleaning and preparing the data. The quality of the data directly
impacts the quality of the resulting models.</p>
<p>One of the initial steps in building a model involves manually
inspecting both the data and metadata.</p>
<div id="challenge-thinking-about-metadata" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="challenge-thinking-about-metadata" class="callout-inner">
<h3 class="callout-title">Challenge: Thinking About Metadata</h3>
<div class="callout-content">
<p>What metadata should you examine in almost any radiological
dataset?</p>
<p>Hint: Consider metadata pertinent to brain and thorax imaging.</p>
</div>
</div>
</div>
<div id="accordionSolution1" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution1" aria-expanded="false" aria-controls="collapseSolution1">
  <h4 class="accordion-header" id="headingSolution1"> Show me the solution </h4>
</button>
<div id="collapseSolution1" class="accordion-collapse collapse" aria-labelledby="headingSolution1" data-bs-parent="#accordionSolution1">
<div class="accordion-body">
<p>In almost any radiological dataset, it is essential to examine the
age and sex distribution. These metadata are crucial for both brain MRIs
and chest X-rays, as well as for many other types of imaging
studies.</p>
</div>
</div>
</div>
</div>
<p>In real-world scenarios, we often face the challenge of investigating
a specific pathology that is relatively rare. Consequently, our dataset
may contain thousands of normal subjects but relatively few with any
pathology, and even fewer with the pathology of interest. This creates
an imbalanced dataset and can introduce biases. For instance, if we aim
to develop a model to detect heart failure on chest X-rays, but all our
heart failure patients are older men, the model may incorrectly “focus”
(by exploiting statistical correlations) on the absence of female
breasts and signs of aging, rather than on true heart failure indicators
like the cardiothoracic ratio.</p>
<p>Here are some initial steps to understanding your dataset if you plan
to do supervised ML:</p>
<ol style="list-style-type: decimal">
<li>Check some images by hand: assess quality, content, and any
unexpected elements.</li>
<li>Check some labeling by hand: ensure accuracy, consistency, and note
any surprises.</li>
<li>Diversity check for obvious protected classes: examine images,
labels, or metadata for representation.</li>
<li>Convert images to a lossless format and store a backup of the data
collection.</li>
<li>Anonymize data: remove identifiable information within images
(consider using OCR for text).</li>
<li>Determine important image features: consider creating cropped
versions.</li>
<li>Normalize images: adjust for size, histogram, and other factors,
ensuring proper centering when necessary.</li>
<li>Re-examine some images by hand: look for artifacts introduced during
preprocessing steps.</li>
<li>Harmonize the dataset.</li>
<li>Create augmented and/or synthetic data.</li>
</ol>
<p>When preparing images for unsupervised learning, many of these steps
still apply. However, you must also consider the specific algorithm’s
requirements, as some segmentation algorithms may not benefit from
additional data.</p>
<p>In our preparation for ML as outlined above, step two involves
verifying the accuracy of labeling. The label represents the target
variable you want your model to predict, so it is crucial to ensure that
the labeling is both accurate and aligns with the outcomes you expect
your model to predict.</p>
<p>In step three, we emphasize the importance of checking for diversity
among protected classes—groups that are often subject to specific
legislation in medical diagnostics due to historical
underrepresentation. It is essential to ensure your dataset includes
women and ethnic minority groups, as population-level variations can
significantly impact model performance. For instance, chest
circumference and height can vary greatly between populations, such as
between Dutch and Indonesian people. While these differences might not
affect an algorithm focused on cerebral blood flow, they are critical
for others.</p>
<p>In step five, the focus is on anonymization, particularly in imaging
data like ultrasounds, where patient names or diagnoses might appear
directly on the image. Simple cropping may sometimes suffice, but more
advanced techniques such as blurring, masking, or optical character
recognition (OCR) should also be considered to ensure thorough
anonymization.</p>
<p>We will cover step nine, dataset harmonization, in a separate
section. For step ten, creating more data, synthetic data generation
will be discussed further in an episode on generative AI. In the next
section, we will explore examples of augmented data creation.</p>
<p>Let’s go throught some examples. First, we import the libraries we
need:</p>
<div class="codewrapper sourceCode" id="cb1">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-2"><a href="#cb1-2" tabindex="-1"></a><span class="im">import</span> matplotlib</span>
<span id="cb1-3"><a href="#cb1-3" tabindex="-1"></a><span class="im">from</span> matplotlib <span class="im">import</span> pyplot <span class="im">as</span> plt</span>
<span id="cb1-4"><a href="#cb1-4" tabindex="-1"></a><span class="im">from</span> skimage <span class="im">import</span> transform</span>
<span id="cb1-5"><a href="#cb1-5" tabindex="-1"></a><span class="im">from</span> skimage <span class="im">import</span> io</span>
<span id="cb1-6"><a href="#cb1-6" tabindex="-1"></a><span class="im">from</span> skimage.transform <span class="im">import</span> rotate</span>
<span id="cb1-7"><a href="#cb1-7" tabindex="-1"></a><span class="im">from</span> skimage <span class="im">import</span> transform <span class="im">as</span> tf</span>
<span id="cb1-8"><a href="#cb1-8" tabindex="-1"></a><span class="im">from</span> skimage.transform <span class="im">import</span> PiecewiseAffineTransform</span>
<span id="cb1-9"><a href="#cb1-9" tabindex="-1"></a><span class="im">from</span> skimage.transform <span class="im">import</span> resize</span></code></pre>
</div>
<p>Then, we import our example images and examine them.</p>
<div class="codewrapper sourceCode" id="cb2">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" tabindex="-1"></a>image_cxr1 <span class="op">=</span> io.imread(<span class="st">'data/ml/rotatechest.png'</span>) <span class="co"># a relatively normal chest X-ray (CXR)</span></span>
<span id="cb2-2"><a href="#cb2-2" tabindex="-1"></a>image_cxr_cmegaly <span class="op">=</span> io.imread(<span class="st">'data/ml/cardiomegaly_cc0.png'</span>) <span class="co"># cardiomegaly CXR</span></span>
<span id="cb2-3"><a href="#cb2-3" tabindex="-1"></a>image_cxr2 <span class="op">=</span> io.imread(<span class="st">'data/ml/other_op.png'</span>) <span class="co"># a relatively normal CXR</span></span>
<span id="cb2-4"><a href="#cb2-4" tabindex="-1"></a><span class="co"># create figure</span></span>
<span id="cb2-5"><a href="#cb2-5" tabindex="-1"></a>fig <span class="op">=</span> plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">7</span>))</span>
<span id="cb2-6"><a href="#cb2-6" tabindex="-1"></a>  </span>
<span id="cb2-7"><a href="#cb2-7" tabindex="-1"></a><span class="co"># setting values to rows and column variables</span></span>
<span id="cb2-8"><a href="#cb2-8" tabindex="-1"></a>rows <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb2-9"><a href="#cb2-9" tabindex="-1"></a>columns <span class="op">=</span> <span class="dv">3</span></span>
<span id="cb2-10"><a href="#cb2-10" tabindex="-1"></a></span>
<span id="cb2-11"><a href="#cb2-11" tabindex="-1"></a><span class="co"># Adds a subplot at the 1st position</span></span>
<span id="cb2-12"><a href="#cb2-12" tabindex="-1"></a>fig.add_subplot(rows, columns, <span class="dv">1</span>)</span>
<span id="cb2-13"><a href="#cb2-13" tabindex="-1"></a><span class="co"># showing image</span></span>
<span id="cb2-14"><a href="#cb2-14" tabindex="-1"></a>plt.imshow(image_cxr1)</span>
<span id="cb2-15"><a href="#cb2-15" tabindex="-1"></a>plt.axis(<span class="st">'off'</span>)</span>
<span id="cb2-16"><a href="#cb2-16" tabindex="-1"></a>plt.title(<span class="st">"Normal 1"</span>)</span>
<span id="cb2-17"><a href="#cb2-17" tabindex="-1"></a>  </span>
<span id="cb2-18"><a href="#cb2-18" tabindex="-1"></a><span class="co"># add a subplot at the 2nd position</span></span>
<span id="cb2-19"><a href="#cb2-19" tabindex="-1"></a>fig.add_subplot(rows, columns, <span class="dv">2</span>)</span>
<span id="cb2-20"><a href="#cb2-20" tabindex="-1"></a><span class="co"># showing image</span></span>
<span id="cb2-21"><a href="#cb2-21" tabindex="-1"></a>plt.imshow(image_cxr_cmegaly)</span>
<span id="cb2-22"><a href="#cb2-22" tabindex="-1"></a>plt.axis(<span class="st">'off'</span>)</span>
<span id="cb2-23"><a href="#cb2-23" tabindex="-1"></a>plt.title(<span class="st">"Cardiomegaly"</span>)</span>
<span id="cb2-24"><a href="#cb2-24" tabindex="-1"></a></span>
<span id="cb2-25"><a href="#cb2-25" tabindex="-1"></a><span class="co"># add a subplot at the 3nd position</span></span>
<span id="cb2-26"><a href="#cb2-26" tabindex="-1"></a>fig.add_subplot(rows, columns, <span class="dv">3</span>)</span>
<span id="cb2-27"><a href="#cb2-27" tabindex="-1"></a><span class="co"># showing image</span></span>
<span id="cb2-28"><a href="#cb2-28" tabindex="-1"></a>plt.imshow(image_cxr2)</span>
<span id="cb2-29"><a href="#cb2-29" tabindex="-1"></a>plt.axis(<span class="st">'off'</span>)</span>
<span id="cb2-30"><a href="#cb2-30" tabindex="-1"></a>plt.title(<span class="st">"Normal 2"</span>)</span></code></pre>
</div>
<figure><img src="../fig/cxr_display_mip.png" alt="CXR examples" class="figure mx-auto d-block"></figure><div id="challenge-can-you-see-some-problems-in-the-following-scenario" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="challenge-can-you-see-some-problems-in-the-following-scenario" class="callout-inner">
<h3 class="callout-title">Challenge: Can You See Some problems in the Following Scenario?</h3>
<div class="callout-content">
<p>Imagine you got the above images and many more because you have been
assigned to make an algorithm for cardiomegaly detection. The goal is to
notify patients if their hospital-acquired X-rays, taken for any reason,
show signs of cardiomegaly. This is an example of using ML for
opportunistic screening.</p>
<p>You are provided with two datasets:</p>
<ul>
<li>A dataset of healthy (no cardiomegaly) patients from an outpatient
clinic in a very poor area, staffed by first-year radiography students.
These patients were being screened for tuberculosis (TB).</li>
<li>A dataset of chest X-rays of cardiomegaly patients from an extremely
prestigious tertiary inpatient hospital.</li>
</ul>
</div>
</div>
</div>
<div id="accordionSolution2" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution2" aria-expanded="false" aria-controls="collapseSolution2">
  <h4 class="accordion-header" id="headingSolution2"> Show me the solution </h4>
</button>
<div id="collapseSolution2" class="accordion-collapse collapse" aria-labelledby="headingSolution2" data-bs-parent="#accordionSolution2">
<div class="accordion-body">
<p>All of the following may pose potential problems:</p>
<ul>
<li>We may have different quality images from different machines for our
healthy versus diseased patients, which could introduce a “batch effect”
and create bias.</li>
<li>At the prestigious hospital, many X-rays might be taken in the
supine position. Will these combine well with standing AP X-rays?</li>
<li>There could be concerns about the accuracy of labeling at the clinic
since it was performed by lower-level staff.</li>
</ul>
</div>
</div>
</div>
</div>
<div id="challenge-prepare-the-images-for-classic-supervised-ml" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="challenge-prepare-the-images-for-classic-supervised-ml" class="callout-inner">
<h3 class="callout-title">Challenge: Prepare the Images for Classic Supervised ML</h3>
<div class="callout-content">
<p>Use <code>skimage.transform.rotate</code> to create two realistic
augmented images from the given ‘normal’ image stored in the
variables.</p>
<p>Then, in a single block of code, apply what you perceive as the most
critical preprocessing operations to prepare these images for classic
supervised ML.</p>
<p>Hint: Carefully examine the shape of the cardiomegaly image. Consider
the impact of harsh lines on ML performance.</p>
</div>
</div>
</div>
<div id="accordionSolution3" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution3" aria-expanded="false" aria-controls="collapseSolution3">
  <h4 class="accordion-header" id="headingSolution3"> Show me the solution </h4>
</button>
<div id="collapseSolution3" class="accordion-collapse collapse" aria-labelledby="headingSolution3" data-bs-parent="#accordionSolution3">
<div class="accordion-body">
<div class="codewrapper sourceCode" id="cb3">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" tabindex="-1"></a><span class="co"># figure out how much to cut on sides</span></span>
<span id="cb3-2"><a href="#cb3-2" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"cut top/bottom:"</span>, (image_cxr_cmegaly.shape[<span class="dv">0</span>] <span class="op">-</span> image_cxr1.shape[<span class="dv">0</span>])<span class="op">/</span><span class="dv">2</span>)</span>
<span id="cb3-3"><a href="#cb3-3" tabindex="-1"></a>cut_top_bottom <span class="op">=</span> <span class="bu">abs</span>(<span class="bu">round</span>((image_cxr_cmegaly.shape[<span class="dv">0</span>] <span class="op">-</span> image_cxr1.shape[<span class="dv">0</span>])<span class="op">/</span><span class="dv">2</span>))</span>
<span id="cb3-4"><a href="#cb3-4" tabindex="-1"></a><span class="co"># figure our how much to cut on top and bottom</span></span>
<span id="cb3-5"><a href="#cb3-5" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"cut sides:"</span>,(image_cxr_cmegaly.shape[<span class="dv">1</span>] <span class="op">-</span> image_cxr1.shape[<span class="dv">1</span>])<span class="op">/</span><span class="dv">2</span>)</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>cut top/bottom: -119.0
cut sides: -208.5</code></pre>
</div>
<div class="codewrapper sourceCode" id="cb5">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" tabindex="-1"></a>cut_sides <span class="op">=</span> <span class="bu">abs</span>(<span class="bu">round</span>((image_cxr_cmegaly.shape[<span class="dv">1</span>] <span class="op">-</span> image_cxr1.shape[<span class="dv">1</span>])<span class="op">/</span><span class="dv">2</span>))</span>
<span id="cb5-2"><a href="#cb5-2" tabindex="-1"></a>list_images <span class="op">=</span> [image_cxr1, image_cxr_cmegaly, image_cxr2]</span>
<span id="cb5-3"><a href="#cb5-3" tabindex="-1"></a>better_for_ml_list <span class="op">=</span> []</span>
<span id="cb5-4"><a href="#cb5-4" tabindex="-1"></a><span class="cf">for</span> image <span class="kw">in</span> list_images:</span>
<span id="cb5-5"><a href="#cb5-5" tabindex="-1"></a>    image <span class="op">=</span> rotate(image,<span class="dv">2</span>)</span>
<span id="cb5-6"><a href="#cb5-6" tabindex="-1"></a>    image <span class="op">=</span> image[cut_top_bottom:<span class="op">-</span>cut_top_bottom, cut_sides: <span class="op">-</span>cut_sides]</span>
<span id="cb5-7"><a href="#cb5-7" tabindex="-1"></a>    better_for_ml_list.append(image)</span>
<span id="cb5-8"><a href="#cb5-8" tabindex="-1"></a></span>
<span id="cb5-9"><a href="#cb5-9" tabindex="-1"></a><span class="co"># create figure for display</span></span>
<span id="cb5-10"><a href="#cb5-10" tabindex="-1"></a>fig <span class="op">=</span> plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">7</span>))</span>
<span id="cb5-11"><a href="#cb5-11" tabindex="-1"></a>  </span>
<span id="cb5-12"><a href="#cb5-12" tabindex="-1"></a><span class="co"># setting values to rows and column variables</span></span>
<span id="cb5-13"><a href="#cb5-13" tabindex="-1"></a>rows <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb5-14"><a href="#cb5-14" tabindex="-1"></a>columns <span class="op">=</span> <span class="dv">3</span></span>
<span id="cb5-15"><a href="#cb5-15" tabindex="-1"></a></span>
<span id="cb5-16"><a href="#cb5-16" tabindex="-1"></a><span class="co"># add a subplot at the 1st position</span></span>
<span id="cb5-17"><a href="#cb5-17" tabindex="-1"></a>fig.add_subplot(rows, columns, <span class="dv">1</span>)</span>
<span id="cb5-18"><a href="#cb5-18" tabindex="-1"></a><span class="co"># showing image</span></span>
<span id="cb5-19"><a href="#cb5-19" tabindex="-1"></a>plt.imshow(better_for_ml_list[<span class="dv">0</span>])</span>
<span id="cb5-20"><a href="#cb5-20" tabindex="-1"></a>plt.axis(<span class="st">'off'</span>)</span>
<span id="cb5-21"><a href="#cb5-21" tabindex="-1"></a>plt.title(<span class="st">"Normal example rotated"</span>)</span>
<span id="cb5-22"><a href="#cb5-22" tabindex="-1"></a>  </span>
<span id="cb5-23"><a href="#cb5-23" tabindex="-1"></a><span class="co"># add a subplot at the 2nd position</span></span>
<span id="cb5-24"><a href="#cb5-24" tabindex="-1"></a>fig.add_subplot(rows, columns, <span class="dv">2</span>)</span>
<span id="cb5-25"><a href="#cb5-25" tabindex="-1"></a><span class="co"># showing image</span></span>
<span id="cb5-26"><a href="#cb5-26" tabindex="-1"></a>plt.imshow(better_for_ml_list[<span class="dv">1</span>])</span>
<span id="cb5-27"><a href="#cb5-27" tabindex="-1"></a>plt.axis(<span class="st">'off'</span>)</span>
<span id="cb5-28"><a href="#cb5-28" tabindex="-1"></a>plt.title(<span class="st">"Cardiomegaly rotated"</span>)</span>
<span id="cb5-29"><a href="#cb5-29" tabindex="-1"></a></span>
<span id="cb5-30"><a href="#cb5-30" tabindex="-1"></a><span class="co"># add a subplot at the 3nd position</span></span>
<span id="cb5-31"><a href="#cb5-31" tabindex="-1"></a>fig.add_subplot(rows, columns, <span class="dv">3</span>)</span>
<span id="cb5-32"><a href="#cb5-32" tabindex="-1"></a><span class="co"># showing image</span></span>
<span id="cb5-33"><a href="#cb5-33" tabindex="-1"></a>plt.imshow(better_for_ml_list[<span class="dv">2</span>])</span>
<span id="cb5-34"><a href="#cb5-34" tabindex="-1"></a>plt.axis(<span class="st">'off'</span>)</span>
<span id="cb5-35"><a href="#cb5-35" tabindex="-1"></a>plt.title(<span class="st">"Another normal rotated"</span>)</span>
<span id="cb5-36"><a href="#cb5-36" tabindex="-1"></a></span></code></pre>
</div>
<figure><img src="../fig/augmented_cxr_rotate_interem.png" alt="augmented chest x-ray different sizes" class="figure mx-auto d-block"></figure><div class="codewrapper sourceCode" id="cb6">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" tabindex="-1"></a><span class="co"># find a size we want to resize to, here the smallest</span></span>
<span id="cb6-2"><a href="#cb6-2" tabindex="-1"></a>zero_side <span class="op">=</span> []</span>
<span id="cb6-3"><a href="#cb6-3" tabindex="-1"></a>one_side <span class="op">=</span> []</span>
<span id="cb6-4"><a href="#cb6-4" tabindex="-1"></a><span class="cf">for</span> image <span class="kw">in</span> better_for_ml_list:</span>
<span id="cb6-5"><a href="#cb6-5" tabindex="-1"></a>    zero_side.append(image.shape[<span class="dv">0</span>])</span>
<span id="cb6-6"><a href="#cb6-6" tabindex="-1"></a>    zero_side.sort()</span>
<span id="cb6-7"><a href="#cb6-7" tabindex="-1"></a>    one_side.append(image.shape[<span class="dv">1</span>])</span>
<span id="cb6-8"><a href="#cb6-8" tabindex="-1"></a>    one_side.sort()</span>
<span id="cb6-9"><a href="#cb6-9" tabindex="-1"></a>smallest_size <span class="op">=</span> zero_side[<span class="dv">0</span>], one_side[<span class="dv">0</span>]</span>
<span id="cb6-10"><a href="#cb6-10" tabindex="-1"></a><span class="co"># make resized images</span></span>
<span id="cb6-11"><a href="#cb6-11" tabindex="-1"></a>resized <span class="op">=</span> []</span>
<span id="cb6-12"><a href="#cb6-12" tabindex="-1"></a><span class="cf">for</span> image <span class="kw">in</span> better_for_ml_list:</span>
<span id="cb6-13"><a href="#cb6-13" tabindex="-1"></a>    image <span class="op">=</span>  resize(image, (smallest_size))</span>
<span id="cb6-14"><a href="#cb6-14" tabindex="-1"></a>    resized.append(image)</span>
<span id="cb6-15"><a href="#cb6-15" tabindex="-1"></a></span>
<span id="cb6-16"><a href="#cb6-16" tabindex="-1"></a><span class="co"># create figure for display</span></span>
<span id="cb6-17"><a href="#cb6-17" tabindex="-1"></a>fig <span class="op">=</span> plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">7</span>))</span>
<span id="cb6-18"><a href="#cb6-18" tabindex="-1"></a>  </span>
<span id="cb6-19"><a href="#cb6-19" tabindex="-1"></a><span class="co"># setting values to rows and column variables</span></span>
<span id="cb6-20"><a href="#cb6-20" tabindex="-1"></a>rows <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb6-21"><a href="#cb6-21" tabindex="-1"></a>columns <span class="op">=</span> <span class="dv">3</span></span>
<span id="cb6-22"><a href="#cb6-22" tabindex="-1"></a></span>
<span id="cb6-23"><a href="#cb6-23" tabindex="-1"></a><span class="co"># add a subplot at the 1st position</span></span>
<span id="cb6-24"><a href="#cb6-24" tabindex="-1"></a>fig.add_subplot(rows, columns, <span class="dv">1</span>)</span>
<span id="cb6-25"><a href="#cb6-25" tabindex="-1"></a><span class="co"># showing image</span></span>
<span id="cb6-26"><a href="#cb6-26" tabindex="-1"></a>plt.imshow(resized[<span class="dv">0</span>])</span>
<span id="cb6-27"><a href="#cb6-27" tabindex="-1"></a>plt.axis(<span class="st">'off'</span>)</span>
<span id="cb6-28"><a href="#cb6-28" tabindex="-1"></a>plt.title(<span class="st">"Normal 1 Augmented Resized"</span>)</span>
<span id="cb6-29"><a href="#cb6-29" tabindex="-1"></a>  </span>
<span id="cb6-30"><a href="#cb6-30" tabindex="-1"></a><span class="co"># add a subplot at the 2nd position</span></span>
<span id="cb6-31"><a href="#cb6-31" tabindex="-1"></a>fig.add_subplot(rows, columns, <span class="dv">2</span>)</span>
<span id="cb6-32"><a href="#cb6-32" tabindex="-1"></a><span class="co"># showing image</span></span>
<span id="cb6-33"><a href="#cb6-33" tabindex="-1"></a>plt.imshow(resized[<span class="dv">1</span>])</span>
<span id="cb6-34"><a href="#cb6-34" tabindex="-1"></a>plt.axis(<span class="st">'off'</span>)</span>
<span id="cb6-35"><a href="#cb6-35" tabindex="-1"></a>plt.title(<span class="st">"Augment Cardiomegaly Resized"</span>)</span>
<span id="cb6-36"><a href="#cb6-36" tabindex="-1"></a></span>
<span id="cb6-37"><a href="#cb6-37" tabindex="-1"></a><span class="co"># add a subplot at the 3nd position</span></span>
<span id="cb6-38"><a href="#cb6-38" tabindex="-1"></a>fig.add_subplot(rows, columns, <span class="dv">3</span>)</span>
<span id="cb6-39"><a href="#cb6-39" tabindex="-1"></a><span class="co"># showing image</span></span>
<span id="cb6-40"><a href="#cb6-40" tabindex="-1"></a>plt.imshow(resized[<span class="dv">2</span>])</span>
<span id="cb6-41"><a href="#cb6-41" tabindex="-1"></a>plt.axis(<span class="st">'off'</span>)</span>
<span id="cb6-42"><a href="#cb6-42" tabindex="-1"></a>plt.title(<span class="st">"Normal 2 Augment  Resized"</span>)</span></code></pre>
</div>
<figure><img src="../fig/augmented_cxr_rotate.png" alt="augmented chest x-ray" class="figure mx-auto d-block"></figure><p>There are a few key considerations to keep in mind regarding the
choices made here. The goal is to create data that realistically
represents what might occur in real-life scenarios. For example, a
2-degree rotation is far more realistic than a 25-degree rotation.
Unless you are using a rotationally invariant algorithm or another
advanced method that accounts for such differences, even small rotations
can make every pixel slightly different for the computer. A 2-degree
rotation closely mirrors what might happen in a clinical setting, making
it a more practical choice.</p>
<p>Additionally, the results can be further improved by cropping the
images. Generally, we aim to minimize harsh, artificial lines in any
images used for ML algorithms, unless those lines are consistent across
all images. It is a common observation among practitioners that many ML
algorithms tend to “focus” on these harsh lines, so if they are not
uniformly present, it is better to remove them. As for how much to crop,
we based our decision on the size of our images. While you don’t need to
follow our exact approach, it is important to ensure that all images are
ultimately the same size and that all critical features are
retained.</p>
</div>
</div>
</div>
</div>
<p>Of course, there are numerous other transformations we can apply to
images beyond rotation. For example, let’s explore applying a shear:</p>
<div class="codewrapper sourceCode" id="cb7">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" tabindex="-1"></a><span class="co"># create affine transform</span></span>
<span id="cb7-2"><a href="#cb7-2" tabindex="-1"></a>affine_tf <span class="op">=</span> tf.AffineTransform(shear<span class="op">=</span><span class="fl">0.2</span>)</span>
<span id="cb7-3"><a href="#cb7-3" tabindex="-1"></a></span>
<span id="cb7-4"><a href="#cb7-4" tabindex="-1"></a><span class="co"># apply transform to image data</span></span>
<span id="cb7-5"><a href="#cb7-5" tabindex="-1"></a>image_affine_tf <span class="op">=</span> tf.warp(image_cxr_cmegaly, inverse_map<span class="op">=</span>affine_tf)</span>
<span id="cb7-6"><a href="#cb7-6" tabindex="-1"></a></span>
<span id="cb7-7"><a href="#cb7-7" tabindex="-1"></a><span class="co"># display the result</span></span>
<span id="cb7-8"><a href="#cb7-8" tabindex="-1"></a>io.imshow(image_affine_tf)</span>
<span id="cb7-9"><a href="#cb7-9" tabindex="-1"></a>io.show()</span></code></pre>
</div>
<figure><img src="../fig/shear_cxr.png" alt="augmented by shear chest x-ray" class="figure mx-auto d-block"></figure><p>And finally, let’s demonstrate a “wave over a mesh.” We’ll start by
creating a grid, or “mesh,” over our image, represented by dots in our
plot. Then, we’ll apply a warp function to transform the image into the
shape of a wave.</p>
<div class="codewrapper sourceCode" id="cb8">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" tabindex="-1"></a>rows, cols <span class="op">=</span> image_affine_tf.shape[<span class="dv">0</span>], image_affine_tf.shape[<span class="dv">1</span>]</span>
<span id="cb8-2"><a href="#cb8-2" tabindex="-1"></a></span>
<span id="cb8-3"><a href="#cb8-3" tabindex="-1"></a><span class="co"># np.linspace will return evenly spaced numbers over an interval</span></span>
<span id="cb8-4"><a href="#cb8-4" tabindex="-1"></a>src_cols <span class="op">=</span> np.linspace(<span class="dv">0</span>, cols, <span class="dv">20</span>)</span>
<span id="cb8-5"><a href="#cb8-5" tabindex="-1"></a><span class="co"># ie above is start=0, stop = cols, num = 50, and num is the number of chops</span></span>
<span id="cb8-6"><a href="#cb8-6" tabindex="-1"></a>src_rows <span class="op">=</span> np.linspace(<span class="dv">0</span>, rows, <span class="dv">10</span>)</span>
<span id="cb8-7"><a href="#cb8-7" tabindex="-1"></a></span>
<span id="cb8-8"><a href="#cb8-8" tabindex="-1"></a><span class="co"># np.meshgrid returns coordinate matrices from coordinate vectors.</span></span>
<span id="cb8-9"><a href="#cb8-9" tabindex="-1"></a>src_rows, src_cols <span class="op">=</span> np.meshgrid(src_rows, src_cols)</span>
<span id="cb8-10"><a href="#cb8-10" tabindex="-1"></a></span>
<span id="cb8-11"><a href="#cb8-11" tabindex="-1"></a><span class="co"># numpy dstack stacks along a third dimension in the concatenation</span></span>
<span id="cb8-12"><a href="#cb8-12" tabindex="-1"></a>src <span class="op">=</span> np.dstack([src_cols.flat, src_rows.flat])[<span class="dv">0</span>]</span>
<span id="cb8-13"><a href="#cb8-13" tabindex="-1"></a>dst_rows <span class="op">=</span> src[:, <span class="dv">1</span>] <span class="op">-</span> np.sin(np.linspace(<span class="dv">0</span>, <span class="dv">3</span> <span class="op">*</span> np.pi, src.shape[<span class="dv">0</span>])) <span class="op">*</span> <span class="dv">50</span></span>
<span id="cb8-14"><a href="#cb8-14" tabindex="-1"></a>dst_cols <span class="op">=</span> src[:, <span class="dv">0</span>]</span>
<span id="cb8-15"><a href="#cb8-15" tabindex="-1"></a>dst_rows <span class="op">*=</span> <span class="fl">1.5</span></span>
<span id="cb8-16"><a href="#cb8-16" tabindex="-1"></a>dst_rows <span class="op">-=</span> <span class="fl">1.5</span> <span class="op">*</span> <span class="dv">50</span></span>
<span id="cb8-17"><a href="#cb8-17" tabindex="-1"></a>dst <span class="op">=</span> np.vstack([dst_cols, dst_rows]).T</span>
<span id="cb8-18"><a href="#cb8-18" tabindex="-1"></a></span>
<span id="cb8-19"><a href="#cb8-19" tabindex="-1"></a>tform <span class="op">=</span> PiecewiseAffineTransform()</span>
<span id="cb8-20"><a href="#cb8-20" tabindex="-1"></a>tform.estimate(src, dst)</span>
<span id="cb8-21"><a href="#cb8-21" tabindex="-1"></a>noform <span class="op">=</span> PiecewiseAffineTransform()</span>
<span id="cb8-22"><a href="#cb8-22" tabindex="-1"></a>noform.estimate(src, src)</span>
<span id="cb8-23"><a href="#cb8-23" tabindex="-1"></a></span>
<span id="cb8-24"><a href="#cb8-24" tabindex="-1"></a>out_rows <span class="op">=</span> image_affine_tf.shape[<span class="dv">0</span>] <span class="op">-</span> <span class="fl">1.5</span> <span class="op">*</span> <span class="dv">50</span></span>
<span id="cb8-25"><a href="#cb8-25" tabindex="-1"></a>out_cols <span class="op">=</span> cols</span>
<span id="cb8-26"><a href="#cb8-26" tabindex="-1"></a>out <span class="op">=</span> tf.warp(image_affine_tf, tform, output_shape<span class="op">=</span>(out_rows, out_cols))</span>
<span id="cb8-27"><a href="#cb8-27" tabindex="-1"></a><span class="co"># create figure</span></span>
<span id="cb8-28"><a href="#cb8-28" tabindex="-1"></a>fig <span class="op">=</span> plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">7</span>))</span>
<span id="cb8-29"><a href="#cb8-29" tabindex="-1"></a>  </span>
<span id="cb8-30"><a href="#cb8-30" tabindex="-1"></a><span class="co"># setting values to rows and column variables</span></span>
<span id="cb8-31"><a href="#cb8-31" tabindex="-1"></a>rows <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb8-32"><a href="#cb8-32" tabindex="-1"></a>columns <span class="op">=</span> <span class="dv">4</span></span>
<span id="cb8-33"><a href="#cb8-33" tabindex="-1"></a></span>
<span id="cb8-34"><a href="#cb8-34" tabindex="-1"></a><span class="co"># add a subplot at the 1st position</span></span>
<span id="cb8-35"><a href="#cb8-35" tabindex="-1"></a>fig.add_subplot(rows, columns, <span class="dv">1</span>)</span>
<span id="cb8-36"><a href="#cb8-36" tabindex="-1"></a><span class="co"># showing image</span></span>
<span id="cb8-37"><a href="#cb8-37" tabindex="-1"></a>plt.imshow(image_affine_tf)</span>
<span id="cb8-38"><a href="#cb8-38" tabindex="-1"></a>plt.axis(<span class="st">'off'</span>)</span>
<span id="cb8-39"><a href="#cb8-39" tabindex="-1"></a>plt.title(<span class="st">"Normal"</span>)</span>
<span id="cb8-40"><a href="#cb8-40" tabindex="-1"></a>  </span>
<span id="cb8-41"><a href="#cb8-41" tabindex="-1"></a><span class="co"># add a subplot at the 2nd position</span></span>
<span id="cb8-42"><a href="#cb8-42" tabindex="-1"></a>fig.add_subplot(rows, columns, <span class="dv">2</span>)</span>
<span id="cb8-43"><a href="#cb8-43" tabindex="-1"></a><span class="co"># showing image</span></span>
<span id="cb8-44"><a href="#cb8-44" tabindex="-1"></a>plt.imshow(image_affine_tf)</span>
<span id="cb8-45"><a href="#cb8-45" tabindex="-1"></a>plt.plot(noform.inverse(src)[:, <span class="dv">0</span>], noform.inverse(src)[:, <span class="dv">1</span>], <span class="st">'.b'</span>)</span>
<span id="cb8-46"><a href="#cb8-46" tabindex="-1"></a>plt.axis(<span class="st">'off'</span>)</span>
<span id="cb8-47"><a href="#cb8-47" tabindex="-1"></a>plt.title(<span class="st">"Normal and Mesh"</span>)</span>
<span id="cb8-48"><a href="#cb8-48" tabindex="-1"></a></span>
<span id="cb8-49"><a href="#cb8-49" tabindex="-1"></a><span class="co"># add a subplot at the 3nd position</span></span>
<span id="cb8-50"><a href="#cb8-50" tabindex="-1"></a>fig.add_subplot(rows, columns, <span class="dv">3</span>)</span>
<span id="cb8-51"><a href="#cb8-51" tabindex="-1"></a><span class="co"># showing image</span></span>
<span id="cb8-52"><a href="#cb8-52" tabindex="-1"></a>plt.imshow(out)</span>
<span id="cb8-53"><a href="#cb8-53" tabindex="-1"></a></span>
<span id="cb8-54"><a href="#cb8-54" tabindex="-1"></a>plt.axis(<span class="st">'off'</span>)</span>
<span id="cb8-55"><a href="#cb8-55" tabindex="-1"></a>plt.title(<span class="st">"Augment"</span>)</span>
<span id="cb8-56"><a href="#cb8-56" tabindex="-1"></a></span>
<span id="cb8-57"><a href="#cb8-57" tabindex="-1"></a><span class="co"># add a subplot at the 3nd position</span></span>
<span id="cb8-58"><a href="#cb8-58" tabindex="-1"></a>fig.add_subplot(rows, columns, <span class="dv">4</span>)</span>
<span id="cb8-59"><a href="#cb8-59" tabindex="-1"></a><span class="co"># showing image</span></span>
<span id="cb8-60"><a href="#cb8-60" tabindex="-1"></a>plt.imshow(out)</span>
<span id="cb8-61"><a href="#cb8-61" tabindex="-1"></a>plt.plot(tform.inverse(src)[:, <span class="dv">0</span>], tform.inverse(src)[:, <span class="dv">1</span>], <span class="st">'.b'</span>)</span>
<span id="cb8-62"><a href="#cb8-62" tabindex="-1"></a>plt.axis(<span class="st">'off'</span>)</span>
<span id="cb8-63"><a href="#cb8-63" tabindex="-1"></a>plt.title(<span class="st">"Augment and Mesh"</span>)</span></code></pre>
</div>
<figure><img src="../fig/augment_and_mesh.png" alt="augmented by waves chest x-ray" class="figure mx-auto d-block"></figure><p>The last transformation doesn’t appear realistic. The chest became
noticeably widened, which could be problematic. When augmenting data,
there are numerous possibilities, but it’s crucial to ensure the
augmented data remains realistic. Only a subject matter expert
(typically a pathologist, nuclear medicine specialist, or radiologist)
can accurately determine what realistic data should look like.</p>
<div id="libraries-for-medical-image-augmentation" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<div id="libraries-for-medical-image-augmentation" class="callout-inner">
<h3 class="callout-title">Libraries for Medical Image Augmentation</h3>
<div class="callout-content">
<p>We demonstrated how to augment medical images using scikit-image.
Another great resource for augmentation is SimpleITK, which offers a
dedicated <a href="https://insightsoftwareconsortium.github.io/SimpleITK-Notebooks/Python_html/70_Data_Augmentation.html" class="external-link">tutorial
on data augmentation</a>. Additionally, OpenCV provides versatile tools
for image processing and augmentations.</p>
<p>To keep your environment manageable, we recommend using just one of
these libraries. scikit-image, SimpleITK, and OpenCV all offer effective
solutions for medical image augmentation.</p>
</div>
</div>
</div>
</section><section><h2 class="section-heading" id="images-features">Images’ Features<a class="anchor" aria-label="anchor" href="#images-features"></a>
</h2>
<hr class="half-width">
<p>So far, we’ve focused on examples where we directly manipulate
images. However, much of ML involves working with derived values from
images, often converted into tabular data. In fact, it’s possible to
combine images with various types of tabular data in multiple ways for
ML. But before exploring these methods, let’s first consider using image
features alone as inputs for ML.</p>
<p>Two notable examples of image features are volumetric data and
radiomic data. Handling such data at scale—when dealing with thousands
of images—typically involves automated processes rather than manual
measurements. In the past, pioneers like Benjamin Felson, a founding
figure in modern chest X-ray analysis, painstakingly analyzed hundreds
of X-rays by hand to gather statistics. Today, processing pipelines
allow us to efficiently analyze thousands of images simultaneously.</p>
<p>We aim to integrate images into a pipeline that automatically
generates various types of data. A notable example of such a feature
pipeline, particularly for brain imaging, is <a href="https://surfer.nmr.mgh.harvard.edu/" class="external-link">Freesurfer</a>. Freesurfer
facilitates the extraction of volume and other relevant data from brain
imaging scans.</p>
<div id="challenge-identifying-issues-with-pipeline-outputs-in-the-absence-of-original-images" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="challenge-identifying-issues-with-pipeline-outputs-in-the-absence-of-original-images" class="callout-inner">
<h3 class="callout-title">Challenge: Identifying Issues with Pipeline Outputs in the Absence of Original Images</h3>
<div class="callout-content">
<p>Consider potential issues that could arise from using the output of a
pipeline without accessing the original images.</p>
</div>
</div>
</div>
<div id="accordionSolution4" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution4" aria-expanded="false" aria-controls="collapseSolution4">
  <h4 class="accordion-header" id="headingSolution4"> Show me the solution </h4>
</button>
<div id="collapseSolution4" class="accordion-collapse collapse" aria-labelledby="headingSolution4" data-bs-parent="#accordionSolution4">
<div class="accordion-body">
<p>One major concern is accuracy, as pipelines may mislabel or miscount
features, potentially leading to unreliable data analysis or model
training. To mitigate these issues, consider:</p>
<ul>
<li>Manual verification: validate pipeline outputs by manually reviewing
a subset of images to ensure consistency with expected results.</li>
<li>Literature review: consult existing research to establish benchmarks
for feature counts and volumes, helping identify discrepancies that may
indicate pipeline errors.</li>
<li>Expert consultation: seek insights from radiologists or domain
experts to assess the credibility of pipeline outputs based on their
clinical experience.</li>
</ul>
<p>Proceed with caution when relying on pipeline outputs, especially
without direct access to the original images.</p>
<p>Another challenge with pipelines is that the results can vary
significantly depending on which one is chosen. For a detailed
exploration of how this can impact fMRI data, see this <a href="https://doi.org/10.1038/s41467-024-48781-5" class="external-link">Nature article</a>.
Without access to the original images, it becomes difficult to determine
which pipeline most accurately captures the desired outcomes from the
data.</p>
</div>
</div>
</div>
</div>
<p>Morphometric or volumetric data represents one type of derived data,
but radiomics introduces another dimension. Radiomics involves analyzing
mathematical characteristics within images, such as entropy or kurtosis.
Similar methodologies applied to pathology are referred to as pathomics.
Some define radiomics and pathomics as advanced texture analysis in
medical imaging, while others argue they encompass a broader range of
quantitative data than traditional texture analysis methods.</p>
<p>Regardless of the data type chosen, the typical approach involves
segmenting and/or masking an image to isolate the area of interest,
followed by applying algorithms to derive radiomic or pathomic features.
While it’s possible to develop custom code for this purpose, using
established packages is preferred for ensuring reproducibility. For
future reproducibility, adherence to standards such as those set by the
International Bio-imaging Standards Initiative (<a href="https://theibsi.github.io/" class="external-link">IBSI</a>) is crucial.</p>
<p>Below is a list of open-source* libraries that facilitate this type
of analysis:</p>
<table class="table">
<colgroup>
<col width="10%">
<col width="14%">
<col width="30%">
<col width="22%">
<col width="22%">
</colgroup>
<thead><tr class="header">
<th></th>
<th><code>mirp</code></th>
<th><code>pyradiomics</code></th>
<th><code>LIFEx</code></th>
<th><code>radiomics</code></th>
</tr></thead>
<tbody>
<tr class="odd">
<td>License</td>
<td>EUPL-1.2</td>
<td>BSD-3</td>
<td>custom</td>
<td>GPL-3.0</td>
</tr>
<tr class="even">
<td>Last updated</td>
<td>5/2024</td>
<td>1/2024</td>
<td>6/2023</td>
<td>11/2019</td>
</tr>
<tr class="odd">
<td>Programming language</td>
<td>Python</td>
<td>Python</td>
<td>Java</td>
<td>MATLAB</td>
</tr>
<tr class="even">
<td>IBSI-1 compliant</td>
<td>yes</td>
<td>partial</td>
<td>yes</td>
<td>no claim</td>
</tr>
<tr class="odd">
<td>IBSI-2 compliant</td>
<td>yes</td>
<td>no claim</td>
<td>yes</td>
<td>no claim</td>
</tr>
<tr class="even">
<td>Interface</td>
<td>high-level API</td>
<td>high-level API, Docker</td>
<td>GUI, low-level API</td>
<td>low-level API</td>
</tr>
<tr class="odd">
<td>Website</td>
<td><a href="https://github.com/oncoray/mirp" class="external-link">GitHub</a></td>
<td><a href="https://github.com/AIM-Harvard/pyradiomics" class="external-link">GitHub</a></td>
<td><a href="https://www.lifexsoft.org/" class="external-link">website</a></td>
<td><a href="https://github.com/mvallieres/radiomics" class="external-link">GitHub</a></td>
</tr>
<tr class="even">
<td>Early publication</td>
<td><a href="https://joss.theoj.org/papers/10.21105/joss.06413" class="external-link">JOSS
publication</a></td>
<td><a href="https://doi.org/10.1158/0008-5472.CAN-17-0339" class="external-link">doi:10.1158/0008-5472.CAN-17-0339</a></td>
<td><a href="https://doi.org/10.1038/s41598-022-16609-1" class="external-link">doi:10.1158/0008-5472.CAN-18-0125</a></td>
<td><a href="https://doi.org/10.1088/0031-9155/60/14/5471" class="external-link">doi:10.1088/0031-9155/60/14/5471</a></td>
</tr>
<tr class="odd">
<td>Notes</td>
<td>relative newcomer</td>
<td>very standard and supported</td>
<td>user-friendly</td>
<td>* MATLAB requires a license</td>
</tr>
</tbody>
</table>
<p>Once we have tabular data, we can apply various algorithms to analyze
it. However, applying ML isn’t just a matter of running algorithms and
getting quick results. In the next section, we will explore one reason
why this process is more complex than it may seem.</p>
</section><section><h2 class="section-heading" id="harmonization">Harmonization<a class="anchor" aria-label="anchor" href="#harmonization"></a>
</h2>
<hr class="half-width">
<p>We often need to harmonize either images or datasets of derived
features. When dealing with images, differences between datasets can be
visually apparent. For example, if one set of X-rays consistently
appears darker, it may be wise to compare average pixel values between
datasets and adjust normalization to achieve more uniformity. This is a
straightforward scenario.</p>
<p>Consider another example involving derived datasets from brain MRIs
with Virchow Robin’s spaces. One dataset was captured using a 1.5 Tesla
machine in a distant location, while the other used an experimental 5
Tesla machine (high resolution) in an advanced hospital. These machines
vary in resolution, meaning what appears as a single Virchow Robin’s
space at low resolution might actually show as two or three smaller
spaces fused together at high resolution. This is just one potential
difference.</p>
<p>Below are images of the same patient scanned with 1.5 Tesla and 3
Tesla machines:</p>
<figure><img src="../fig/t1_vT3_large.jpg" alt="T1 v T3" class="figure mx-auto d-block"></figure><p><em>Sourced from <a href="https://www.ajnr.org/content/26/9/2229" class="external-link">Bernd L. Schmitz, Andrik
J. Aschoff, Martin H.K. Hoffmann and Georg Grön, Advantages and Pitfalls
in 3T MR Brain Imaging: A Pictorial Review, American Journal of
Neuroradiology October 2005, 26 (9) 2229-2237</a></em></p>
<p>Different contrast levels significantly affect the visibility of
structures like the caudate and thalami in brain images. As a result,
the radiomic characteristics of these images, including contrast and
possibly other parameters, can vary even when they originate from the
same patient.</p>
<p>Constructing a dataset based solely on extensive, unharmonized
derived data is not always practical. However, without access to the
original images, it becomes difficult to fully understand and account
for these variations.</p>
<p>We recommend the following approach:</p>
<ol style="list-style-type: decimal">
<li>Compare the data using descriptive statistics and your knowledge of
the patient groups. Consider whether you expect similar counts across
both groups, and justify your reasoning.</li>
<li>Explore the use of a harmonization package to standardize data
across different datasets.</li>
</ol>
<p>Below are three examples from the field of neuroimaging:</p>
<table class="table">
<colgroup>
<col width="18%">
<col width="25%">
<col width="22%">
<col width="33%">
</colgroup>
<thead><tr class="header">
<th></th>
<th><code>neurocombat</code></th>
<th><code>haca3</code></th>
<th><code>autocombat</code></th>
</tr></thead>
<tbody>
<tr class="odd">
<td>License</td>
<td>MIT for Python/ Artistic for R</td>
<td>Missing?</td>
<td>Apache 2.0</td>
</tr>
<tr class="even">
<td>Last updated</td>
<td>2021</td>
<td>2024</td>
<td>2022</td>
</tr>
<tr class="odd">
<td>Programming language</td>
<td>Python or R</td>
<td>Python</td>
<td>Python</td>
</tr>
<tr class="even">
<td>Organ/area and modality</td>
<td>brain MRI</td>
<td>brain MRI</td>
<td>brain MRI</td>
</tr>
<tr class="odd">
<td>Data type</td>
<td>derived values</td>
<td>images</td>
<td>derived values</td>
</tr>
<tr class="even">
<td>Notes</td>
<td>no standard release yet</td>
<td>versioned but not released</td>
<td>not release, not versioned</td>
</tr>
<tr class="odd">
<td>Original website</td>
<td><a href="https://github.com/Jfortin1/ComBatHarmonization" class="external-link">GitHub</a></td>
<td><a href="https://github.com/lianruizuo/haca3" class="external-link">GitHub</a></td>
<td><a href="https://github.com/Alxaline/ComScan" class="external-link">GitHub</a></td>
</tr>
<tr class="even">
<td>Early publication</td>
<td><a href="https://doi.org/10.1016/j.neuroimage.2017.08.047" class="external-link">doi:
10.1016/j.neuroimage.2017.08.047</a></td>
<td><a href="https://doi.org/10.1016/j.compmedimag.2023.102285" class="external-link">doi:10.1016/j.compmedimag.2023.102285</a></td>
<td><a href="https://doi.org/10.1038/s41598-022-16609-1" class="external-link">doi:
10.1038/s41598-022-16609-1</a></td>
</tr>
<tr class="odd">
<td>Versioned website</td>
<td><a href="https://github.com/brainspinner/cvasl/tree/main/cvasl/vendor/neurocombat" class="external-link">versioned
on cvasl</a></td>
<td><a href="https://github.com/lianruizuo/haca3" class="external-link">versioned on
GitHub</a></td>
<td><a href="https://github.com/brainspinner/cvasl/tree/main/cvasl/vendor/comscan" class="external-link">versioned
on cvasl</a></td>
</tr>
</tbody>
</table>
<p>There are numerous packages available for brain MRI alone, not to
mention those for imaging the rest of the body. We have selected these
three examples to highlight some of the common issues and pitfalls
associated with such packages.</p>
<div id="challenge-identifying-potential-problems-in-each-package" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="challenge-identifying-potential-problems-in-each-package" class="callout-inner">
<h3 class="callout-title">Challenge: Identifying Potential Problems in Each Package</h3>
<div class="callout-content">
<p>Consider issues that might hinder your implementation with each
package.</p>
</div>
</div>
</div>
<div id="accordionSolution5" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution5" aria-expanded="false" aria-controls="collapseSolution5">
  <h4 class="accordion-header" id="headingSolution5"> Show me the solution </h4>
</button>
<div id="collapseSolution5" class="accordion-collapse collapse" aria-labelledby="headingSolution5" data-bs-parent="#accordionSolution5">
<div class="accordion-body">
<p>Aging code: <code>neurocombat</code> was last modified three years
ago. It may rely on outdated dependencies that could pose compatibility
issues with newer hardware and software environments. Additionally, the
lack of versioned releases makes it challenging to track changes and
updates.</p>
<p>No license: <code>haca3</code> does not have a clear license
available. Using code without a proper license could lead to legal
complications and uncertainties about its usage rights.</p>
<p>No releases and versioning: <code>autocombat</code> lacks both
released versions and versioning practices. Without clear versioning, it
becomes difficult to ensure consistency and compare results across
different implementations of the package.</p>
</div>
</div>
</div>
</div>
<p>We’ve pointed out a few potential issues to highlight common
challenges with such programs. Looking ahead, consider developing a
harmonization package that’s sustainable and useful for your research
community.</p>
<p>While each of these packages has its strengths for researchers, none
are perfect. Choose wisely when selecting a package! Also, remember you
can create your own harmonization method through coding or develop it
into a package for others to use.</p>
<div id="keypoints1" class="callout keypoints">
<div class="callout-square">
<i class="callout-icon" data-feather="key"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Key Points</h3>
<div class="callout-content">
<ul>
<li>Direct knowledge of specific data cannot be substituted</li>
<li>Statistical analysis is essential to detect and mitigate biases in
patient distribution</li>
<li>Verify if derived data aligns with known clinical realities through
statistical examination</li>
<li>Evaluate the validity and utility of data augmentation methods
before applying them</li>
<li>Radiomics enables the use of mathematical image qualities as
features</li>
<li>There are several accessible pipelines for volumetrics and
radiomics</li>
<li>Data from different machines (or even the same machines with
different settings) often requires harmonization, achievable through
coding and the use of existing libraries</li>
</ul>
</div>
</div>
</div>
<!--
Place links that you need to refer to multiple times across pages here. Delete
any links that you are not going to use.
 -->
</section></section><section id="aio-anonymization"><p>Content from <a href="anonymization.html">Anonymizing Medical Images</a></p>
<hr>
<p>Last updated on 2024-08-14 |

        <a href="https://github.com/esciencecenter-digital-skills/medical-image-processing/edit/main/episodes/anonymization.md" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<p>Estimated time: <i aria-hidden="true" data-feather="clock"></i> 55 minutes</p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul>
<li>What types of data make patient’s imaging data identifiable?</li>
<li>How can I ensure the safe sharing of medical image data?</li>
<li>How can I remove specific metadata from DICOM files?</li>
</ul>
</div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul>
<li>Provide examples of data that makes patient images identifiable</li>
<li>Discuss the concept of anonymization</li>
<li>Demonstrate the use of the Pydicom library to manage DICOM
metadata</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<section><h2 class="section-heading" id="introduction">Introduction<a class="anchor" aria-label="anchor" href="#introduction"></a>
</h2>
<hr class="half-width">
<p>Each of us is similar yet unique, and this individuality can make us
identifiable, posing challenges for medical research. While open data
sharing advances research, most patients would not want their medical
details shared if they could be identified. In most countries, patient
information is protected by law.</p>
<p>Metadata elements in imaging, such as patient names and addresses,
are often clearly designed to identify patients. However, the uniqueness
of patients means that even images without obvious metadata, such as
names, can potentially be identified as belonging to a specific
individual. With advancements in facial recognition software and search
engines, images we previously thought were non-identifiable, like head
CTs, MRIs, <a href="https://doi.org/10.1016/j.neuroimage.2022.119357" class="external-link">or
even PET scans</a>, can theoretically be traced back to a specific
patient. To address this, we can implement de-identification strategies
to create shareable data.</p>
</section><section><h2 class="section-heading" id="types-of-patient-identifying-data">Types of Patient Identifying Data<a class="anchor" aria-label="anchor" href="#types-of-patient-identifying-data"></a>
</h2>
<hr class="half-width">
<div class="section level3">
<h3 id="metadata">Metadata<a class="anchor" aria-label="anchor" href="#metadata"></a>
</h3>
<p>DICOM files contain metadata, which includes various types of
identifying information that should remain confidential. The easiest way
to mitigate issues with DICOM metadata is to avoid having it in the
first place. If possible, opt to receive just the images and select
metadata rather than the entire DICOM file. When sharing data with
collaborators, there is often no need to share the full DICOM files.</p>
</div>
<div class="section level3">
<h3 id="faces-in-images">Faces in Images<a class="anchor" aria-label="anchor" href="#faces-in-images"></a>
</h3>
<p>A full CT, MRI, or PET scan of the head can be reconstructed into a
detailed facial image, potentially revealing the patient’s identity and
demographic information, such as ethnicity and gender. To mitigate this
risk, many image analysis programs employ ‘defacing’ techniques to
obscure these identifiable features.</p>
<p>There are various tools available for defacing head imaging, ranging
from fully developed software products like <a href="https://surfer.nmr.mgh.harvard.edu/" class="external-link">FreeSurfer</a>, which
includes built-in defacing capabilities, to specialized functions within
coding libraries.</p>
<p>However, a key issue under current investigation is that some
defacing algorithms may inadvertently alter more than just the facial
features. Emerging research, including studies still in pre-print,
suggests that these algorithms might also affect the morphometry of the
brain image. This could lead to the unintended loss or distortion of
critical data. Therefore, it is advisable to proceed with caution and,
whenever possible, compare the original and defaced images to ensure
that important information remains intact and unaltered.</p>
<figure><img src="../fig/deface-example.jpg" alt="Defacing examples" class="figure mx-auto d-block"><div class="figcaption">Image from “A reproducibility evaluation of the
effects of MRI defacing on brain segmentation” by Chenyu Gao, Bennett A.
Landman, Jerry L. Prince, and Aaron Carass. The preprint is available <a href="https://pubmed.ncbi.nlm.nih.gov/37293070/" class="external-link">here</a>.</div>
</figure>
</div>
<div class="section level3">
<h3 id="text-on-images">Text on Images<a class="anchor" aria-label="anchor" href="#text-on-images"></a>
</h3>
<p>Occasionally, technicians will burn information directly onto images
as part of a burned-in annotation. This may include details such as
diagnoses, demographics, or the patient’s name. Fortunately, this text
is usually typed rather than handwritten, making it recognizable by
optical character recognition (OCR) functions. Often, this text is
placed away from the center of the image, allowing for clever cropping
to eliminate it entirely in some datasets.</p>
</div>
<div class="section level3">
<h3 id="other-parts-of-images">Other Parts of Images<a class="anchor" aria-label="anchor" href="#other-parts-of-images"></a>
</h3>
<p>Patient identity can often be inferred with just a few pieces of
data. In some cases, a single piece of information can be enough to
track down a patient’s identity, especially if medical files are
accessible. For instance, a serial number or other identifying number on
a medical device may be traceable back to a specific patient.</p>
<p>In other situations, slightly more data might be required to identify
a patient. Some patients may wear unique jewelry, such as a MedicAlert
bracelet or necklace with initials or a name. While most routine
ambulatory images are taken without jewelry, in emergency situations,
medical personnel may not have had the time to remove these items. The
more data points we have on a patient, the easier it becomes to identify
them.</p>
<figure><img src="../fig/jewellery_artifact.jpg" alt="jewlery artifact" class="figure mx-auto d-block"><div class="figcaption">Case courtesy of Ian Bickle,
<a href="https://radiopaedia.org/" class="external-link">Radiopaedia.org</a>. From the case
<a href="https://radiopaedia.org/cases/61830" class="external-link">rID: 61830</a>
</div>
</figure><p>Various tools are available to help de-identify DICOM files in terms
of metadata. A notable one is <a href="https://github.com/KitwareMedical/dicom-anonymizer" class="external-link">DicomAnonymizer</a>,
an open-source tool written in Python.</p>
<p>In some cases, you may need to examine and remove metadata manually
or programmatically. For example, in some countries, DICOM fields are
used inconsistently, and patient-identifying data can appear in
unexpected fields. Therefore, careful examination and customized removal
of metadata may be necessary.</p>
<div id="many-ways-to-handle-a-dicom" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<div id="many-ways-to-handle-a-dicom" class="callout-inner">
<h3 class="callout-title">Many Ways to Handle a DICOM:</h3>
<div class="callout-content">
<ul>
<li>Multiple libraries, such as Pydicom and SimpleITK (SITK), allow you
to read, access, and manipulate DICOM metadata.</li>
<li>DICOMs follow an extremely complex <a href="https://www.dicomstandard.org/" class="external-link">standard</a>, so it is usually
better to use existing libraries rather than raw Python to handle
them.</li>
</ul>
</div>
</div>
</div>
<p>For various reasons, we may prefer Pydicom, SITK, or another method
to handle DICOM metadata, typically based on the principle of minimizing
dependencies and maintaining simplicity. SITK was introduced earlier in
this course. Pydicom is an excellent alternative, particularly because
of its comprehensive <a href="https://pydicom.github.io/pydicom/stable/" class="external-link">documentation</a>.</p>
<p>Now, let’s see how to open a DICOM file and work with it using
Pydicom.</p>
<p>First, let’s import Pydicom and read in a CT scan:</p>
<div class="codewrapper sourceCode" id="cb1">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a><span class="im">import</span> pydicom</span>
<span id="cb1-2"><a href="#cb1-2" tabindex="-1"></a><span class="im">from</span> pydicom <span class="im">import</span> dcmread</span>
<span id="cb1-3"><a href="#cb1-3" tabindex="-1"></a>fpath <span class="op">=</span> <span class="st">"data/anonym/our_sample_dicom.dcm"</span></span>
<span id="cb1-4"><a href="#cb1-4" tabindex="-1"></a>ds <span class="op">=</span> dcmread(fpath)</span>
<span id="cb1-5"><a href="#cb1-5" tabindex="-1"></a><span class="bu">print</span>(ds)</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>Dataset.file_meta -------------------------------
(0002, 0000) File Meta Information Group Length  UL: 218
(0002, 0001) File Meta Information Version       OB: b'\x00\x01'
(0002, 0002) Media Storage SOP Class UID         UI: CT Image Storage
(0002, 0003) Media Storage SOP Instance UID      UI: 1.3.46.670589.33.1.63849049636503447100001.4758671761353145811
(0002, 0010) Transfer Syntax UID                 UI: JPEG Lossless, Non-Hierarchical, First-Order Prediction (Process 14 [Selection Value 1])
(0002, 0012) Implementation Class UID            UI: 1.2.840.113845.1.1
(0002, 0013) Implementation Version Name         SH: 'Syn7,3,0,258'
(0002, 0016) Source Application Entity Title     AE: 'SynapseDicomSCP'
-------------------------------------------------
(0008, 0005) Specific Character Set              CS: 'ISO_IR 100'
(0008, 0008) Image Type                          CS: ['DERIVED', 'SECONDARY', 'MPR']
(0008, 0012) Instance Creation Date              DA: '20240418'
(0008, 0013) Instance Creation Time              TM: '150716.503'
(0008, 0016) SOP Class UID                       UI: CT Image Storage
(0008, 0018) SOP Instance UID                    UI: 1.3.46.670589.33.1.63849049636503447100001.4758671761353145811
(0008, 0020) Study Date                          DA: '20240418'
(0008, 0022) Acquisition Date                    DA: '20240418'
(0008, 0023) Content Date                        DA: '20240418'
(0008, 002a) Acquisition DateTime                DT: '20240418150313.020'
(0008, 0030) Study Time                          TM: '150045'
(0008, 0032) Acquisition Time                    TM: '150313'
(0008, 0033) Content Time                        TM: '150314.375'
(0008, 0050) Accession Number                    SH: '2001433888'
(0008, 0060) Modality                            CS: 'CT'
(0008, 0070) Manufacturer                        LO: 'Philips'
(0008, 0080) Institution Name                    LO: 'BovenIJ Ziekenhuis iCT'
(0008, 0081) Institution Address                 ST: ''
(0008, 0090) Referring Physician's Name          PN: 'WILTING^I^I^""'
(0008, 1010) Station Name                        SH: 'HOST-999999'
(0008, 1030) Study Description                   LO: 'CT thorax met iv contrast'
(0008, 103e) Series Description                  LO: 'Cor IMR med'
(0008, 1040) Institutional Department Name       LO: 'Radiology'
(0008, 1080) Admitting Diagnoses Description     LO: ''
(0008, 1084)  Admitting Diagnoses Code Sequence  0 item(s) ----
(0008, 1090) Manufacturer's Model Name           LO: 'iCT 256'
(0008, 1111)  Referenced Performed Procedure Step Sequence  1 item(s) ----
   (0008, 1150) Referenced SOP Class UID            UI: Modality Performed Procedure Step SOP Class
   (0008, 1155) Referenced SOP Instance UID         UI: 1.3.46.670589.33.1.63849049241567858000001.4675122277016890611
   ---------
(0008, 1140)  Referenced Image Sequence  1 item(s) ----
   (0008, 1150) Referenced SOP Class UID            UI: CT Image Storage
   (0008, 1155) Referenced SOP Instance UID         UI: 1.3.46.670589.33.1.63849049294969912500001.5475332148846191441
   ---------
(0008, 3010) Irradiation Event UID               UI: 1.3.46.670589.33.1.63849049343237673200010.5507538603167078985
(0010, 0010) Patient's Name                      PN: 'OurBeloved^Colleague'
(0010, 0020) Patient ID                          LO: 'party like 1999'
(0010, 0030) Patient's Birth Date                DA: '19421104'
(0010, 0040) Patient's Sex                       CS: 'M'
(0010, 1000) Other Patient IDs                   LO: '1989442112'
(0010, 1010) Patient's Age                       AS: '041Y'
(0018, 0010) Contrast/Bolus Agent                LO: 'Iodine'
(0018, 0015) Body Part Examined                  CS: 'CHEST'
(0018, 0022) Scan Options                        CS: 'HELIX'
(0018, 0050) Slice Thickness                     DS: '2.0'
(0018, 0060) KVP                                 DS: '100.0'
(0018, 0088) Spacing Between Slices              DS: '2.0'
(0018, 0090) Data Collection Diameter            DS: '500.0'
(0018, 1000) Device Serial Number                LO: ''
(0018, 1020) Software Versions                   LO: '4.1'
(0018, 1030) Protocol Name                       LO: 'Thorax std /Thorax'
(0018, 1040) Contrast/Bolus Route                LO: 'IV'
(0018, 1041) Contrast/Bolus Volume               DS: '80.0'
(0018, 1044) Contrast/Bolus Total Dose           DS: '40.0'
(0018, 1046) Contrast Flow Rate                  DS: [3, 3]
(0018, 1047) Contrast Flow Duration              DS: [17, 10]
(0018, 1049) Contrast/Bolus Ingredient Concentra DS: '300.0'
(0018, 1100) Reconstruction Diameter             DS: '348.0'
(0018, 1110) Distance Source to Detector         DS: '1040.0'
(0018, 1111) Distance Source to Patient          DS: '570.0'
(0018, 1120) Gantry/Detector Tilt                DS: '0.0'
(0018, 1130) Table Height                        DS: '85.1'
(0018, 1150) Exposure Time                       IS: '434'
(0018, 1151) X-Ray Tube Current                  IS: '258'
(0018, 1152) Exposure                            IS: '108'
(0018, 1160) Filter Type                         SH: 'IMR'
(0018, 1210) Convolution Kernel                  SH: 'IMR1,Soft Tissue'
(0018, 5100) Patient Position                    CS: 'FFS'
(0018, 9305) Revolution Time                     FD: 0.33
(0018, 9306) Single Collimation Width            FD: 0.625
(0018, 9307) Total Collimation Width             FD: 80.0
(0018, 9309) Table Speed                         FD: 185.0
(0018, 9310) Table Feed per Rotation             FD: 97.664
(0018, 9311) Spiral Pitch Factor                 FD: 0.763
(0018, 9345) CTDIvol                             FD: 4.330253533859318
(0018, a001)  Contributing Equipment Sequence  1 item(s) ----
   (0008, 0070) Manufacturer                        LO: 'PHILIPS'
   (0008, 0080) Institution Name                    LO: 'BRILLIANCE4'
   (0008, 0081) Institution Address                 ST: 'BRILLIANCE4'
   (0008, 1010) Station Name                        SH: 'HOST-999999'
   (0008, 1040) Institutional Department Name       LO: 'BRILLIANCE4'
   (0008, 1090) Manufacturer's Model Name           LO: 'BRILLIANCE4'
   (0018, 1000) Device Serial Number                LO: 'BRILLIANCE4'
   (0018, 1020) Software Versions                   LO: '4.5.0.30020'
   (0040, a170)  Purpose of Reference Code Sequence  1 item(s) ----
      (0008, 0100) Code Value                          SH: '109102'
      (0008, 0102) Coding Scheme Designator            SH: 'DCM'
      (0008, 0104) Code Meaning                        LO: 'Processing Equipment'
      ---------
   ---------
(0020, 000d) Study Instance UID                  UI: 1.3.46.670589.33.1.63849049241560857600001.4706589000974752499
(0020, 000e) Series Instance UID                 UI: 1.3.46.670589.33.1.63849049343237673200004.5226562961912261811
(0020, 0010) Study ID                            SH: '8041'
(0020, 0011) Series Number                       IS: '203'
(0020, 0012) Acquisition Number                  IS: '2'
(0020, 0013) Instance Number                     IS: '1'
(0020, 0032) Image Position (Patient)            DS: [-172.7884, 8.90000000000001, 1201.43792746114]
(0020, 0037) Image Orientation (Patient)         DS: [1, 0, 0, 0, 0, -1]
(0020, 0052) Frame of Reference UID              UI: 1.3.46.670589.33.1.63849049263758127200002.5362237490253193614
(0020, 1040) Position Reference Indicator        LO: ''
(0020, 4000) Image Comments                      LT: 'Cor IMR med'
(0028, 0002) Samples per Pixel                   US: 1
(0028, 0004) Photometric Interpretation          CS: 'MONOCHROME2'
(0028, 0010) Rows                                US: 832
(0028, 0011) Columns                             US: 772
(0028, 0030) Pixel Spacing                       DS: [0.4507772, 0.4507772]
(0028, 0100) Bits Allocated                      US: 16
(0028, 0101) Bits Stored                         US: 12
(0028, 0102) High Bit                            US: 11
(0028, 0103) Pixel Representation                US: 0
(0028, 1050) Window Center                       DS: [50, 50]
(0028, 1051) Window Width                        DS: [350, 350]
(0028, 1052) Rescale Intercept                   DS: '-1024.0'
(0028, 1053) Rescale Slope                       DS: '1.0'
(0032, 1033) Requesting Service                  LO: 'CHIPSOFT'
(0040, 1001) Requested Procedure ID              SH: 'CT5001IV'
(0054, 1001) Units                               CS: 'HU'
(00e1, 0010) Private Creator                     LO: 'ELSCINT1'
(00e1, 1036) Private tag data                    CS: 'YES'
(00e1, 1040) [Image Label]                       SH: 'Cor IMR med'
(00e1, 1046) Private tag data                    OB: Array of 512 elements
(01f1, 0010) Private Creator                     LO: 'ELSCINT1'
(01f1, 1001) [Acquisition Type]                  CS: 'SPIRAL'
(01f1, 1002) [Unknown]                           CS: 'STANDARD'
(01f1, 100e) [Unknown]                           FL: 0.0
(01f1, 1027) [Rotation Time]                     DS: '0.33'
(01f1, 1032) [Image View Convention]             CS: 'RIGHT_ON_LEFT'
(01f1, 104a) [Unknown]                           SH: 'DOM'
(01f1, 104b) [Unknown]                           SH: '128x0.625'
(01f1, 104d) [Unknown]                           SH: 'NO'
(01f1, 104e) [Unknown]                           SH: 'Chest'
(01f1, 1054) Private tag data                    IS: '11'
(01f1, 1056) Private tag data                    LO: '30.0451206729581'
(01f7, 0010) Private Creator                     LO: 'ELSCINT1'
(01f7, 1022) [Unknown]                           UI: 1.3.46.670589.33.1.63849049343237673200010.5507538603167078985
(07a1, 0010) Private Creator                     LO: 'ELSCINT1'
(07a1, 1010) [Tamar Software Version]            LO: '4.0.0'
(7fe0, 0010) Pixel Data                          OB: Array of 309328 elements</code></pre>
</div>
<div id="challenge-identifying-safe-metadata-in-dicom" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="challenge-identifying-safe-metadata-in-dicom" class="callout-inner">
<h3 class="callout-title">Challenge: Identifying Safe Metadata in DICOM</h3>
<div class="callout-content">
<p>Can you determine which metadata for this CT scan is likely safe,
meaning it does not lead to patient identification? When would you
choose to retain such data?</p>
</div>
</div>
</div>
<div id="accordionSolution1" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution1" aria-expanded="false" aria-controls="collapseSolution1">
  <h4 class="accordion-header" id="headingSolution1"> Show me the solution </h4>
</button>
<div id="collapseSolution1" class="accordion-collapse collapse" aria-labelledby="headingSolution1" data-bs-parent="#accordionSolution1">
<div class="accordion-body">
<p>Metadata related to the machine, image type, and file type are
generally safe. This information is particularly valuable when sorting
through numerous DICOM files to locate specific types of images or when
generating tabular data for harmonization purposes.</p>
</div>
</div>
</div>
</div>
<p>We can modify elements of our DICOM metadata:</p>
<div class="codewrapper sourceCode" id="cb3">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" tabindex="-1"></a>elem <span class="op">=</span> ds[<span class="bn">0x0010</span>, <span class="bn">0x0010</span>]</span>
<span id="cb3-2"><a href="#cb3-2" tabindex="-1"></a><span class="bu">print</span>(elem.value)</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code><span><span class="st">'OurBeloved^Colleague'</span></span></code></pre>
</div>
<div class="codewrapper sourceCode" id="cb5">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" tabindex="-1"></a>elem.value <span class="op">=</span> <span class="st">'Citizen^Almoni'</span></span>
<span id="cb5-2"><a href="#cb5-2" tabindex="-1"></a><span class="bu">print</span>(elem)</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>(0010, 0010) Patient's Name                      PN: 'Citizen^Almoni'</code></pre>
</div>
<p>In some cases, as here we are dealing with a standard <a href="https://pydicom.github.io/pydicom/stable/reference/generated/pydicom.dataelem.DataElement.html#pydicom.dataelem.DataElement.keyword" class="external-link">keyword</a>.
The keyword <code>PatientName</code> is in programming terms technically
a property of the class <code>FileDataset</code>, but here we are using
“keyword” to refer to it and other very standard properties of the
DICOM. Certain keywords can be modified as it follows:</p>
<div class="codewrapper sourceCode" id="cb7">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" tabindex="-1"></a>ds.PatientName <span class="op">=</span> <span class="st">'Almoni^Shmalmoni'</span></span>
<span id="cb7-2"><a href="#cb7-2" tabindex="-1"></a><span class="bu">print</span>(elem)</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>(0010, 0010) Patient's Name                      PN: 'Almoni^Shmalmoni' </code></pre>
</div>
<p>You can also just set an element to empty by using None:</p>
<div class="codewrapper sourceCode" id="cb9">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" tabindex="-1"></a>ds.PatientName <span class="op">=</span> <span class="va">None</span></span>
<span id="cb9-2"><a href="#cb9-2" tabindex="-1"></a><span class="bu">print</span>(elem)</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>(0010, 0010) Patient's Name                      PN: None</code></pre>
</div>
<p>You can also delete and add elements. After making modifications,
remember to save your file:</p>
<div class="codewrapper sourceCode" id="cb11">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" tabindex="-1"></a>ds.save_as(<span class="st">'data/anonym/my_modified_dicom.dcm'</span>)</span></code></pre>
</div>
<p>We recommend removing at least the patient IDs and birthdates in most
cases. Additionally, consider examining the data elements
‘OtherPatientIDs’ and ‘OtherPatientIDsSequence’.</p>
<div id="challenge-accessing-additional-patient-identifying-data" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="challenge-accessing-additional-patient-identifying-data" class="callout-inner">
<h3 class="callout-title">Challenge: Accessing Additional Patient Identifying Data</h3>
<div class="callout-content">
<p>How can you access and print additional patient identifying data?
Hint: Refer to the documentation and compare with what we have already
printed.</p>
</div>
</div>
</div>
<div id="accordionSolution2" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution2" aria-expanded="false" aria-controls="collapseSolution2">
  <h4 class="accordion-header" id="headingSolution2"> Show me the solution </h4>
</button>
<div id="collapseSolution2" class="accordion-collapse collapse" aria-labelledby="headingSolution2" data-bs-parent="#accordionSolution2">
<div class="accordion-body">
<div class="codewrapper sourceCode" id="cb12">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" tabindex="-1"></a><span class="bu">print</span>(ds.PatientBirthDate)</span>
<span id="cb12-2"><a href="#cb12-2" tabindex="-1"></a><span class="bu">print</span>(ds.PatientID)</span>
<span id="cb12-3"><a href="#cb12-3" tabindex="-1"></a><span class="bu">print</span>(ds.OtherPatientIDs)</span>
<span id="cb12-4"><a href="#cb12-4" tabindex="-1"></a><span class="bu">print</span>(ds.PatientSex)</span>
<span id="cb12-5"><a href="#cb12-5" tabindex="-1"></a><span class="bu">print</span>(ds.PatientAge)</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>19421104
party like 1999
1989442112
M
41Y</code></pre>
</div>
</div>
</div>
</div>
</div>
<p>Pydicom offers a wide range of capabilities. You can visualize your
DICOM data in a hierarchical tree format for user-friendly GUI reading.
It supports downsizing images and handling waveform data such as EKGs.
By integrating with Matplotlib, you can load and plot files seamlessly.
Before adding additional libraries, explore Pydicom’s full potential to
leverage its extensive functionalities.</p>
<div id="keypoints1" class="callout keypoints">
<div class="callout-square">
<i class="callout-icon" data-feather="key"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Key Points</h3>
<div class="callout-content">
<ul>
<li>Certain metadata should almost always be removed from DICOM files
before sharing</li>
<li>Sharing only image files such as JPEGs or NIfTI can mitigate risks
associated with metadata</li>
<li>Imaging data alone, even without explicit metadata, can sometimes
lead to patient identification</li>
<li>Automated tools are available to strip metadata from DICOMs, but
manual verification is necessary due to inconsistencies in how fields
are utilized.</li>
<li>Tools exist to deface images to further protect patient
identity</li>
<li>Several Python libraries enable access to DICOM metadata</li>
</ul>
</div>
</div>
</div>
<!--
Place links that you need to refer to multiple times across pages here. Delete
any links that you are not going to use.
 -->
</div>
</section></section><section id="aio-generative_ai"><p>Content from <a href="generative_ai.html">Generative AI in Medical Imaging</a></p>
<hr>
<p>Last updated on 2024-08-15 |

        <a href="https://github.com/esciencecenter-digital-skills/medical-image-processing/edit/main/episodes/generative_ai.md" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<p>Estimated time: <i aria-hidden="true" data-feather="clock"></i> 40 minutes</p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul>
<li>What is generative AI?</li>
<li>How can generative AI be safely used in my work?</li>
</ul>
</div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul>
<li>Showcase practical examples of generative AI applications</li>
<li>Explore critical considerations regarding the risks and challenges
associated with generative AI</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<section><h2 class="section-heading" id="introduction">Introduction<a class="anchor" aria-label="anchor" href="#introduction"></a>
</h2>
<hr class="half-width">
<p>Generative artificial intelligence (AI) includes technologies that
create or generate data. Since the early 2020s, there has been
significant attention on large language models (LLMs) like ChatGPT,
Copilot, and LLaMA, alongside image generation tools such as Stable
Diffusion, Midjourney, and DALL-E. These tools not only create images
but can also manipulate styles.</p>
<p>The applications of generative AI span widely across fields like
medical imaging, where they hold potential for image interpretation and
numerous other tasks. Of particular interest to participants in this
course are the capabilities of LLMs to generate code and produce large
volumes of synthetic data. Ongoing research continues to explore these
capabilities.</p>
<p>However, the safety implications of these technologies remain a
subject of debate. Depending on the software or model used, data entered
into the system may become the property of the software’s creators. It
is crucial to exercise caution when inputting sensitive information,
such as patient data, into these systems. Understanding where and how
data is stored (i.e., whether on your servers or in the cloud) is
essential to safeguard privacy and confidentiality.</p>
</section><section><h2 class="section-heading" id="mastering-the-use-of-generative-ai-tools">Mastering the Use of Generative AI Tools<a class="anchor" aria-label="anchor" href="#mastering-the-use-of-generative-ai-tools"></a>
</h2>
<hr class="half-width">
<div class="section level3">
<h3 id="architectures-of-generative-ai">Architectures of Generative AI<a class="anchor" aria-label="anchor" href="#architectures-of-generative-ai"></a>
</h3>
<p>Generative AI encompasses models capable of generating new data
across various formats: text, images, video, or audio. Several prominent
architectures, such as generative adversarial networks (GANs),
variational autoencoders, diffusion models, and transformers, achieve
this capability. While the technical intricacies of these architectures
exceed this course’s scope, understanding their operational principles
can illuminate the nature of their outputs. For instance, models
treating data as sequential sequences (e.g., pixels or words) predict
the next element based on probabilities derived from the training
dataset, potentially perpetuating common patterns rather than reflecting
absolute truths.</p>
<p>However, using generative algorithms carries inherent risks,
including inadvertently embedding biases into both data and algorithms.
While humans cannot infer patient ethnicity from body imaging, recent
studies demonstrate that certain AI algorithms can leverage correlations
in data, introducing potential ethical implications. This represents
just one facet of the risks associated with these technologies, which we
will further explore in the following safety section.</p>
</div>
<div class="section level3">
<h3 id="safely-leveraging-generative-ai">Safely Leveraging Generative AI<a class="anchor" aria-label="anchor" href="#safely-leveraging-generative-ai"></a>
</h3>
<ul>
<li>Never upload non-anonymized sensitive patient data outside your
secure servers</li>
<li>Avoid using sensitive patient data with online tools (e.g., chatGPT,
integrated tools like co-pilot, MS Office)</li>
<li>Be mindful of internet connectivity with your code editors and
tools</li>
<li>Prefer systems that operate locally (on your machine or hospital
servers)</li>
<li>Acknowledge that tools may generate erroneous information
(hallucinations)</li>
<li>Don’t assume code generated by a LLM lacks potential bugs or harmful
code</li>
<li>Address intellectual property and ethical considerations when
utilizing generative AI models</li>
<li>Document model usage thoroughly in academic work, detailing versions
and exact applications</li>
<li>Recognize that the training corpus of such models may introduce
biases</li>
</ul>
<p><a href="https://github.com/Stability-AI/StableDiffusion" class="external-link">Stable
Diffusion</a> is an open-source tool widely used for image generation,
capable of running locally on your machine. While older versions
typically require GPUs, recent builds are rumored to run on CPUs,
although at a slower pace. For efficient large-scale image generation,
utilizing a GPU is recommended to save time.</p>
<p>One limitation of Stable Diffusion, and similar tools, is that the
model cannot be customized with your own datasets, restricting the
ability to train or modify it according to specific needs.</p>
<div id="challenge-assessing-the-risks-of-local-model-deployment" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="challenge-assessing-the-risks-of-local-model-deployment" class="callout-inner">
<h3 class="callout-title">Challenge: Assessing the Risks of Local Model Deployment</h3>
<div class="callout-content">
<p>Can you identify a few risks associated with running a model entirely
on your machine?</p>
</div>
</div>
</div>
<div id="accordionSolution1" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution1" aria-expanded="false" aria-controls="collapseSolution1">
  <h4 class="accordion-header" id="headingSolution1"> Show me the solution </h4>
</button>
<div id="collapseSolution1" class="accordion-collapse collapse" aria-labelledby="headingSolution1" data-bs-parent="#accordionSolution1">
<div class="accordion-body">
<p>One significant risk of running any model locally is the potential
for malicious code. Since you need to download the model to your
machine, there is no guarantee that it will not contain malware. To
mitigate this risk, we recommend using open-source models, which allow
for community scrutiny and transparency.</p>
<p>Another concern is the environmental impact of model usage. Training
and maintaining these models consume considerable resources, including
carbon and water. We advise against running models like ChatGPT
continuously due to their substantial environmental footprint. Even when
operating a model locally, there are inherent risks, such as relying on
a smaller training dataset to fit on your machine, which could
compromise the model’s quality and performance.</p>
</div>
</div>
</div>
</div>
</div>
<div class="section level3">
<h3 id="maximizing-the-effectiveness-of-generative-ai">Maximizing the Effectiveness of Generative AI<a class="anchor" aria-label="anchor" href="#maximizing-the-effectiveness-of-generative-ai"></a>
</h3>
<p>The output of generative tools is highly dependent on the specific
words or images inputted, a process known as prompt engineering.</p>
<p>To create content that balances a dataset for machine learning, start
by analyzing the dataset’s balance according to important labels. Then,
describe images with these labels and build prompts based on the
descriptions of label categories with fewer images. This approach helps
focus the model’s use on generating useful outcomes while minimizing
risks.</p>
<p>Keep in mind that content generation can be a somewhat stochastic
process. Therefore, you may need to experiment with and refine your
prompts and even the sequences of prompts to achieve the desired
results.</p>
<div id="effective-prompting-tips" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<div id="effective-prompting-tips" class="callout-inner">
<h3 class="callout-title">Effective Prompting Tips</h3>
<div class="callout-content">
<ol style="list-style-type: decimal">
<li>Ensure accuracy in spelling and grammar to avoid misdirection</li>
<li>Provide clear and detailed prompts</li>
<li>Explain the context or goal (e.g., “helping my son… can you explain
it in a specific way?”)</li>
<li>Try different wordings to refine results</li>
<li>Fact-check thoroughly
<ul>
<li>First, ask the tool to fact-check its output</li>
<li>Then, verify using search engines and experts</li>
<li>Never fully trust references or facts generated by an LLM, seek
human-generated content for confirmation</li>
</ul>
</li>
<li>Clearly indicate if you need a particular format (e.g., bullet
points, square images)</li>
<li>Use prompts like “you are a Python expert” or “you are an expert
pathologist” to guide the tool</li>
<li>For more accurate outputs, fine-tune foundation models with your
specific data</li>
</ol>
<p>These tips will help you optimize the quality and relevance of the
content generated by AI tools.</p>
</div>
</div>
</div>
<p>Attention to detail in creating prompts is critical. For example, the
following image was generated from the misspelling of “X-ray” with a
popular AI tool:</p>
<figure><img src="../fig/chest_xay.png" alt="Misled image" class="figure mx-auto d-block"><div class="figcaption">Image generated by Dr. Candace Makeda Moore
prompting <a href="https://www.adobe.com/products/firefly.html" class="external-link">Adobe
Firely</a>.</div>
</figure><p>While this result is laughable, more subtle mistakes can be harder to
identify, especially for those without extensive training in a specific
field. Therefore, having a specialist review the generated data is
crucial to ensure its validity.</p>
<p>It is also important to remember that the training data for some
algorithms is often sourced from the internet, which includes a lot of
irrelevant or misleading content. For instance, while there are many
cute cat pictures available, there are very few good examples of rare
medical conditions like desmoplastic infantile ganglioma. Additionally,
using terms like “CAT scan” (historically referring to computer axial
tomography) might result in images of cats rather than the intended
medical imagery:</p>
<figure><img src="../fig/CAT_scan.png" alt="Misled image of cats" class="figure mx-auto d-block"><div class="figcaption">Image generated by Dr. Candace Makeda Moore
prompting <a href="https://www.adobe.com/products/firefly.html" class="external-link">Adobe
Firely</a>.</div>
</figure><div id="keypoints1" class="callout keypoints">
<div class="callout-square">
<i class="callout-icon" data-feather="key"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Key Points</h3>
<div class="callout-content">
<ul>
<li>Generative programs can create synthetic data, potentially enhancing
various algorithms</li>
<li>Generative AI models have inherent limitations</li>
<li>Running generative AI programs locally on your own server is safer
than using programs that send prompts to external servers</li>
<li>Exercise caution when entering patient data into generative AI
programs</li>
<li>Numerous policies exist to ensure the safe and ethical use of
generative AI tools across institutions</li>
</ul>
</div>
</div>
</div>
<!--
Place links that you need to refer to multiple times across pages here. Delete
any links that you are not going to use.
 -->
</div>
</section></section>
</div>
    </main>
</div>
<!-- END  : inst/pkgdown/templates/content-extra.html -->

      </div>
<!--/div.row-->
      		<footer class="row footer mx-md-3"><hr>
<div class="col-md-6">
        <p>This lesson is subject to the <a href="CODE_OF_CONDUCT.html">Code of Conduct</a></p>
        <p>

        <a href="https://github.com/esciencecenter-digital-skills/medical-image-processing/edit/main/README.md" class="external-link">Edit on GitHub</a>

	
        | <a href="https://github.com/esciencecenter-digital-skills/medical-image-processing/blob/main/CONTRIBUTING.md" class="external-link">Contributing</a>
        | <a href="https://github.com/esciencecenter-digital-skills/medical-image-processing/" class="external-link">Source</a></p>
				<p><a href="https://github.com/esciencecenter-digital-skills/medical-image-processing/blob/main/CITATION" class="external-link">Cite</a> | <a href="mailto:c.moore@esciencecenter.nl">Contact</a> | <a href="https://carpentries.org/about/" class="external-link">About</a></p>
			</div>
			<div class="col-md-6">

        <p>Materials licensed under <a href="LICENSE.html">CC-BY 4.0</a> by the authors</p>

        <p>Template licensed under <a href="https://creativecommons.org/licenses/by-sa/4.0/" class="external-link">CC-BY 4.0</a> by <a href="https://carpentries.org/" class="external-link">The Carpentries</a></p>
        <p>Built with <a href="https://github.com/carpentries/sandpaper/tree/0.16.8" class="external-link">sandpaper (0.16.8)</a>, <a href="https://github.com/carpentries/pegboard/tree/0.7.6" class="external-link">pegboard (0.7.6)</a>, and <a href="https://github.com/carpentries/varnish/tree/1.0.4" class="external-link">varnish (1.0.4)</a></p>
			</div>
		</footer>
</div> <!-- / div.container -->
	<div id="to-top">
		<a href="#top">
      <i class="search-icon" data-feather="arrow-up" role="img" aria-label="Back To Top"></i><br><!-- <span class="d-none d-sm-none d-md-none d-lg-none d-xl-block">Back</span> To Top --><span class="d-none d-sm-none d-md-none d-lg-none d-xl-block">Back</span> To Top
		</a>
	</div>
  <script type="application/ld+json">
    {
  "@context": "https://schema.org",
  "@type": "TrainingMaterial",
  "@id": "https://esciencecenter-digital-skills.github.io/medical-image-processing/instructor/aio.html",
  "inLanguage": "en",
  "dct:conformsTo": "https://bioschemas.org/profiles/TrainingMaterial/1.0-RELEASE",
  "description": "A Carpentries Lesson teaching foundational data and coding skills to researchers worldwide",
  "keywords": "python, software, lesson, medical images, DICOMs",
  "name": "All in One View",
  "creativeWorkStatus": "active",
  "url": "https://esciencecenter-digital-skills.github.io/medical-image-processing/instructor/aio.html",
  "identifier": "https://esciencecenter-digital-skills.github.io/medical-image-processing/instructor/aio.html",
  "dateCreated": "2024-10-14",
  "dateModified": "2024-10-14",
  "datePublished": "2024-10-14"
}

  </script><script>
		feather.replace();
	</script>
</body>
</html><!-- END:   inst/pkgdown/templates/layout.html-->

